{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "BertSum-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgxF_r0W1GoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5c2964-3d56-4b91-cb04-8ca654f297ed"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXTqE-o41GoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f87c2d6-d167-4cdf-abc6-e74c5ab5a06e"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqPukq8-7p9"
      },
      "source": [
        "EVAL = False # set if running in eval only mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZzLukghfCP6"
      },
      "source": [
        "!pip install -q tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYjxy07C1GoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048958ce-ea9a-443f-bfc3-8086d0f0aade"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duVpp_GbBCEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353b1465-ce25-4444-a55e-bf6a3969eba5"
      },
      "source": [
        "!ls /content/drive/'My Drive'/summ_data/cnn/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints  icsiruns  logs  processing  result  result2  tboard  tensors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbEHmTwH2Zil"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import datetime, time\n",
        "import io\n",
        "import re\n",
        "from csv import reader\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import glob, os\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertModel, AdamW, BertConfig,BertTokenizer,BertPreTrainedModel\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "from rouge import Rouge \n",
        "import logging\n",
        "from collections import OrderedDict, defaultdict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSLt1fDx23xu"
      },
      "source": [
        "## Be very careful with block below (ensure that it is deleting the results directory and nothing else in your drive!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aws8nctQ1GoO"
      },
      "source": [
        "BASE_DIR = \"/content/drive/My Drive/summ_data/cnnv2/\"\n",
        "INPUT_DIR = \"/content/drive/My Drive/summ_data/cnn/\"+\"processing/\"\n",
        "DAT_DIR = BASE_DIR+\"tensors/\"\n",
        "RESULT_DIR = BASE_DIR+\"result2/\"\n",
        "TBOARD_LOG_DIR = BASE_DIR+\"tboard/\"\n",
        "LOG_DIR = BASE_DIR+\"logs/\"\n",
        "\n",
        "CP_DIR = \"/content/drive/My Drive/summ_data/cnn/\"+\"checkpoints/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc993ReJaZCG"
      },
      "source": [
        "for dir in (DAT_DIR, RESULT_DIR, TBOARD_LOG_DIR, CP_DIR, LOG_DIR):\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "\n",
        "if not EVAL:\n",
        "    for dir in (RESULT_DIR, TBOARD_LOG_DIR):\n",
        "        for f in glob.glob(dir+\"*\"):\n",
        "            os.remove(f)\n",
        "\n",
        "else:\n",
        "    for dir in (RESULT_DIR,):\n",
        "        for f in glob.glob(dir+\"*\"):\n",
        "            print(f)\n",
        "            os.remove(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khkcJ2dHuSzw"
      },
      "source": [
        "log_file = LOG_DIR+datetime.datetime.now().strftime(\"%Y%m%d.%H.%M\")+\".txt\"\n",
        "logging.basicConfig(filename=log_file, level=logging.DEBUG)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.addHandler(logging.StreamHandler(sys.stdout))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7SYp_-emUEr",
        "outputId": "3c28abc9-025e-4799-b453-7c3418d786ed"
      },
      "source": [
        "def save_cp(model_state, optimizer_state, scheduler, epoch):\n",
        "    cp = {'model_state': model_state, 'optimizer_state':optimizer_state, 'scheduler': scheduler}\n",
        "    filename = datetime.datetime.now().strftime(\"%Y%m%d.%H.\")+str(epoch)+\".pth.tar\"\n",
        "    torch.save(cp, CP_DIR+filename )\n",
        "\n",
        "def load_cp(filename):\n",
        "    cp = torch.load(filename)\n",
        "    return cp['model_state'], cp['optimizer_state'], cp['scheduler']\n",
        "\n",
        "logger.debug(\"logging initializer\")\n",
        "print(log_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logging initializer\n",
            "/content/drive/My Drive/summ_data/cnnv2/logs/20201129.12.18.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke_S8mz1GoS"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqDNoA44Dth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b7964d-703c-4e2a-83e0-dab232944fc3"
      },
      "source": [
        "files = glob.glob(INPUT_DIR+'*')\n",
        "print(INPUT_DIR)\n",
        "for f in files:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/summ_data/cnn/processing/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFp1_kuem-4h"
      },
      "source": [
        "train_d, test_d, val_d = defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "src_maps = dict()\n",
        "for d_type, d_dict in zip(('train', 'valid', 'test'), (train_d, test_d, val_d)):\n",
        "    row_idx = 0\n",
        "\n",
        "    for file_idx, inp_file in enumerate(glob.glob(INPUT_DIR+\"*\"+d_type+\"*\")):\n",
        "        file_d = torch.load(inp_file)\n",
        "        #we have ['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt']\n",
        "        #we need ['idx', 'src', 'labels', 'segs', 'clss', 'attn', 'mask_cls']\n",
        "        for rownum, dat in enumerate(file_d):\n",
        "            d_dict['idx'].append(rownum)\n",
        "            for k in ('src', 'labels', 'segs', 'clss'):\n",
        "                dat[k].extend([0 for _ in range(512-len(dat[k]))])\n",
        "                d_dict[k].append(dat[k])\n",
        "\n",
        "            #attn\n",
        "            attn= [1 if x!=0 else 0 for x in dat['src'] ]\n",
        "            d_dict['attn'].append(attn)\n",
        "\n",
        "            # mask_cls\n",
        "            mask_cls = [1 if x!=0 else 0 for x in dat['clss'] ]\n",
        "            if dat['clss'][0] == 0:\n",
        "                mask_cls[0] = 1\n",
        "            d_dict['mask_cls'].append(mask_cls)\n",
        "\n",
        "            #save src and target text\n",
        "            src_maps[d_type+\"_src_\"+str(rownum)] = dat[\"src_txt\"]\n",
        "            src_maps[d_type+\"_tgt_\"+str(rownum)] = dat[\"tgt_txt\"]\n",
        "\n",
        "            row_idx += 1\n",
        "#        if (file_idx%75) == 0:\n",
        "#            break\n",
        "    for k, v in d_dict.items():\n",
        "        d_dict[k] = torch.LongTensor(v)\n",
        "\n",
        "train_dataset = TensorDataset(train_d[\"idx\"], train_d[\"src\"],train_d[\"labels\"], train_d[\"segs\"], \n",
        "                              train_d[\"clss\"], train_d[\"attn\"], train_d[\"mask_cls\"])\n",
        "val_dataset = TensorDataset(val_d[\"idx\"], val_d[\"src\"], val_d[\"labels\"], val_d[\"segs\"], \n",
        "                              val_d[\"clss\"], val_d[\"attn\"], val_d[\"mask_cls\"])\n",
        "test_dataset = TensorDataset(test_d[\"idx\"], test_d[\"src\"], test_d[\"labels\"], test_d[\"segs\"], \n",
        "                              test_d[\"clss\"], test_d[\"attn\"], test_d[\"mask_cls\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6KelvDx1GoU"
      },
      "source": [
        "## Encoder classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCiy60801GoU"
      },
      "source": [
        "class BertForSummarization(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = BertModel(config)\n",
        "        # Initialize model weights (inherited function).\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        # the below returns a tuple. First element in the tuple is last hidden state. Second element in tuple is pooler output\n",
        "        self.result = self.model(input_ids=x, attention_mask =mask, token_type_ids=segs)\n",
        "        top_vec = self.result[0]\n",
        "        return top_vec\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask_cls):\n",
        "#        h = self.linear2(self.tanh(self.linear1(x)))\n",
        "        h = self.linear1(x)\n",
        "        h = h.squeeze(-1)\n",
        "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
        "        return sent_scores, x, h\n",
        "\n",
        "\n",
        "\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args=None, num_hidden = 768, load_pretrained_bert = True, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.bert = BertForSummarization.from_pretrained(\n",
        "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "            output_attentions = False, # Whether the model returns attentions weights.\n",
        "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "        )\n",
        "#        if (args.encoder == 'classifier'):\n",
        "        self.encoder = Classifier(num_hidden)\n",
        "        logger.debug(\"dropout = \"+str(nn.Dropout(self.bert.config.hidden_dropout_prob)))\n",
        "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        for p in self.encoder.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "\n",
        "    def load_cp(self, state):\n",
        "        self.load_state_dict(state, strict=True)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "        top_vec = self.bert(x, segs, mask)\n",
        "#        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "#        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sents_vec = top_vec.gather(1, clss.unsqueeze(-1).expand(-1, -1, 768))\n",
        "        sents_vec = self.dropout(sents_vec)\n",
        "#        cls_outs = sents_vec.clone().detach().cpu().numpy()\n",
        "#        print(\">>> \", cls_outs.shape, np.sum(cls_outs, axis=1)[:, :20])\n",
        "\n",
        "        sent_scores, x, h = self.encoder(sents_vec, mask_cls)\n",
        "        sent_scores = sent_scores.squeeze(-1)\n",
        "        return sent_scores, mask_cls, x, h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya6lST7vfnt2"
      },
      "source": [
        "# setup tensorboard for visualizing results\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter(log_dir=TBOARD_LOG_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8jovkq1GoX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s9QAJzC1GoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cb2bc8-fa97-4a09-b49c-5fe9e078f3f8"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print('GPU device not found')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU device not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCaPKGs1GoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa11a724-5a27-4f9b-98d2-03cee63f1d8d"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYNj9BO1Gog"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsd1R0hj1Goi"
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The test samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aOu9sDc218H"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buj0b0AM2yrZ"
      },
      "source": [
        "model = Summarizer()\n",
        "# tell pytorch to run on GPU\n",
        "torch.cuda.empty_cache()\n",
        "_= model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGytz8Jj1Gol"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLohVl61Gon"
      },
      "source": [
        "## set all layers to train\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad=True\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j01a9eZP1Gop"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLQW2YaQ1Gos"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = int(total_steps * .2),\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3vf6hj2RyH"
      },
      "source": [
        "# Load checkpoint if required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW2AOHZQ1xKg"
      },
      "source": [
        "#load checkpoints if provided\n",
        "cp_file = CP_DIR+'20201128.16.0.pth.tar'\n",
        "if cp_file:\n",
        "    m_state, o_state, s_state = load_cp(cp_file)\n",
        "    model.load_cp(m_state)\n",
        "    optimizer.load_state_dict(o_state)\n",
        "    scheduler.load_state_dict(s_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhetRFG1Gov"
      },
      "source": [
        "This function writes out the labeled data as well as the result for evaluation using rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjIpM-zwjSUp"
      },
      "source": [
        "def _get_ngrams(n, text):\n",
        "    ngram_set = set()\n",
        "    text_length = len(text)\n",
        "    max_index_ngram_start = text_length - n\n",
        "    for i in range(max_index_ngram_start + 1):\n",
        "        ngram_set.add(tuple(text[i:i + n]))\n",
        "    return ngram_set\n",
        "\n",
        "def _block_tri(c, p):\n",
        "    tri_c = _get_ngrams(3, c.split())\n",
        "    for s in p:        \n",
        "        tri_s = _get_ngrams(3, s.split())\n",
        "        if len(tri_c.intersection(tri_s))>0:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2is8hx6W1Gov"
      },
      "source": [
        "def gen_outputs(epoch, probs, labels, cls_ids, mask_cls,src,  meetings, is_test=False,):\n",
        "    # extract sentences and labels\n",
        "\n",
        "    f_handles_p = OrderedDict()\n",
        "    f_handles_l = OrderedDict()\n",
        "    f_content_p = OrderedDict()\n",
        "\n",
        "    for i, m in enumerate(meetings):\n",
        "        if m not in f_handles_p:\n",
        "            if not is_test:\n",
        "                f_handles_p[m] = open(RESULT_DIR+\"PRED_\"+str(m)+\"_\"+str(epoch)+\".txt\", \"a\")\n",
        "                f_handles_l[m] = open(RESULT_DIR+\"LABEL_\"+str(m)+\"_\"+str(epoch)+\".txt\", \"a\")\n",
        "            else:\n",
        "                f_handles_p[m] = open(RESULT_DIR+\"TPRED_\"+str(m)+\"_\"+\".txt\", \"a\")\n",
        "                f_handles_l[m] = open(RESULT_DIR+\"TLABEL_\"+str(m)+\"_\"+\".txt\", \"a\")\n",
        "            p_file = RESULT_DIR+\"PRED_\"+str(m)+\"_\"+str(epoch)+\".txt\"\n",
        "            if os.path.isfile(p_file):\n",
        "                with open (RESULT_DIR+\"PRED_\"+str(m)+\"_\"+str(epoch)+\".txt\", \"r\") as pf:\n",
        "                    f_content_p[m] = pf.readlines()\n",
        "            else:\n",
        "                f_content_p[m] = []\n",
        "\n",
        "    sel_num = np.rint(np.sum(mask_cls, axis=1) * 0.15)\n",
        "    selected_ids = np.argsort(-probs, 1)\n",
        "    selected = dict()\n",
        "    for y in range(probs.shape[0]):\n",
        "        selected[y] = set()\n",
        "        for x in range(probs.shape[1]):\n",
        "            if x < sel_num[y]:\n",
        "                selected[y].add(selected_ids[y,x])\n",
        "    files = None\n",
        "\n",
        "\n",
        "    f_names = [(a.name, b.name) for a, b in zip(f_handles_p.values(), f_handles_l.values())]\n",
        "    try:\n",
        "        for pi, passage in enumerate(src):\n",
        "            #print(probs[p,:20])\n",
        "            lines = tokenizer.decode(passage)\n",
        "            lines = lines.split(\"[SEP]\")\n",
        "            for i, sent in enumerate(lines):\n",
        "                sent = sent.replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").replace(\"[PAD]\", \"\")\n",
        "                if labels[pi, i] == 1:\n",
        "                    f_handles_l[meetings[pi]].write(sent + \"\\n\")\n",
        "                #if probs_bin[p, i] == 1:\n",
        "                if i in selected[pi]:\n",
        "                    if (len(sent.split()) >= 3 and not _block_tri(sent, f_content_p[meetings[pi]])):\n",
        "                        f_content_p[meetings[pi]].append(sent)\n",
        "                        #print(\">>>\",probs[p,i])\n",
        "                        f_handles_p[meetings[pi]].write(sent + \"\\n\")\n",
        "    finally:\n",
        "        for handle in list(f_handles_p.values()) + list(f_handles_l.values()):\n",
        "            handle.close()\n",
        "    #logger.debug(f_names)\n",
        "    return f_names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF3gRdUhmf9W"
      },
      "source": [
        "def avg_rouge(filesets):\n",
        "  scores = list()  \n",
        "  for i, (hyp_file, lab_file) in enumerate(filesets):\n",
        "    with open(hyp_file, \"r\") as ref, open(lab_file, \"r\") as lab:\n",
        "      reference = lab.read()\n",
        "      hypothesis = ref.read()\n",
        "      rouge = Rouge()\n",
        "      try:\n",
        "        score = rouge.get_scores(hypothesis, reference)\n",
        "        scores.append(score)\n",
        "      except:\n",
        "        print(\"empty file\", hyp_file, lab_file)\n",
        "        pass\n",
        "\n",
        "  # average rouge scores\n",
        "  result_rouge = None\n",
        "  for rouge_res in scores:\n",
        "    if result_rouge is None:\n",
        "      result_rouge = rouge_res[0]\n",
        "      continue\n",
        "    for key, obj in rouge_res[0].items():\n",
        "      for k1, v1 in obj.items():\n",
        "        result_rouge[key][k1] += v1\n",
        "  results = None\n",
        "  if len(scores) > 0:\n",
        "    print(\"Averaging number of scores = \", len(scores))\n",
        "    res = scores[0][0] # just for initializing keys. Values do not matter here\n",
        "    for k, v in result_rouge.items():\n",
        "        for k1, v1 in v.items():\n",
        "            print(k,k1)\n",
        "            res[k][k1] = v1/i\n",
        "    results = res\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEP-_5J1Gox"
      },
      "source": [
        "## setup loss function and other variables required for training\n",
        "\n",
        "#seed_val = 42\n",
        "\n",
        "#random.seed(seed_val)\n",
        "#np.random.seed(seed_val)\n",
        "#torch.manual_seed(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "loss_c = torch.nn.BCELoss(reduction='none')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtW7t-Edxb0R"
      },
      "source": [
        "# setup tensorboard for visualizing results\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter(log_dir=TBOARD_LOG_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZLUyra1Goz"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmBU9JRG1Goz"
      },
      "source": [
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(1, epochs):\n",
        "    if EVAL:\n",
        "        break\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            logger.debug('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        \n",
        "        idx, src, labels, segs, clss, attn, mask_cls= batch\n",
        "        idx, src, labels, segs, clss, attn, mask_cls = idx, src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
        "        loss = loss_c(probs, labels.float())\n",
        "        loss = (loss * mask_cls.float()).sum()\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    # save checkpoint\n",
        "    #if (epoch_i+1 )%5 == 0:\n",
        "    if True:\n",
        "        logger.debug(\"saving checkpoint\"+ str(epoch_i))\n",
        "        save_cp(model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), epoch_i)\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch_i)\n",
        "    writer.flush()\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    logger.debug(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    logger.debug(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    step = -1\n",
        "    filesets = list()\n",
        "    for batch in validation_dataloader:\n",
        "        step += 1\n",
        "        idx, src, labels, segs, clss, attn, mask_cls= batch\n",
        "        idx, src, labels, segs, clss, attn, mask_cls = idx, src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
        "            loss = loss_c(probs, labels.float())\n",
        "            loss = (loss * mask_cls.float()).sum()\n",
        "            # write results so that we can use rouge to compare\n",
        "            if False:\n",
        "                files = gen_outputs(epoch_i, probs.to('cpu').numpy(), labels.to('cpu').numpy(), clss.to('cpu').numpy(), mask_cls.to('cpu').numpy(), src.to('cpu').numpy(), idx.to('cpu').numpy())\n",
        "                filesets.extend(files)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
        "    logger.debug(\"  Average validation loss: {0:.2f}\".format(avg_eval_loss))\n",
        "    logger.debug(\"  Validation epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    writer.add_scalar(\"Loss/validation\", avg_eval_loss, epoch_i)\n",
        "    writer.flush()\n",
        "\n",
        "    #rouge scores for the epoch\n",
        "    #if (epoch_i+1 )%5 == 0:\n",
        "    if False:\n",
        "        logger.debug(\"prior to running rouge\")\n",
        "        rouge_scores = avg_rouge(filesets)\n",
        "        logger.debug(\"completed running rouge\")\n",
        "        logger.debug(rouge_scores)\n",
        "        if rouge_scores is not None:\n",
        "            for r_type in (\"rouge-1\", 'rouge-2'):\n",
        "                for metric in (\"f\", \"p\", \"r\"):\n",
        "                    writer.add_scalar(r_type+\"/\"+metric+\"/\"+\"Validation\", rouge_scores[r_type][metric], epoch_i)\n",
        "        writer.flush()\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "writer.close()\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ6x76UGW2H9"
      },
      "source": [
        "!kill 1799"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCqq-x_8aNDR"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXOSnBNv3aMa"
      },
      "source": [
        "# ========================================\n",
        "#               Testing\n",
        "# ========================================\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Running Test...\")\n",
        "t0 = time.time()\n",
        "\n",
        "# Put the model in evaluation mode--the dropout layers behave differently\n",
        "# during evaluation.\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_test_accuracy = 0\n",
        "total_test_loss = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "step = -1\n",
        "filesets = list()\n",
        "for batch in test_dataloader:\n",
        "    step += 1\n",
        "    idx, src, labels, segs, clss, attn, mask_cls= batch\n",
        "    idx, src, labels, segs, clss, attn, mask_cls = idx, src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "    \n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():        \n",
        "        probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
        "        loss = loss_c(probs, labels.float())\n",
        "        loss = (loss * mask_cls.float()).sum()\n",
        "        # write results so that we can use rouge to compare\n",
        "        files = gen_outputs(None, probs.to('cpu').numpy(), labels.to('cpu').numpy(), clss.to('cpu').numpy(), mask_cls.to('cpu').numpy(), src.to('cpu').numpy(), idx.to('cpu').numpy(), is_test=True)\n",
        "        filesets.extend(files)\n",
        "        logger.debug(\"TEST LOSS TOTAL\"+str(loss.item()))\n",
        "\n",
        "    # Accumulate the test loss.\n",
        "    total_test_loss += loss.item()\n",
        "\n",
        "# Calculate the average loss over all of the batches.\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "logger.debug(\"  Average Test loss: {0:.2f}\".format(avg_test_loss))\n",
        "\n",
        "writer.add_scalar(\"Loss/Test\", avg_test_loss)\n",
        "writer.flush()\n",
        "\n",
        "#rouge scores for the epoch\n",
        "rouge_scores = avg_rouge(filesets)\n",
        "logger.debug(rouge_scores)\n",
        "if rouge_scores is not None:\n",
        "    for r_type in (\"rouge-1\", 'rouge-2'):\n",
        "        for metric in (\"f\", \"p\", \"r\"):\n",
        "            print(\"writing metrics\")\n",
        "            writer.add_scalar(r_type+\"/\"+metric+\"/\"+\"Test\", rouge_scores[r_type][metric])\n",
        "writer.flush() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbWXpiXM1vbl"
      },
      "source": [
        "print(TBOARD_LOG_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fAijryyeBwK"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir=\"/content/drive/My Drive/summ_data/cnn/tboard/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgfplc-phTCz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}