{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "BertSum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e460675831d4f929fb1a0d188f45fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6868fd8370e248d4bc1e86b862d871f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_24ff99261e8943ae8fa74103e528ca2e",
              "IPY_MODEL_915b55ab1c6e45419887d30ce324f576"
            ]
          }
        },
        "6868fd8370e248d4bc1e86b862d871f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24ff99261e8943ae8fa74103e528ca2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5a811bfa8dda41d0aad538c1c7b03432",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbd66041dea0478f962419ecc5b83874"
          }
        },
        "915b55ab1c6e45419887d30ce324f576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e411dd373f1a40088102b4d543a4923a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 521kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea950038967147d6982a0f47b70bdedc"
          }
        },
        "5a811bfa8dda41d0aad538c1c7b03432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbd66041dea0478f962419ecc5b83874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e411dd373f1a40088102b4d543a4923a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea950038967147d6982a0f47b70bdedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc54d039af214ac7b44eec55ea62e367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_43bbbbef687b4bce839e1c133b9a3b2d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fdda9480f5c34afab56ac1f44356511e",
              "IPY_MODEL_605497209ed24e28829d35c8cbba7b26"
            ]
          }
        },
        "43bbbbef687b4bce839e1c133b9a3b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdda9480f5c34afab56ac1f44356511e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1878e8261ebc48f799c42b082c937d4f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf7e1f1e840f4abca2f38f871d7ae605"
          }
        },
        "605497209ed24e28829d35c8cbba7b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e16f0c3a74dc4283ac703014d01f561d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 875B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97492f4b1a5a43b586da8989c87014f9"
          }
        },
        "1878e8261ebc48f799c42b082c937d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf7e1f1e840f4abca2f38f871d7ae605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e16f0c3a74dc4283ac703014d01f561d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97492f4b1a5a43b586da8989c87014f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5343c08215414811ab49ada775fd7dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cac749510b3a42daa195a2234c7bbf0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ade843f9d2a44dcf8844d1842cb4889e",
              "IPY_MODEL_52315ea3f50c40cfa57c0583c60b60d0"
            ]
          }
        },
        "cac749510b3a42daa195a2234c7bbf0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ade843f9d2a44dcf8844d1842cb4889e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1001cbc33f2a4563957615288a8fe097",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c5fb826cf9c46e7a7a82c65e2089f48"
          }
        },
        "52315ea3f50c40cfa57c0583c60b60d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0918d7b224014852a85a61ad2d1d2c0c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:09&lt;00:00, 47.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_890d0baa01aa4cdb883c74394724c6d3"
          }
        },
        "1001cbc33f2a4563957615288a8fe097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c5fb826cf9c46e7a7a82c65e2089f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0918d7b224014852a85a61ad2d1d2c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "890d0baa01aa4cdb883c74394724c6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgxF_r0W1GoF",
        "outputId": "f7a152d8-fc41-42b7-a2cd-fa8e35410057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 42.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a77d8ec518896b02a287a8b486b3d193e58fc1533be46f56119454fc62ab3b74\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXTqE-o41GoI",
        "outputId": "c277ab6a-90c0-444a-c4d9-1840f6471f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqPukq8-7p9"
      },
      "source": [
        "EVAL = False # set if running in eval only mode"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYjxy07C1GoM",
        "outputId": "4eacf4fb-b4e9-4202-db99-66ef75fe37b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duVpp_GbBCEz",
        "outputId": "6924888b-e927-4c63-9e1a-68c2e792a542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/drive/'My Drive'/summ_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  ICSI_plus_NXT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbEHmTwH2Zil",
        "outputId": "73f09d15-3392-4efb-d5b1-b5428acc015f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1e460675831d4f929fb1a0d188f45fee",
            "6868fd8370e248d4bc1e86b862d871f5",
            "24ff99261e8943ae8fa74103e528ca2e",
            "915b55ab1c6e45419887d30ce324f576",
            "5a811bfa8dda41d0aad538c1c7b03432",
            "bbd66041dea0478f962419ecc5b83874",
            "e411dd373f1a40088102b4d543a4923a",
            "ea950038967147d6982a0f47b70bdedc"
          ]
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import datetime, time\n",
        "import io\n",
        "import re\n",
        "from csv import reader\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "import glob, os\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertModel, AdamW, BertConfig,BertTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "from rouge import Rouge \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e460675831d4f929fb1a0d188f45fee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSLt1fDx23xu"
      },
      "source": [
        "## Be very careful with block below (ensure that it is deleting the results directory and nothing else in your drive!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aws8nctQ1GoO"
      },
      "source": [
        "BASE_DIR = \"/content/drive/My Drive/summ_data/\"\n",
        "INPUT_DIR = BASE_DIR+\"ICSI_plus_NXT/processing/\"\n",
        "DAT_DIR = BASE_DIR+\"ICSI_plus_NXT/tensors/\"\n",
        "RESULT_DIR = BASE_DIR+\"ICSI_plus_NXT/result/\"\n",
        "\n",
        "if not os.path.exists(DAT_DIR):\n",
        "    os.makedirs(DAT_DIR)\n",
        "\n",
        "if not os.path.exists(RESULT_DIR):\n",
        "    os.makedirs(RESULT_DIR)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cke_S8mz1GoS"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAcDUCqw1GoS"
      },
      "source": [
        "class BertDataProcessor:\n",
        "    def __init__(self, data_dir, out_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.out_dir = out_dir\n",
        "        self.inp_df = None\n",
        "        self.meetings = set()\n",
        "\n",
        "        for inp_file in sorted(glob.glob(self.data_dir+\"/*txt\")):\n",
        "            f_name = os.path.basename(inp_file)\n",
        "            self.meetings.add(f_name.split(\".\")[0])\n",
        "        if not os.path.exists(self.out_dir):\n",
        "            os.makedirs(self.out_dir)\n",
        "\n",
        "    def split(self):\n",
        "        '''\n",
        "        train, validation and test split by meeting\n",
        "        '''\n",
        "        meetings = list(self.meetings)\n",
        "        self.train_list = meetings[:50]\n",
        "        self.validation_list = meetings[50:58]\n",
        "        self.test_list = meetings[58:]\n",
        "\n",
        "#        self.train_list = meetings[:10]\n",
        "#        self.validation_list = meetings[10:15]\n",
        "#        self.test_list = meetings[15:20]\n",
        "\n",
        "\n",
        "\n",
        "    def format_to_bert(self, args=None):\n",
        "\n",
        "        cls_vid = tokenizer.vocab[\"[CLS]\"]\n",
        "        sep_vid = tokenizer.vocab[\"[SEP]\"]\n",
        "\n",
        "        for batch, f_names in ((\"train\", self.train_list), (\"validation\",self.test_list),\n",
        "                 (\"test\", self.validation_list)):\n",
        "            output = []\n",
        "            for f_name in f_names:\n",
        "                # process a meeting at a time\n",
        "                pth = self.data_dir+\"/\"+f_name+\".\"+\"*txt\"\n",
        "                cur_chunk = None\n",
        "                cur_labels = []\n",
        "                for inp_file in sorted(glob.glob(pth)):\n",
        "                    with open(inp_file, \"r\") as mtg_f:\n",
        "                        for line in mtg_f:\n",
        "                            sent, label = line.split(\"|\")\n",
        "                            s_chunk = tokenizer.tokenize(sent) [:510]\n",
        "                            s_chunk = [\"[CLS]\"] + s_chunk + [\"[SEP]\"]\n",
        "                            if cur_chunk is None:\n",
        "                                cur_chunk = s_chunk\n",
        "                                cur_labels.append(int(label))\n",
        "\n",
        "                            else:\n",
        "                                # if new line fits in to remaining space, add it, else fill with spaces and add a new line\n",
        "                                if len(s_chunk) + len(cur_chunk) < 512:\n",
        "                                    cur_chunk += s_chunk\n",
        "                                    cur_labels.append(int(label))\n",
        "                                else:\n",
        "                                    input_ids = tokenizer.convert_tokens_to_ids(cur_chunk)\n",
        "                                    attn_masks = [1]*len(input_ids)\n",
        "                                    cls_ids = [i for i, t in enumerate(input_ids) if t == cls_vid ]\n",
        "                                    mask_cls = [1 for _ in range(len(cls_ids))]\n",
        "\n",
        "                                    [attn_masks.append(0) for _ in range(len(attn_masks), 512)]\n",
        "                                    [input_ids.append(0) for _ in range(len(input_ids), 512)]\n",
        "                                    [cls_ids.append(0) for _ in range(len(cls_ids), 512)]\n",
        "                                    [mask_cls.append(0) for _ in range(len(mask_cls), 512)]\n",
        "\n",
        "                                    _segs = [-1] + [i for i, t in enumerate(input_ids) if t == sep_vid]\n",
        "                                    segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "                                    segments_ids = []\n",
        "                                    for i, s in enumerate(segs):\n",
        "                                        if (i % 2 == 0):\n",
        "                                            segments_ids += s * [0]\n",
        "                                        else:\n",
        "                                            segments_ids += s * [1]\n",
        "                                    [cur_labels.append(0) for _ in range(len(cur_labels), 512)]\n",
        "                                    [segments_ids.append(0) for _ in range(len(segments_ids), 512)]\n",
        "                                    b_data_dict = {\"src\": input_ids, \"labels\": cur_labels, \"segs\": segments_ids, \n",
        "                                                'clss': cls_ids, \"attn\": attn_masks, \"mask_cls\":mask_cls}\n",
        "                                    output.append(b_data_dict)\n",
        "                                    cur_chunk = s_chunk\n",
        "                                    cur_labels = [int(label)]\n",
        "            out =  {\"src\": [], \"labels\": [], \"segs\": [], \n",
        "                                                'clss': [], \"attn\": [], \"mask_cls\":[]}\n",
        "            for sample in output:\n",
        "                for key, val in sample.items():\n",
        "                    out[key].append(val)\n",
        "            for k, v in out.items():\n",
        "                out[k] = torch.LongTensor(v)\n",
        "            for k, v in out.items():\n",
        "                torch.save(v, self.out_dir+\"/\"+k+\"_\"+batch+\".pt\")\n",
        "\n",
        "#dp = BertDataProcessor(INPUT_DIR, DAT_DIR)\n",
        "#dp.split()\n",
        "#dp.format_to_bert()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqDNoA44Dth",
        "outputId": "12dfef70-0c35-4ba2-e481-fdafc9dff857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "files = glob.glob(DAT_DIR+'*')\n",
        "for f in files:\n",
        "    print(f)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/src_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/labels_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/segs_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/clss_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/attn_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/mask_cls_train.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/src_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/labels_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/segs_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/clss_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/attn_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/mask_cls_validation.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/src_test.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/labels_test.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/segs_test.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/clss_test.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/attn_test.pt\n",
            "/content/drive/My Drive/summ_data/ICSI_plus_NXT/tensors/mask_cls_test.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6KelvDx1GoU"
      },
      "source": [
        "## Encoder classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCiy60801GoU"
      },
      "source": [
        "class Bert(nn.Module):\n",
        "    def __init__(self, temp_dir=\"/tmp/bert\", load_pretrained_bert=True, bert_config=None):\n",
        "        super(Bert, self).__init__()\n",
        "        print(temp_dir)\n",
        "        if(load_pretrained_bert):\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        else:\n",
        "            self.model = BertModel(bert_config)\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        # the below returns a tuple. First element in the tuple is last hidden state. Second element in tuple is pooler output\n",
        "        result = self.model(x, attention_mask =mask, position_ids=segs)\n",
        "#        top_vec = encoded_layers[-1]\n",
        "\n",
        "        top_vec = result[0]\n",
        "        return top_vec\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask_cls):\n",
        "        h = self.linear1(x)\n",
        "        h = h.squeeze(-1)\n",
        "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args=None, num_hidden = 768, load_pretrained_bert = True, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.bert = Bert( load_pretrained_bert, bert_config=None)\n",
        "#        if (args.encoder == 'classifier'):\n",
        "        self.encoder = Classifier(num_hidden)\n",
        "\n",
        "    def load_cp(self, pt):\n",
        "        self.load_state_dict(pt['model'], strict=True)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "        top_vec = self.bert(x, segs, mask)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8jovkq1GoX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s9QAJzC1GoX",
        "outputId": "025bd361-1c3a-4cfc-ff5d-714bb9b953e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print('GPU device not found')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCaPKGs1GoZ",
        "outputId": "a32954d0-4c5a-488a-cca2-43843347f6ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFJE6ndv1Goc"
      },
      "source": [
        "train_d = dict()\n",
        "val_d = dict()\n",
        "test_d=dict()\n",
        "\n",
        "for d_set in (\"src\", \"labels\", \"segs\", 'clss', \"attn\", \"mask_cls\"):\n",
        "    train_d[d_set] = torch.load(DAT_DIR + d_set+\"_\"+\"train.pt\")\n",
        "    val_d[d_set] = torch.load(DAT_DIR + d_set+\"_\"+\"validation.pt\")\n",
        "    test_d[d_set] = torch.load(DAT_DIR + d_set+\"_\"+\"test.pt\")\n",
        "\n",
        "train_dataset = TensorDataset(train_d[\"src\"],train_d[\"labels\"], train_d[\"segs\"], \n",
        "                              train_d[\"clss\"], train_d[\"attn\"], train_d[\"mask_cls\"])\n",
        "val_dataset = TensorDataset(val_d[\"src\"], val_d[\"labels\"], val_d[\"segs\"], \n",
        "                              val_d[\"clss\"], val_d[\"attn\"], val_d[\"mask_cls\"])\n",
        "test_dataset = TensorDataset(test_d[\"src\"], test_d[\"labels\"], test_d[\"segs\"], \n",
        "                              test_d[\"clss\"], test_d[\"attn\"], test_d[\"mask_cls\"])\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYNj9BO1Gog"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsd1R0hj1Goi"
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxyCiu4H1Gok",
        "outputId": "212c32e1-f443-4017-c258-53d34ee56f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc54d039af214ac7b44eec55ea62e367",
            "43bbbbef687b4bce839e1c133b9a3b2d",
            "fdda9480f5c34afab56ac1f44356511e",
            "605497209ed24e28829d35c8cbba7b26",
            "1878e8261ebc48f799c42b082c937d4f",
            "cf7e1f1e840f4abca2f38f871d7ae605",
            "e16f0c3a74dc4283ac703014d01f561d",
            "97492f4b1a5a43b586da8989c87014f9",
            "5343c08215414811ab49ada775fd7dbc",
            "cac749510b3a42daa195a2234c7bbf0e",
            "ade843f9d2a44dcf8844d1842cb4889e",
            "52315ea3f50c40cfa57c0583c60b60d0",
            "1001cbc33f2a4563957615288a8fe097",
            "5c5fb826cf9c46e7a7a82c65e2089f48",
            "0918d7b224014852a85a61ad2d1d2c0c",
            "890d0baa01aa4cdb883c74394724c6d3"
          ]
        }
      },
      "source": [
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "model = Summarizer()\n",
        "# tell pytorch to run on GPU\n",
        "torch.cuda.empty_cache()\n",
        "model.cuda()\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc54d039af214ac7b44eec55ea62e367",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5343c08215414811ab49ada775fd7dbc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Summarizer(\n",
              "  (bert): Bert(\n",
              "    (model): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): Classifier(\n",
              "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGytz8Jj1Gol",
        "outputId": "e87301bd-6cb8-4058-d891-cb410a57cf79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad=True\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.model.embeddings.word_embeddings.weight            (30522, 768)\n",
            "bert.model.embeddings.position_embeddings.weight          (512, 768)\n",
            "bert.model.embeddings.token_type_embeddings.weight          (2, 768)\n",
            "bert.model.embeddings.LayerNorm.weight                        (768,)\n",
            "bert.model.embeddings.LayerNorm.bias                          (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.model.encoder.layer.0.attention.self.query.weight    (768, 768)\n",
            "bert.model.encoder.layer.0.attention.self.query.bias          (768,)\n",
            "bert.model.encoder.layer.0.attention.self.key.weight      (768, 768)\n",
            "bert.model.encoder.layer.0.attention.self.key.bias            (768,)\n",
            "bert.model.encoder.layer.0.attention.self.value.weight    (768, 768)\n",
            "bert.model.encoder.layer.0.attention.self.value.bias          (768,)\n",
            "bert.model.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
            "bert.model.encoder.layer.0.attention.output.dense.bias        (768,)\n",
            "bert.model.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "bert.model.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "bert.model.encoder.layer.0.intermediate.dense.weight     (3072, 768)\n",
            "bert.model.encoder.layer.0.intermediate.dense.bias           (3072,)\n",
            "bert.model.encoder.layer.0.output.dense.weight           (768, 3072)\n",
            "bert.model.encoder.layer.0.output.dense.bias                  (768,)\n",
            "bert.model.encoder.layer.0.output.LayerNorm.weight            (768,)\n",
            "bert.model.encoder.layer.0.output.LayerNorm.bias              (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.model.pooler.dense.weight                            (768, 768)\n",
            "bert.model.pooler.dense.bias                                  (768,)\n",
            "encoder.linear1.weight                                      (1, 768)\n",
            "encoder.linear1.bias                                            (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLohVl61Gon"
      },
      "source": [
        "## set all layers to train\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad=True"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j01a9eZP1Gop"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLQW2YaQ1Gos"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhetRFG1Gov"
      },
      "source": [
        "This function writes out the labeled data as well as the result for evaluation using rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2is8hx6W1Gov"
      },
      "source": [
        "def gen_outputs(batch_id, probs, labels, cls_ids, mask_cls,src):\n",
        "    # extract sentences and labels\n",
        "#    print (probs)\n",
        "#    probs = np.where(probs>=0.12, 1, 0)\n",
        "    probs_bin = np.where(probs>=0.12, 1, 0)\n",
        "    reference = []\n",
        "    result = []\n",
        "    for p, passage in enumerate(src):\n",
        "#        print(probs[p,])\n",
        "        lines = tokenizer.decode(passage)\n",
        "        lines = lines.split(\"[SEP]\")\n",
        "        for i, sent in enumerate(lines):\n",
        "            sent = sent.replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").replace(\"[PAD]\", \"\")\n",
        "            with open(RESULT_DIR+\"PRED\"+batch_id+\".txt\", \"a\") as ref, open(RESULT_DIR+\"LAB\"+batch_id+\".txt\", \"a\") as lab, open(RESULT_DIR+\"GOOD\"+batch_id+\".txt\", \"a\") as good, open(RESULT_DIR+\"BAD\"+batch_id+\".txt\", \"a\") as bad:\n",
        "                if labels[p, i] == 1:\n",
        "#                    print(\"LABEL\", \"p=\", p, \"i=\", i, probs[p, i], labels[p, i], sent)\n",
        "                    reference.append(sent)\n",
        "                    lab.write(sent + \"\\n\")\n",
        "                if probs_bin[p, i] == 1:\n",
        "#                    print(\"PRED\", \"p=\", p, \"i=\", i, probs[p, i], labels[p, i], sent)\n",
        "                    result.append(sent)\n",
        "                    ref.write(sent + \"\\n\")\n",
        "                if labels[p, i] == 1 and probs_bin[p, i] == 1:\n",
        "                    good.write(str(probs[p, :np.argmin(mask_cls[p,])]) +\"|\"+ sent + \"\\n\")\n",
        "                if labels[p, i] != probs_bin[p, i]:\n",
        "                    bad.write(\"perd = \"+str(probs[p, :np.argmin(mask_cls[p,])]) +\"|\"+ \"label = \"+str(labels[p, i]) +\"|\"+sent + \"\\n\")\n",
        "    return reference, result # just return for debugging\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEP-_5J1Gox"
      },
      "source": [
        "## setup loss function and other variables required for training\n",
        "\n",
        "#seed_val = 42\n",
        "\n",
        "#random.seed(seed_val)\n",
        "#np.random.seed(seed_val)\n",
        "#torch.manual_seed(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "loss_c = torch.nn.BCELoss(reduction='none')\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZLUyra1Goz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmBU9JRG1Goz",
        "outputId": "4e3e4378-3ecc-4c75-b475-cf24684b9fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        \n",
        "        src, labels, segs, clss, attn, mask_cls = batch\n",
        "        src, labels, segs, clss, attn, mask_cls = src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "#        x, segs, clss, mask, mask_cls, sentence_range=None\n",
        "        probs, mask_cls = model( src, segs, clss, attn, mask_cls)\n",
        "        loss = loss_c(probs, labels.float())\n",
        "        loss = (loss * attn.float()).sum()\n",
        "\n",
        "        probs = probs.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        #gen_outputs(\"TBATCH\"+str(step), probs, labels, clss.to(device), mask_cls.to(device), src.to(device))\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    104.    Elapsed: 0:01:09.\n",
            "\n",
            "  Average training loss: 179.14\n",
            "  Training epcoh took: 0:01:29\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:34.\n",
            "  Batch    80  of    104.    Elapsed: 0:01:09.\n",
            "\n",
            "  Average training loss: 170.10\n",
            "  Training epcoh took: 0:01:29\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:34.\n",
            "  Batch    80  of    104.    Elapsed: 0:01:09.\n",
            "\n",
            "  Average training loss: 168.96\n",
            "  Training epcoh took: 0:01:29\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    104.    Elapsed: 0:00:34.\n",
            "  Batch    80  of    104.    Elapsed: 0:01:09.\n",
            "\n",
            "  Average training loss: 165.74\n",
            "  Training epcoh took: 0:01:29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZSObMb6BMyg",
        "outputId": "afe8d8f7-e697-4cc5-a1f5-07231066b376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = BASE_DIR+'data/model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "torch.save(model.state_dict(), output_dir+\"bertsum_classifier\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/summ_data/data/model_save/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6qavqcP-Xe1"
      },
      "source": [
        "if EVAL:\n",
        "  output_dir = BASE_DIR+'data/model_save/'\n",
        "  model = Summarizer()\n",
        "  model.cuda()\n",
        "\n",
        "  model.load_state_dict(torch.load(output_dir+\"bertsum_classifier\"))\n",
        "  model.eval()\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3DCsxum1Go1"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zclE7Gr51Go2",
        "outputId": "5dff8cb3-a4e1-43ef-f14d-1e632d3cec26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ========================================\n",
        "#               Validation\n",
        "# ========================================\n",
        "# After the completion of each training epoch, measure our performance on\n",
        "# our validation set.\n",
        "files = glob.glob(RESULT_DIR+'*')\n",
        "for f in files:\n",
        "    os.remove(f)\n",
        "    #print(f)\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Running Validation...\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Put the model in evaluation mode--the dropout layers behave differently\n",
        "# during evaluation.\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "step = -1\n",
        "for batch in validation_dataloader:\n",
        "#for batch in train_dataloader:\n",
        "    step += 1\n",
        "    # Unpack this training batch from our dataloader. \n",
        "    #\n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "    # the `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids \n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels \n",
        "    src, labels, segs, clss, attn, mask_cls = batch\n",
        "    src, labels, segs, clss, attn, mask_cls = src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "    \n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():        \n",
        "        probs, mask_cls = model( src, segs, clss, attn, mask_cls)\n",
        "        loss = loss_c(probs, labels.float())\n",
        "        loss = (loss * attn.float()).sum()\n",
        "\n",
        "        probs = probs.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "        \n",
        "    # Accumulate the validation loss.\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    #print(type(logits), type(b_labels))\n",
        "    \n",
        "#    probs = probs.numpy() #logits.detach().cpu().numpy()\n",
        "#    label_ids = labels.numpy() #b_labels.to('cpu').numpy()\n",
        "\n",
        "    # write results so that we can use rouge to compare\n",
        "    gen_outputs(\"BATCH\"+str(step), probs, labels, clss.to('cpu').numpy(), mask_cls.to('cpu').numpy(), src.to('cpu').numpy())\n",
        "    \n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "\n",
            "Training complete!\n",
            "Total training took 1:05:14 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBcKOFZLOEPq"
      },
      "source": [
        "## Run ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WEydXooOFq9",
        "outputId": "8c7cc15d-3768-4672-aaa9-ddd66f8e381e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from rouge import Rouge\n",
        "scores = list()\n",
        "for i in range(step):\n",
        "  hyp_file = RESULT_DIR+\"PREDBATCH\"+str(i)+\".txt\"\n",
        "  lab_file = RESULT_DIR+\"LABBATCH\"+str(i)+\".txt\"\n",
        "  with open(hyp_file, \"r\") as ref, open(lab_file, \"r\") as lab:\n",
        "    reference = lab.read()\n",
        "    hypothesis = ref.read()\n",
        "    rouge = Rouge()\n",
        "    try:\n",
        "      score = rouge.get_scores(hypothesis, reference)\n",
        "      scores.append(score)\n",
        "    except:\n",
        "      print(\"empty file\", hyp_file, lab_file)\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH1.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH1.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH2.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH2.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH3.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH3.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH4.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH4.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH5.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH5.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH8.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH8.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH17.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH17.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH18.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH18.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH19.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH19.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH22.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH22.txt\n",
            "empty file /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/PREDBATCH26.txt /content/drive/My Drive/summ_data/ICSI_plus_NXT/result/LABBATCH26.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc7LM1JVQNDP"
      },
      "source": [
        "# average rouge scores\n",
        "result_rouge = None\n",
        "for rouge_res in scores:\n",
        "  if result_rouge is None:\n",
        "    result_rouge = rouge_res[0]\n",
        "    continue\n",
        "  for key, obj in rouge_res[0].items():\n",
        "    for k1, v1 in obj.items():\n",
        "      result_rouge[key][k1] += v1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPpM-pYVQOOv",
        "outputId": "9b3c3d8c-3fa5-46af-bbf3-0e6ed0bd38a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "res = scores[0][0] # just for initializing keys. Values do not matter here\n",
        "\n",
        "for k, v in result_rouge.items():\n",
        "  for k1, v1 in v.items():\n",
        "    print(k,k1)\n",
        "    res[k][k1] = v1/len(scores)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rouge-1 f\n",
            "rouge-1 p\n",
            "rouge-1 r\n",
            "rouge-2 f\n",
            "rouge-2 p\n",
            "rouge-2 r\n",
            "rouge-l f\n",
            "rouge-l p\n",
            "rouge-l r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKuKOhuhUWpG",
        "outputId": "cfcfcf91-60b9-4a64-a171-feb5856b445a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "res"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.5589966934672524,\n",
              "  'p': 0.535628170863283,\n",
              "  'r': 0.6619463567538667},\n",
              " 'rouge-2': {'f': 0.31210914975002946,\n",
              "  'p': 0.29030536529177686,\n",
              "  'r': 0.384693307936061},\n",
              " 'rouge-l': {'f': 0.5042695938844134,\n",
              "  'p': 0.49460063200514076,\n",
              "  'r': 0.5481617113989556}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAPu1cvfczC6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}