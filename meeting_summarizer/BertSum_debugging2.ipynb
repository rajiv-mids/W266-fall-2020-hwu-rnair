{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "BertSum debugging2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgxF_r0W1GoF",
        "outputId": "9e36e995-d3b3-4adb-c176-496a37671601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXTqE-o41GoI",
        "outputId": "4018307b-d9f8-4827-e745-8e1b60772106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqPukq8-7p9"
      },
      "source": [
        "EVAL = False # set if running in eval only mode"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYjxy07C1GoM",
        "outputId": "73ea221f-5dc1-4cb5-b11a-112f0d0ff637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbEHmTwH2Zil"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import datetime, time\n",
        "import io\n",
        "import re\n",
        "from csv import reader\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "import glob, os\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertModel, AdamW, BertConfig,BertTokenizer\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "import tensorflow as tf\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "from rouge import Rouge \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSLt1fDx23xu"
      },
      "source": [
        "## Create result Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aws8nctQ1GoO"
      },
      "source": [
        "BASE_DIR = \"/content/drive/My Drive/summ_data/\"\n",
        "MODEL_INPUT_DIR = BASE_DIR+\"ICSI_plus_NXT/toy_tensor/\"\n",
        "RESULT_DIR = BASE_DIR+\"ICSI_plus_NXT/toy_result/\"\n",
        "TOY_DIR = BASE_DIR+\"ICSI_plus_NXT/toy/\"\n",
        "for d in (MODEL_INPUT_DIR, RESULT_DIR, TOY_DIR):\n",
        "  if not os.path.exists(d):\n",
        "      os.makedirs(d)\n",
        "  files = glob.glob(d+'*')\n",
        "  for f in files:\n",
        "    os.remove(f)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6KelvDx1GoU"
      },
      "source": [
        "## Encoder classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCiy60801GoU"
      },
      "source": [
        "class Bert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bert, self).__init__()\n",
        "        self.model = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        # the below returns a tuple. First element in the tuple is last hidden state. Second element in tuple is pooler output\n",
        "        result = self.model(x, attention_mask =mask, position_ids=segs)\n",
        "        top_vec = result[0]\n",
        "        return top_vec\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask_cls):\n",
        "        h = self.linear1(x)\n",
        "        h = h.squeeze(-1)\n",
        "        x2 = x.clone().detach().cpu().numpy()\n",
        "        print(\"X (CLS) totals = \", np.sum(x2[0,:,:][:20], axis=1))\n",
        "        print(\"Logit = \", h[0,:10])\n",
        "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
        "        return sent_scores, x, h\n",
        "\n",
        "\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args=None, num_hidden = 768, load_pretrained_bert = True, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.bert = Bert()\n",
        "#        if (args.encoder == 'classifier'):\n",
        "        self.encoder = Classifier(num_hidden)\n",
        "        for p in self.encoder.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "        top_vec = self.bert(x, segs, mask)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores, x, h = self.encoder(sents_vec, mask_cls)\n",
        "        sent_scores = sent_scores.squeeze(-1)\n",
        "        return sent_scores, mask_cls, x, h\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8jovkq1GoX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s9QAJzC1GoX",
        "outputId": "03ee255b-17ba-4828-f254-70e10c63c81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    print('GPU device not found')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCaPKGs1GoZ",
        "outputId": "fb2d6f4d-2cb7-4f22-d145-659fea65e4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhjA1Tqa3aF"
      },
      "source": [
        "## Toy train and Validation datasets. here the model is trying to predict sentences that have upper case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZF22DEhay7H"
      },
      "source": [
        "dat_files = [\"\"\"The steel photographs the nutty amusement. The miniature plant publicizes the swim. The voiceless interest publicizes the blood. The striped industry logs the attraction. The rate scans the time. The authority launchs the yellow development.The part counsels the cultured wine.The thing familiarizes the detail. Did the neighborly interaction really dance the perspective. The scientific location can't return the trash\"\"\",\n",
        "             \"\"\"She did her best to help him. Flesh-colored yoga pants were far worse than even he feared. He dreamed of eating green apples with worms. Most shark attacks occur about 10 feet from the beach since that's where the people are. He hated that he loved what she hated about hate. The Guinea fowl flies through the air with all the grace of a turtle.If eating three-egg omelets causes weight-gain, budgie eggs are a good substitute.\"\"\",\n",
        "             \"\"\"The zippy mind starts the market. The wary value creates the event. The tangy language quantifys the liquid. The tacit food relates the market. The point packages the somber smash. The scattered burst authorizes the step. The hearty double can't blink the tour. It was then the ripe expert met the accidental energy.\"\"\"]\n",
        "\n",
        "for fi, f in enumerate(dat_files):\n",
        "  with open(TOY_DIR+\"M\"+str(fi)+\".txt\", \"w\") as out:\n",
        "    dat = f.lower()\n",
        "    dat = f.split(\".\")\n",
        "    labels = np.random.randint(2, size=len(dat))\n",
        "    for i, label in enumerate(labels):\n",
        "      if dat[i] is None or dat[i].strip() == \"\":\n",
        "        continue\n",
        "      if label == 0:\n",
        "        out.write(dat[i].lower() + \"|\"+ str(label)+ \"\\n\")\n",
        "      else:\n",
        "        out.write(dat[i].upper() +  \"|\"+ str(label)+ \"\\n\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMy84ecmZYPg"
      },
      "source": [
        "### Construct the input tokens as specified in the bertsum paper https://arxiv.org/pdf/1903.10318.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5vdU8MMiTHj",
        "outputId": "175fcd4c-dc3a-430b-b162-cd6d63687702",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cls_vid = tokenizer.vocab[\"[CLS]\"]\n",
        "sep_vid = tokenizer.vocab[\"[SEP]\"]\n",
        "\n",
        "class BertDataProcessor:\n",
        "    def __init__(self, data_dir, out_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.out_dir = out_dir\n",
        "        self.inp_df = None\n",
        "        self.meetings = set()\n",
        "\n",
        "        for inp_file in sorted(glob.glob(self.data_dir+\"/*txt\")):\n",
        "            f_name = os.path.basename(inp_file)\n",
        "            self.meetings.add(f_name.split(\".\")[0])\n",
        "        if not os.path.exists(self.out_dir):\n",
        "            os.makedirs(self.out_dir)\n",
        "\n",
        "    def split(self):\n",
        "        '''\n",
        "        train, validation and test split by meeting\n",
        "        '''\n",
        "        meetings = list(self.meetings)\n",
        "        self.train_list = meetings[:2]\n",
        "        self.validation_list = meetings[2:]\n",
        "\n",
        "    def tokenize(self, chunk, labels):\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(chunk)\n",
        "      attn_masks = [1]*len(input_ids)\n",
        "      cls_ids = [i for i, t in enumerate(input_ids) if t == cls_vid ]\n",
        "      mask_cls = [1 for _ in range(len(cls_ids))]\n",
        "\n",
        "      [attn_masks.append(0) for _ in range(len(attn_masks), 512)]\n",
        "      [input_ids.append(0) for _ in range(len(input_ids), 512)]\n",
        "      [cls_ids.append(0) for _ in range(len(cls_ids), 512)]\n",
        "      [mask_cls.append(0) for _ in range(len(mask_cls), 512)]\n",
        "\n",
        "      _segs = [-1] + [i for i, t in enumerate(input_ids) if t == sep_vid]\n",
        "      segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "      segments_ids = []\n",
        "      for i, s in enumerate(segs):\n",
        "          if (i % 2 == 0):\n",
        "              segments_ids += s * [0]\n",
        "          else:\n",
        "              segments_ids += s * [1]\n",
        "      [labels.append(0) for _ in range(len(labels), 512)]\n",
        "      [segments_ids.append(0) for _ in range(len(segments_ids), 512)]\n",
        "      print(\"input_ids\", input_ids)\n",
        "      print(\"attn_masks\", attn_masks)\n",
        "      print(\"segments_ids\", segments_ids)\n",
        "      print(\"cls_ids\", cls_ids)\n",
        "      print(\"labels\", labels)\n",
        "      print(\"mask_cls\", mask_cls)\n",
        "      b_data_dict = {\"src\": input_ids, \"labels\": labels, \"segs\": segments_ids, \n",
        "                  'clss': cls_ids, \"attn\": attn_masks, \"mask_cls\":mask_cls}\n",
        "      return b_data_dict\n",
        "\n",
        "    def format_to_bert(self, args=None):\n",
        "        for batch, f_names in ((\"train\", self.train_list), (\"validation\",self.validation_list)):\n",
        "            output = []\n",
        "            for f_name in f_names:\n",
        "                # process a meeting at a time\n",
        "                pth = self.data_dir+\"/\"+f_name+\".\"+\"*txt\"\n",
        "                cur_chunk = None\n",
        "                cur_labels = []\n",
        "                for inp_file in sorted(glob.glob(pth)):\n",
        "                    with open(inp_file, \"r\") as mtg_f:\n",
        "                        for line in mtg_f:\n",
        "                            sent, label = line.split(\"|\")\n",
        "                            s_chunk = tokenizer.tokenize(sent) [:510]\n",
        "                            s_chunk = [\"[CLS]\"] + s_chunk + [\"[SEP]\"]\n",
        "\n",
        "                            if cur_chunk is None:\n",
        "                                cur_chunk = s_chunk\n",
        "                                cur_labels.append(int(label))\n",
        "                            else:\n",
        "                                # if new line fits in to remaining space, add it, else fill with spaces and add a new line\n",
        "                                if len(s_chunk) + len(cur_chunk) < 512:\n",
        "                                    cur_chunk += s_chunk\n",
        "                                    cur_labels.append(int(label))\n",
        "                                else:\n",
        "                                    print(\"\\ninput: \",cur_chunk)\n",
        "                                    b_data_dict = self.tokenize(cur_chunk, cur_labels)\n",
        "                                    output.append(b_data_dict)\n",
        "                                    cur_chunk = s_chunk\n",
        "                                    cur_labels = [int(label)]\n",
        "                        # handle last sentence\n",
        "                        if cur_chunk is not None and len(cur_chunk) > 0:\n",
        "                            print(\"\\ninput: \",cur_chunk)\n",
        "                            b_data_dict = self.tokenize(cur_chunk, cur_labels)\n",
        "                            output.append(b_data_dict)\n",
        "\n",
        "            out =  {\"src\": [], \"labels\": [], \"segs\": [], \n",
        "                                                'clss': [], \"attn\": [], \"mask_cls\":[]}\n",
        "            for sample in output:\n",
        "                for key, val in sample.items():\n",
        "                    out[key].append(val)\n",
        "            for k, v in out.items():\n",
        "                out[k] = torch.LongTensor(v)\n",
        "            for k, v in out.items():\n",
        "                torch.save(v, self.out_dir+\"/\"+k+\"_\"+batch+\".pt\")\n",
        "\n",
        "dp = BertDataProcessor(TOY_DIR, MODEL_INPUT_DIR)\n",
        "dp.split()\n",
        "dp.format_to_bert()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "input:  ['[CLS]', 'the', 'steel', 'photographs', 'the', 'nut', '##ty', 'amusement', '[SEP]', '[CLS]', 'the', 'miniature', 'plant', 'public', '##izes', 'the', 'swim', '[SEP]', '[CLS]', 'the', 'voice', '##less', 'interest', 'public', '##izes', 'the', 'blood', '[SEP]', '[CLS]', 'the', 'striped', 'industry', 'logs', 'the', 'attraction', '[SEP]', '[CLS]', 'the', 'rate', 'scan', '##s', 'the', 'time', '[SEP]', '[CLS]', 'THE', 'AU', '##TH', '##OR', '##IT', '##Y', 'LA', '##UN', '##CH', '##S', 'THE', 'Y', '##EL', '##L', '##OW', 'DE', '##VE', '##L', '##OP', '##ME', '##NT', '[SEP]', '[CLS]', 'THE', 'PA', '##RT', 'CO', '##UN', '##SE', '##LS', 'THE', 'C', '##U', '##LT', '##UR', '##ED', 'W', '##IN', '##E', '[SEP]', '[CLS]', 'the', 'thing', 'familiar', '##izes', 'the', 'detail', '[SEP]', '[CLS]', 'D', '##ID', 'THE', 'NE', '##IG', '##H', '##BO', '##RL', '##Y', 'IN', '##TE', '##RA', '##CT', '##ION', 'R', '##EA', '##LL', '##Y', 'D', '##AN', '##CE', 'THE', 'P', '##ER', '##SP', '##EC', '##TI', '##VE', '[SEP]', '[CLS]', 'THE', 'SC', '##IE', '##NT', '##IF', '##IC', 'L', '##OC', '##AT', '##ION', 'CA', '##N', \"'\", 'T', 'R', '##ET', '##UR', '##N', 'THE', 'T', '##RA', '##S', '##H', '[SEP]']\n",
            "input_ids [101, 1103, 3649, 6810, 1103, 22664, 2340, 10367, 102, 101, 1103, 14547, 2582, 1470, 9534, 1103, 11231, 102, 101, 1103, 1490, 2008, 2199, 1470, 9534, 1103, 1892, 102, 101, 1103, 20399, 2380, 16959, 1103, 8322, 102, 101, 1103, 2603, 14884, 1116, 1103, 1159, 102, 101, 7462, 21646, 24162, 9565, 12150, 3663, 10722, 27370, 23258, 1708, 7462, 162, 21678, 2162, 17056, 18581, 17145, 2162, 17195, 14424, 15681, 102, 101, 7462, 8544, 10460, 18732, 27370, 12649, 15928, 7462, 140, 2591, 26909, 19556, 10069, 160, 11607, 2036, 102, 101, 1103, 1645, 4509, 9534, 1103, 6505, 102, 101, 141, 9949, 7462, 26546, 23413, 3048, 23904, 20550, 3663, 15969, 12880, 9664, 16647, 24805, 155, 12420, 23955, 3663, 141, 14962, 10954, 7462, 153, 9637, 15735, 8231, 21669, 17145, 102, 101, 7462, 9314, 17444, 15681, 15499, 9741, 149, 9244, 13821, 24805, 8784, 2249, 112, 157, 155, 11943, 19556, 2249, 7462, 157, 9664, 1708, 3048, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attn_masks [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segments_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "cls_ids [0, 9, 18, 28, 36, 44, 67, 85, 93, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "mask_cls [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "input:  ['[CLS]', 'she', 'did', 'her', 'best', 'to', 'help', 'him', '[SEP]', '[CLS]', 'flesh', '-', 'colored', 'yoga', 'pants', 'were', 'far', 'worse', 'than', 'even', 'he', 'feared', '[SEP]', '[CLS]', 'H', '##E', 'DR', '##EA', '##ME', '##D', 'OF', 'EA', '##TI', '##NG', 'G', '##RE', '##EN', 'AP', '##PL', '##ES', 'W', '##IT', '##H', 'W', '##OR', '##MS', '[SEP]', '[CLS]', 'most', 'shark', 'attacks', 'occur', 'about', '10', 'feet', 'from', 'the', 'beach', 'since', 'that', \"'\", 's', 'where', 'the', 'people', 'are', '[SEP]', '[CLS]', 'H', '##E', 'H', '##AT', '##ED', 'T', '##HA', '##T', 'H', '##E', 'L', '##O', '##VE', '##D', 'W', '##HA', '##T', 'SH', '##E', 'H', '##AT', '##ED', 'AB', '##O', '##UT', 'H', '##AT', '##E', '[SEP]', '[CLS]', 'THE', 'G', '##UI', '##NE', '##A', 'F', '##OW', '##L', 'FL', '##IE', '##S', 'T', '##H', '##RO', '##U', '##G', '##H', 'THE', 'AI', '##R', 'W', '##IT', '##H', 'AL', '##L', 'THE', 'G', '##RA', '##CE', 'OF', 'A', 'T', '##UR', '##TL', '##E', '[SEP]', '[CLS]', 'if', 'eating', 'three', '-', 'egg', 'o', '##mel', '##ets', 'causes', 'weight', '-', 'gain', ',', 'b', '##ud', '##gie', 'eggs', 'are', 'a', 'good', 'substitute', '[SEP]']\n",
            "input_ids [101, 1131, 1225, 1123, 1436, 1106, 1494, 1140, 102, 101, 5352, 118, 6805, 20631, 6023, 1127, 1677, 4146, 1190, 1256, 1119, 8253, 102, 101, 145, 2036, 22219, 12420, 14424, 2137, 11345, 23616, 21669, 11780, 144, 16941, 11680, 10997, 27258, 9919, 160, 12150, 3048, 160, 9565, 7182, 102, 101, 1211, 15623, 3690, 4467, 1164, 1275, 1623, 1121, 1103, 4640, 1290, 1115, 112, 188, 1187, 1103, 1234, 1132, 102, 101, 145, 2036, 145, 13821, 10069, 157, 11612, 1942, 145, 2036, 149, 2346, 17145, 2137, 160, 11612, 1942, 17730, 2036, 145, 13821, 10069, 16151, 2346, 16830, 145, 13821, 2036, 102, 101, 7462, 144, 22054, 22680, 1592, 143, 17056, 2162, 23485, 17444, 1708, 157, 3048, 21564, 2591, 2349, 3048, 7462, 19016, 2069, 160, 12150, 3048, 18589, 2162, 7462, 144, 9664, 10954, 11345, 138, 157, 19556, 20156, 2036, 102, 101, 1191, 5497, 1210, 118, 9069, 184, 10212, 6248, 4680, 2841, 118, 4361, 117, 171, 4867, 9747, 6471, 1132, 170, 1363, 7359, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attn_masks [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segments_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "cls_ids [0, 9, 23, 47, 67, 97, 134, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "mask_cls [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "input:  ['[CLS]', 'the', 'z', '##ip', '##py', 'mind', 'starts', 'the', 'market', '[SEP]', '[CLS]', 'the', 'wary', 'value', 'creates', 'the', 'event', '[SEP]', '[CLS]', 'THE', 'T', '##AN', '##G', '##Y', 'LA', '##NG', '##U', '##AG', '##E', 'Q', '##U', '##AN', '##TI', '##F', '##Y', '##S', 'THE', 'L', '##I', '##Q', '##UI', '##D', '[SEP]', '[CLS]', 'THE', 'T', '##AC', '##IT', 'F', '##O', '##OD', 'R', '##EL', '##AT', '##ES', 'THE', 'MA', '##R', '##KE', '##T', '[SEP]', '[CLS]', 'the', 'point', 'packages', 'the', 'so', '##mber', 'smash', '[SEP]', '[CLS]', 'THE', 'SC', '##AT', '##TE', '##RE', '##D', 'B', '##UR', '##ST', 'AU', '##TH', '##OR', '##I', '##Z', '##ES', 'THE', 'ST', '##EP', '[SEP]', '[CLS]', 'the', 'heart', '##y', 'double', 'can', \"'\", 't', 'blink', 'the', 'tour', '[SEP]', '[CLS]', 'IT', 'WA', '##S', 'THE', '##N', 'THE', 'R', '##IP', '##E', 'E', '##X', '##P', '##ER', '##T', 'ME', '##T', 'THE', 'ACC', '##ID', '##EN', '##TA', '##L', 'E', '##NE', '##R', '##G', '##Y', '[SEP]']\n",
            "input_ids [101, 1103, 195, 9717, 5005, 1713, 3816, 1103, 2319, 102, 101, 1103, 16970, 2860, 8743, 1103, 1856, 102, 101, 7462, 157, 14962, 2349, 3663, 10722, 11780, 2591, 22689, 2036, 154, 2591, 14962, 21669, 2271, 3663, 1708, 7462, 149, 2240, 4880, 22054, 2137, 102, 101, 7462, 157, 8101, 12150, 143, 2346, 15609, 155, 21678, 13821, 9919, 7462, 9960, 2069, 22441, 1942, 102, 101, 1103, 1553, 15611, 1103, 1177, 10615, 24881, 102, 101, 7462, 9314, 13821, 12880, 16941, 2137, 139, 19556, 9272, 21646, 24162, 9565, 2240, 5301, 9919, 7462, 23676, 16668, 102, 101, 1103, 1762, 1183, 2702, 1169, 112, 189, 14119, 1103, 2465, 102, 101, 9686, 22751, 1708, 7462, 2249, 7462, 155, 11410, 2036, 142, 3190, 2101, 9637, 1942, 22157, 1942, 7462, 18396, 9949, 11680, 9159, 2162, 142, 22680, 2069, 2349, 3663, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attn_masks [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segments_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "cls_ids [0, 10, 18, 43, 61, 70, 90, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels [0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "mask_cls [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFJE6ndv1Goc"
      },
      "source": [
        "train_d = dict()\n",
        "val_d = dict()\n",
        "\n",
        "for d_set in (\"src\", \"labels\", \"segs\", 'clss', \"attn\", \"mask_cls\"):\n",
        "    train_d[d_set] = torch.load(MODEL_INPUT_DIR + d_set+\"_\"+\"train.pt\")\n",
        "    val_d[d_set] = torch.load(MODEL_INPUT_DIR + d_set+\"_\"+\"validation.pt\")\n",
        "\n",
        "train_dataset = TensorDataset(train_d[\"src\"],train_d[\"labels\"], train_d[\"segs\"], \n",
        "                              train_d[\"clss\"], train_d[\"attn\"], train_d[\"mask_cls\"])\n",
        "val_dataset = TensorDataset(val_d[\"src\"], val_d[\"labels\"], val_d[\"segs\"], \n",
        "                              val_d[\"clss\"], val_d[\"attn\"], val_d[\"mask_cls\"])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYNj9BO1Gog"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsd1R0hj1Goi"
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxyCiu4H1Gok"
      },
      "source": [
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "model = Summarizer()\n",
        "# tell pytorch to run on GPU\n",
        "torch.cuda.empty_cache()\n",
        "_=model.cuda()\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLohVl61Gon"
      },
      "source": [
        "## set all layers to train\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad=True\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=True"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j01a9eZP1Gop"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "#optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLQW2YaQ1Gos"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhetRFG1Gov"
      },
      "source": [
        "This function writes out the labeled data as well as the result for evaluation using rouge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEP-_5J1Gox"
      },
      "source": [
        "## setup loss function and other variables required for training\n",
        "\n",
        "#seed_val = 42\n",
        "\n",
        "#random.seed(seed_val)\n",
        "#np.random.seed(seed_val)\n",
        "#torch.manual_seed(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "loss_c = torch.nn.BCELoss(reduction='none').to(device)\n",
        "#loss_c = torch.nn.MSELoss(reduction='none').to(device)\n",
        "#loss_c = torch.nn.CrossEntropyLoss(reduction='none').to(device)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZLUyra1Goz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzv0yAPpJtM2",
        "outputId": "a4035816-19d5-462f-ac4a-bbac69500909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "files = glob.glob(RESULT_DIR+'*')\n",
        "for f in files:\n",
        "    os.remove(f)\n",
        "    #print(f)\n",
        "\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        \n",
        "        src, labels, segs, clss, attn, mask_cls = batch\n",
        "        src, labels, segs, clss, attn, mask_cls = src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "#        x, segs, clss, mask, mask_cls, sentence_range=None\n",
        "        probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
        "        print(\"TRAIN OUTPUTS:\", probs.shape, probs[:,:20])\n",
        "        print(\"TRAIN LABELS:\",labels.shape,labels[:,:20])\n",
        "        loss = loss_c(probs, labels.float())\n",
        "        print(\"TRAIN LOSS\",loss[:,:20])\n",
        "        loss = (loss * mask_cls.float()).sum()\n",
        "        print(\"TRAIN LOSS TOTAL\",loss)\n",
        "        \n",
        "        #probs = probs.detach().cpu().numpy()\n",
        "        #labels = labels.to('cpu').numpy()\n",
        "\n",
        "        #gen_outputs(\"TBATCH\"+str(step), probs, labels, clss.to(device), mask_cls.to(device), src.to(device))\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        (loss/loss.numel()).backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    for i in range(len(list(model.parameters()))):\n",
        "        #print(\"list(model.parameters())[0].grad = \", i, list(model.parameters())[i].grad)\n",
        "        pass\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    step = -1\n",
        "#    for batch in validation_dataloader:\n",
        "    for batch in train_dataloader:\n",
        "        step += 1\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        src, labels, segs, clss, attn, mask_cls = batch\n",
        "        src, labels, segs, clss, attn, mask_cls = src.to(device), labels.to(device), segs.to(device), clss.to(device), attn.to(device), mask_cls.to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            probs, mask_cls, x, h = model( src, segs, clss, attn, mask_cls)\n",
        "\n",
        "            print(\"VALIDATION OUTPUTS:\", probs.shape, probs[:,:20])\n",
        "            print(\"VALIDATION LABELS:\",labels.shape,labels[:,:20])\n",
        "\n",
        "            loss = loss_c(probs[:20], labels[:20].float())\n",
        "            print(\"VALIDATION LOSS\",loss[:,:20])\n",
        "            loss = (loss * attn.float()).sum()\n",
        "            print(\"VALIDATION LOSS TOTAL\",loss)\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        #print(type(logits), type(b_labels))\n",
        "        \n",
        "    #    probs = probs.numpy() #logits.detach().cpu().numpy()\n",
        "    #    label_ids = labels.numpy() #b_labels.to('cpu').numpy()\n",
        "\n",
        "    # write results so that we can use rouge to compare\n",
        "#        gen_outputs(\"BATCH\"+str(epoch_i)+\"_\"+str(step), probs, labels, clss.to('cpu').numpy(), mask_cls.to('cpu').numpy(), src.to('cpu').numpy())\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [ -3.8937967  -7.733228   -3.8720956  -6.1603384  -3.7517133  -3.1392064\n",
            " -12.254707    0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([-0.1639,  0.7479, -0.1173,  0.0648, -0.3375, -0.2434, -0.1251, -0.0013,\n",
            "        -0.0013, -0.0013], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.4591, 0.6787, 0.4707, 0.5162, 0.4164, 0.4394, 0.4688, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4428, 0.4988, 0.5365, 0.4800, 0.4196, 0.4312, 0.4643, 0.4871, 0.5507,\n",
            "         0.4861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.6146, 1.1354, 0.7535, 0.7261, 0.8761, 0.8222, 0.6326, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5848, 0.6908, 0.7690, 0.6539, 0.5440, 0.8413, 0.7673, 0.6677, 0.5966,\n",
            "         0.7212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(12.3970, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 12.40\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-3.3892896 -3.373177  -3.3892896 -3.373177  -3.3892896 -3.373177\n",
            " -3.3892896 -3.373177  -3.3892896 -3.373177   0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-2.7290, -2.6850, -2.7290, -2.6850, -2.7290, -2.6850, -2.7290, -2.6850,\n",
            "        -2.7290, -2.6850], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.0613, 0.0639, 0.0613, 0.0639, 0.0613, 0.0639, 0.0613, 0.0639, 0.0613,\n",
            "         0.0639, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0425, 0.0434, 0.0425, 0.0434, 0.0425, 0.0434, 0.0425, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.0632, 0.0660, 0.0632, 0.0660, 0.0632, 2.7510, 2.7923, 0.0660, 2.7923,\n",
            "         2.7510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0434, 0.0444, 3.1586, 0.0444, 3.1586, 3.1367, 0.0434, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(21.1038, device='cuda:0')\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-3.383206  -3.3777008 -4.3404956 -3.2435255 -3.3002882 -3.8120742\n",
            " -2.9699712 -3.1794581 -3.4431314 -3.5559254  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-2.8922, -3.3113, -3.4546, -2.7283, -2.5613, -3.3701, -2.9768, -3.2584,\n",
            "        -2.9786, -2.7588], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.0525, 0.0352, 0.0306, 0.0613, 0.0717, 0.0332, 0.0485, 0.0370, 0.0484,\n",
            "         0.0596, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0322, 0.0305, 0.0288, 0.0348, 0.0251, 0.0325, 0.0344, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.0540, 0.0358, 0.0311, 0.0633, 0.0744, 3.4040, 3.0265, 0.0377, 3.0282,\n",
            "         2.8202, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0328, 0.0310, 3.5470, 0.0354, 3.6841, 3.4256, 0.0350, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(23.3660, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 23.37\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-2.6445866 -2.6437755 -2.6445866 -2.6437755 -2.6445866 -2.6437755\n",
            " -2.6445866  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 5.7846e+00,  5.7098e+00,  5.7846e+00,  5.7098e+00,  5.7846e+00,\n",
            "         5.7098e+00,  5.7846e+00, -1.5714e-03, -1.5714e-03, -1.5714e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.9969, 0.9967, 0.9969, 0.9967, 0.9969, 0.9967, 0.9969, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9963, 0.9960, 0.9963, 0.9960, 0.9963, 0.9960, 0.9963, 0.9960, 0.9963,\n",
            "         0.9960, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[5.7876e+00, 5.7132e+00, 3.0699e-03, 5.7132e+00, 3.0699e-03, 3.3077e-03,\n",
            "         5.7876e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [5.6001e+00, 5.5153e+00, 5.6001e+00, 5.5153e+00, 5.6001e+00, 4.0328e-03,\n",
            "         3.7042e-03, 5.5153e+00, 3.7042e-03, 4.0328e-03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(56.3728, device='cuda:0')\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-2.632246  -2.7193317 -2.3857718 -2.3020935 -2.5218801 -2.665279\n",
            " -2.5283442  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 5.3765e+00,  5.6278e+00,  5.3985e+00,  5.5333e+00,  5.3612e+00,\n",
            "         5.1536e+00,  5.1716e+00, -1.5714e-03, -1.5714e-03, -1.5714e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.9954, 0.9964, 0.9955, 0.9961, 0.9953, 0.9943, 0.9944, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9971, 0.9980, 0.9973, 0.9956, 0.9956, 0.9967, 0.9961, 0.9977, 0.9956,\n",
            "         0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[5.3812e+00, 5.6314e+00, 4.5133e-03, 5.5373e+00, 4.6846e-03, 5.7620e-03,\n",
            "         5.1772e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [5.8487e+00, 6.1916e+00, 5.9011e+00, 5.4220e+00, 5.4295e+00, 3.3015e-03,\n",
            "         3.9529e-03, 6.0696e+00, 4.3773e-03, 5.0477e-03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(56.6212, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 56.62\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.107358  -1.1551414 -1.107358  -1.1551414 -1.107358  -1.1551414\n",
            " -1.107358  -1.1551414 -1.107358  -1.1551414  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([0.1958, 0.2222, 0.1958, 0.2222, 0.1958, 0.2222, 0.1958, 0.2222, 0.1958,\n",
            "        0.2222], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.5488, 0.5553, 0.5488, 0.5553, 0.5488, 0.5553, 0.5488, 0.5553, 0.5488,\n",
            "         0.5553, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5139, 0.5252, 0.5139, 0.5252, 0.5139, 0.5252, 0.5139, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.7958, 0.8104, 0.7958, 0.8104, 0.7958, 0.5882, 0.6001, 0.8104, 0.6001,\n",
            "         0.5882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.7212, 0.7449, 0.6658, 0.7449, 0.6658, 0.6440, 0.7212, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(12.1030, device='cuda:0')\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-1.2392046 -1.1181345 -1.1982722 -1.1036472 -1.2387447 -1.3212657\n",
            " -1.3278341  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-0.1861,  0.1831, -0.1087,  0.2918,  0.2361, -0.0763, -0.0659, -0.0017,\n",
            "        -0.0017, -0.0017], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.4536, 0.5456, 0.4728, 0.5724, 0.5587, 0.4809, 0.4835, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4944, 0.5686, 0.5758, 0.6004, 0.3951, 0.5427, 0.5446, 0.5322, 0.5082,\n",
            "         0.5501, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.6044, 0.7889, 0.7490, 0.8497, 0.5821, 0.7320, 0.6607, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.6819, 0.8407, 0.8575, 0.9172, 0.5026, 0.6113, 0.6078, 0.7596, 0.6769,\n",
            "         0.5976, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(12.0198, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 12.02\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-4.389009  -4.3900704 -4.389009  -4.3900704 -4.389009  -4.3900704\n",
            " -4.389009   0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-5.6025e+00, -5.6056e+00, -5.6025e+00, -5.6056e+00, -5.6025e+00,\n",
            "        -5.6056e+00, -5.6025e+00, -1.9214e-03, -1.9214e-03, -1.9214e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
            "         0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[3.6817e-03, 3.6706e-03, 5.6062e+00, 3.6706e-03, 5.6062e+00, 5.6092e+00,\n",
            "         3.6817e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [6.6123e-04, 6.5950e-04, 6.6123e-04, 6.5950e-04, 6.6123e-04, 7.3243e+00,\n",
            "         7.3217e+00, 6.5950e-04, 7.3217e+00, 7.3243e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(46.1324, device='cuda:0')\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-3.4889736 -4.12003   -3.758861  -3.6191664 -3.9175768 -4.0252028\n",
            " -3.3124886  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-6.5249e+00, -7.0686e+00, -6.3454e+00, -6.4604e+00, -6.5183e+00,\n",
            "        -6.4705e+00, -6.0222e+00, -1.9214e-03, -1.9214e-03, -1.9214e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.0015, 0.0009, 0.0018, 0.0016, 0.0015, 0.0015, 0.0024, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0003, 0.0009, 0.0009, 0.0009, 0.0008, 0.0009, 0.0009, 0.0008, 0.0007,\n",
            "         0.0009, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[1.4654e-03, 8.5104e-04, 6.3472e+00, 1.5629e-03, 6.5198e+00, 6.4721e+00,\n",
            "         2.4214e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [3.0397e-04, 9.2418e-04, 8.6100e-04, 8.7073e-04, 7.7934e-04, 6.9975e+00,\n",
            "         7.0579e+00, 8.2175e-04, 7.3016e+00, 7.0182e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(47.7251, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 47.73\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.9909906 -1.9910717 -1.9909906 -1.9910717 -1.9909906 -1.9910717\n",
            " -1.9909906  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-1.1992, -1.1992, -1.1992, -1.1992, -1.1992, -1.1992, -1.1992, -0.0021,\n",
            "        -0.0021, -0.0021], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.2316, 0.2316, 0.2316, 0.2316, 0.2316, 0.2316, 0.2316, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2314, 0.2314, 0.2314, 0.2314, 0.2314, 0.2314, 0.2314, 0.2314, 0.2314,\n",
            "         0.2314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.2635, 0.2635, 1.4626, 0.2635, 1.4626, 1.4627, 0.2635, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2632, 0.2631, 0.2632, 0.2631, 0.2632, 1.4638, 1.4637, 0.2631, 1.4637,\n",
            "         1.4638, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(12.8757, device='cuda:0')\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-1.9211397 -1.9519734 -2.130168  -1.8146405 -1.7800431 -1.8391342\n",
            " -2.0343428  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-1.0612, -1.1494, -1.1166, -1.0522, -1.2058, -1.4504, -1.0404, -0.0021,\n",
            "        -0.0021, -0.0021], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.2571, 0.2406, 0.2466, 0.2588, 0.2304, 0.1899, 0.2611, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2501, 0.1850, 0.2589, 0.2092, 0.2798, 0.2219, 0.2485, 0.2570, 0.2305,\n",
            "         0.3136, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.2972, 0.2752, 1.3999, 0.2995, 1.4677, 1.6610, 0.3026, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2878, 0.2046, 0.2996, 0.2348, 0.3282, 1.5053, 1.3923, 0.2970, 1.4675,\n",
            "         1.1595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(12.8797, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 12.88\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-2.357291  -2.3573117 -2.357291  -2.3573117 -2.357291  -2.3573117\n",
            " -2.357291   0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 4.4283e+00,  4.4283e+00,  4.4283e+00,  4.4283e+00,  4.4283e+00,\n",
            "         4.4283e+00,  4.4283e+00, -2.0329e-03, -2.0329e-03, -2.0329e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882, 0.9882,\n",
            "         0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[4.4402, 4.4401, 0.0119, 4.4401, 0.0119, 0.0119, 4.4402, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [4.4423, 4.4422, 4.4423, 4.4422, 4.4423, 0.0118, 0.0118, 4.4422, 0.0118,\n",
            "         0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(44.4970, device='cuda:0')\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-2.4455745 -2.3360655 -2.3384628 -2.173488  -2.7947314 -2.1825056\n",
            " -2.3166473  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 4.4896e+00,  4.3660e+00,  4.0959e+00,  4.0160e+00,  4.8475e+00,\n",
            "         4.1591e+00,  4.0437e+00, -2.0329e-03, -2.0329e-03, -2.0329e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.9889, 0.9875, 0.9836, 0.9823, 0.9922, 0.9846, 0.9828, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9804, 0.9845, 0.9781, 0.9846, 0.9776, 0.9818, 0.9832, 0.9837, 0.9830,\n",
            "         0.9836, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[4.5007, 4.3786, 0.0165, 4.0338, 0.0078, 0.0155, 4.0611, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [3.9327, 4.1669, 3.8229, 4.1762, 3.8006, 0.0184, 0.0169, 4.1168, 0.0171,\n",
            "         0.0165, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(41.0992, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 41.10\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.4318175 -1.431828  -1.4318175 -1.431828  -1.4318175 -1.431828\n",
            " -1.4318175  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 2.3404e+00,  2.3404e+00,  2.3404e+00,  2.3404e+00,  2.3404e+00,\n",
            "         2.3404e+00,  2.3404e+00, -2.0941e-03, -2.0941e-03, -2.0941e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122, 0.9122,\n",
            "         0.9122, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[2.4323, 2.4323, 0.0919, 2.4323, 0.0919, 0.0919, 2.4323, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [2.4323, 2.4323, 2.4323, 2.4323, 2.4323, 0.0919, 0.0919, 2.4323, 0.0919,\n",
            "         0.0919, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(24.9666, device='cuda:0')\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-1.4563651 -1.3870616 -1.4811544 -1.5252523 -1.4517074 -1.4752259\n",
            " -1.3991237  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([ 2.0312e+00,  2.0759e+00,  2.0612e+00,  1.9348e+00,  2.0876e+00,\n",
            "         1.9427e+00,  2.1180e+00, -2.0941e-03, -2.0941e-03, -2.0941e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.8840, 0.8885, 0.8871, 0.8738, 0.8897, 0.8746, 0.8926, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.8934, 0.8958, 0.9068, 0.8966, 0.9655, 0.8984, 0.9675, 0.8960, 0.9042,\n",
            "         0.8772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[2.1544, 2.1940, 0.1198, 2.0698, 0.1169, 0.1339, 2.2316, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [2.2391, 2.2619, 2.3728, 2.2696, 3.3654, 0.1071, 0.0330, 2.2633, 0.1007,\n",
            "         0.1310, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(24.1644, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 24.16\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.3239918 -1.3239989 -1.3239918 -1.3239989 -1.3239918 -1.3239989\n",
            " -1.3239918  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-0.2081, -0.2082, -0.2081, -0.2082, -0.2081, -0.2082, -0.2081, -0.0022,\n",
            "        -0.0022, -0.0022], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481, 0.4481,\n",
            "         0.4481, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.5945, 0.5945, 0.8026, 0.5945, 0.8026, 0.8026, 0.5945, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5944, 0.5944, 0.5944, 0.5944, 0.5944, 0.8027, 0.8027, 0.5944, 0.8027,\n",
            "         0.8027, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(11.5631, device='cuda:0')\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-1.2471504 -1.2094274 -1.4849477 -1.3663421 -1.3723516 -1.39049\n",
            " -1.2615566 -1.3425264 -1.2895927 -1.3591423  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-0.3869, -0.4607, -0.1443, -0.3465, -0.3751, -0.1678, -0.4302, -0.2289,\n",
            "        -0.2554, -0.3134], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.4045, 0.3868, 0.4640, 0.4142, 0.4073, 0.4581, 0.3941, 0.4430, 0.4365,\n",
            "         0.4223, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4268, 0.4645, 0.4492, 0.5127, 0.4178, 0.4586, 0.4144, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.5183, 0.4891, 0.6236, 0.5348, 0.5231, 0.7806, 0.9312, 0.5852, 0.8290,\n",
            "         0.8620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5565, 0.6245, 0.8004, 0.7189, 0.8726, 0.7795, 0.5351, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(11.5646, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 11.56\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.4658356 -1.4658856 -1.4658356 -1.4658856 -1.4658356 -1.4658856\n",
            " -1.4658356 -1.4658856 -1.4658356 -1.4658856  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-2.1043, -2.1043, -2.1043, -2.1043, -2.1043, -2.1043, -2.1043, -2.1043,\n",
            "        -2.1043, -2.1043], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.1087, 0.1087, 0.1087, 0.1087, 0.1087, 0.1087, 0.1087, 0.1087, 0.1087,\n",
            "         0.1087, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1083, 0.1083, 0.1083, 0.1083, 0.1083, 0.1083, 0.1083, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.1151, 0.1151, 0.1151, 0.1151, 0.1151, 2.2194, 2.2193, 0.1151, 2.2193,\n",
            "         2.2194, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1146, 0.1146, 2.2229, 0.1146, 2.2229, 2.2229, 0.1146, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(16.6949, device='cuda:0')\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-1.4756546 -1.4043036 -1.446795  -1.4568291 -1.6754999 -1.4449453\n",
            " -1.5436702 -1.4882393 -1.4267678 -1.3991146  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-2.2304, -2.1127, -2.2472, -2.1014, -2.4450, -1.9935, -2.0959, -2.1005,\n",
            "        -2.3170, -2.1773], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.0971, 0.1079, 0.0956, 0.1090, 0.0798, 0.1199, 0.1095, 0.1091, 0.0897,\n",
            "         0.1018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0903, 0.0979, 0.1058, 0.0979, 0.0967, 0.1067, 0.1100, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.1021, 0.1141, 0.1005, 0.1154, 0.0832, 2.1212, 2.2118, 0.1155, 2.4110,\n",
            "         2.2847, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0947, 0.1030, 2.2465, 0.1030, 2.3359, 2.2380, 0.1165, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(16.8971, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 16.90\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-1.6070266 -1.6070609 -1.6070266 -1.6070609 -1.6070266 -1.6070609\n",
            " -1.6070266  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-0.8629, -0.8629, -0.8629, -0.8629, -0.8629, -0.8629, -0.8629, -0.0024,\n",
            "        -0.0024, -0.0024], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.2967, 0.2967, 0.2967, 0.2967, 0.2967, 0.2967, 0.2967, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2997, 0.2997, 0.2997, 0.2997, 0.2997, 0.2997, 0.2997, 0.2997, 0.2997,\n",
            "         0.2997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.3520, 0.3520, 1.2149, 0.3520, 1.2149, 1.2149, 0.3520, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3563, 0.3563, 0.3563, 0.3563, 0.3563, 1.2049, 1.2050, 0.3563, 1.2050,\n",
            "         1.2049, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(12.0102, device='cuda:0')\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-2.4757905 -1.8118811 -1.3935075 -1.7760491 -1.8081894 -1.5165153\n",
            " -1.4776211 -1.875287  -1.6669865 -1.4514227  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-0.8075, -0.9208, -1.1549, -1.1334, -0.9653, -1.0817, -1.1439, -1.0918,\n",
            "        -1.0537, -1.0442], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.3084, 0.2848, 0.2396, 0.2435, 0.2758, 0.2532, 0.2416, 0.2513, 0.2585,\n",
            "         0.2603, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2580, 0.2361, 0.2709, 0.2172, 0.2295, 0.2766, 0.2305, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.3688, 0.3352, 0.2739, 0.2791, 0.3227, 1.3736, 1.4205, 0.2894, 1.3528,\n",
            "         1.3458, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2984, 0.2693, 1.3060, 0.2449, 1.4718, 1.2851, 0.2620, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(12.4994, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 12.50\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-11.019026 -11.018909 -11.019026 -11.018909 -11.019026 -11.018909\n",
            " -11.019026   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-5.2707e+00, -5.2713e+00, -5.2707e+00, -5.2713e+00, -5.2707e+00,\n",
            "        -5.2713e+00, -5.2707e+00, -2.3745e-03, -2.3745e-03, -2.3745e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0051, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052,\n",
            "         0.0052, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[5.1267e-03, 5.1240e-03, 5.2759e+00, 5.1240e-03, 5.2759e+00, 5.2764e+00,\n",
            "         5.1267e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [5.2178e-03, 5.2151e-03, 5.2178e-03, 5.2151e-03, 5.2178e-03, 5.2588e+00,\n",
            "         5.2583e+00, 5.2151e-03, 5.2583e+00, 5.2588e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(36.9141, device='cuda:0')\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-10.380339 -11.337053 -12.063601  -9.801391 -10.940261 -10.981867\n",
            " -10.958899   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-4.7438e+00, -5.1620e+00, -3.5820e+00, -3.0765e+00, -5.3241e+00,\n",
            "        -5.4622e+00, -5.7327e+00, -2.3745e-03, -2.3745e-03, -2.3745e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.0086, 0.0057, 0.0271, 0.0441, 0.0048, 0.0042, 0.0032, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.0024, 0.3868, 0.0059, 0.0016, 0.0022, 0.0020, 0.0019, 0.0027, 0.0022,\n",
            "         0.0304, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[8.6678e-03, 5.7139e-03, 3.6095e+00, 4.5090e-02, 5.3289e+00, 5.4664e+00,\n",
            "         3.2331e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [2.4452e-03, 4.8899e-01, 5.8958e-03, 1.5953e-03, 2.2155e-03, 6.2201e+00,\n",
            "         6.2572e+00, 2.6742e-03, 6.1185e+00, 3.4926e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(37.0596, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 37.06\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-11.037801 -11.037792 -11.037801 -11.037792 -11.037801 -11.037792\n",
            " -11.037801   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-1.8562, -1.8562, -1.8562, -1.8562, -1.8562, -1.8562, -1.8562, -0.0023,\n",
            "        -0.0023, -0.0023], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.1351, 0.1351, 0.1351, 0.1351, 0.1351, 0.1351, 0.1351, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376, 0.1376,\n",
            "         0.1376, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.1452, 0.1452, 2.0014, 0.1452, 2.0014, 2.0014, 0.1452, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1481, 0.1480, 0.1481, 0.1480, 0.1481, 1.9833, 1.9833, 0.1480, 1.9833,\n",
            "         1.9833, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(15.4065, device='cuda:0')\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-10.906787 -11.005111 -11.315432 -10.787605 -10.285477 -10.973194\n",
            " -10.745305   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-1.7122e+00,  1.6602e-01, -1.7529e+00, -1.0115e+00, -3.4216e+00,\n",
            "        -2.3682e+00, -4.2521e-02, -2.3496e-03, -2.3496e-03, -2.3496e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.1529, 0.5414, 0.1477, 0.2667, 0.0316, 0.0856, 0.4894, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9709, 0.1768, 0.0429, 0.8102, 0.0803, 0.3727, 0.1301, 0.1277, 0.4721,\n",
            "         0.2060, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.1659, 0.7796, 1.9127, 0.3102, 3.4537, 2.4577, 0.6721, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [3.5376, 0.1946, 0.0439, 1.6616, 0.0837, 0.9869, 2.0392, 0.1367, 0.7506,\n",
            "         1.5801, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(20.7668, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 20.77\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.66371   -10.6637125 -10.66371   -10.6637125 -10.66371   -10.6637125\n",
            " -10.66371   -10.6637125 -10.66371   -10.6637125   0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([3.6260, 3.6260, 3.6260, 3.6260, 3.6260, 3.6260, 3.6260, 3.6260, 3.6260,\n",
            "        3.6260], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
            "         0.9741, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9733, 0.9733, 0.9733, 0.9733, 0.9733, 0.9733, 0.9733, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[3.6522, 3.6523, 3.6522, 3.6523, 3.6522, 0.0263, 0.0263, 3.6523, 0.0263,\n",
            "         0.0263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [3.6222, 3.6223, 0.0271, 3.6223, 0.0271, 0.0271, 3.6222, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(36.5889, device='cuda:0')\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-10.735647 -10.661517  -9.991322 -10.863505 -11.68774  -10.587137\n",
            " -10.118315   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([ 4.1552e+00,  3.8055e+00,  5.2865e+00,  3.0822e+00,  4.3491e+00,\n",
            "         3.3084e+00,  3.5030e+00, -2.3229e-03, -2.3229e-03, -2.3229e-03],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.9846, 0.9782, 0.9950, 0.9562, 0.9872, 0.9647, 0.9708, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9307, 0.9613, 0.9597, 0.9883, 0.9285, 0.9785, 0.9690, 0.9941, 0.9958,\n",
            "         0.9553, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[4.1707e+00, 3.8275e+00, 5.0466e-03, 3.1270e+00, 1.2836e-02, 3.5923e-02,\n",
            "         3.5327e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [2.6690e+00, 3.2517e+00, 3.2110e+00, 4.4451e+00, 2.6383e+00, 2.1743e-02,\n",
            "         3.1513e-02, 5.1350e+00, 4.2414e-03, 4.5761e-02, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(36.1651, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 36.17\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.546399 -10.546406 -10.546399 -10.546406 -10.546399 -10.546406\n",
            " -10.546399   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([ 3.5293e+00,  3.5293e+00,  3.5293e+00,  3.5293e+00,  3.5293e+00,\n",
            "         3.5293e+00,  3.5293e+00, -2.3109e-03, -2.3109e-03, -2.3109e-03],\n",
            "       device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.9715, 0.9715, 0.9715, 0.9715, 0.9715, 0.9715, 0.9715, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9717, 0.9717, 0.9717, 0.9717, 0.9717, 0.9717, 0.9717, 0.9717, 0.9717,\n",
            "         0.9717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[3.5582, 3.5582, 0.0289, 3.5582, 0.0289, 0.0289, 3.5582, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [3.5647, 3.5647, 3.5647, 3.5647, 3.5647, 0.0287, 0.0287, 3.5647, 0.0287,\n",
            "         0.0287, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(35.8224, device='cuda:0')\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [ -9.914979  -10.382501  -10.659573   -9.434505  -10.564009  -10.459026\n",
            " -10.3518915 -10.802999  -10.930968  -10.387707    0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([5.4174, 3.6557, 2.9746, 3.0965, 3.2823, 2.8377, 4.3661, 3.9663, 2.9674,\n",
            "        3.4767], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.9956, 0.9748, 0.9514, 0.9567, 0.9638, 0.9447, 0.9875, 0.9814, 0.9511,\n",
            "         0.9700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.9577, 0.9708, 0.9675, 0.9882, 0.9303, 0.9623, 0.9662, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[5.4218, 3.6812, 3.0244, 3.1407, 3.3192, 0.0569, 0.0126, 3.9851, 0.0502,\n",
            "         0.0304, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [3.1623, 3.5321, 0.0330, 4.4392, 0.0723, 0.0384, 3.3880, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(37.3878, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 37.39\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.321588 -10.321589 -10.321588 -10.321589 -10.321588 -10.321589\n",
            " -10.321588 -10.321589 -10.321588 -10.321589   0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([1.6631, 1.6632, 1.6631, 1.6632, 1.6631, 1.6632, 1.6631, 1.6632, 1.6631,\n",
            "        1.6632], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.8407, 0.8407, 0.8407, 0.8407, 0.8407, 0.8407, 0.8407, 0.8407, 0.8407,\n",
            "         0.8407, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.8401, 0.8401, 0.8401, 0.8401, 0.8401, 0.8401, 0.8401, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[1.8367, 1.8367, 1.8367, 1.8367, 1.8367, 0.1736, 0.1736, 1.8367, 0.1736,\n",
            "         0.1736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [1.8329, 1.8330, 0.1743, 1.8330, 0.1743, 0.1743, 1.8329, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(19.5692, device='cuda:0')\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [ -9.788727 -10.594732 -10.043334 -10.748368  -9.836754 -10.102998\n",
            " -10.214372 -10.699915 -10.741014 -10.86067    0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([2.4195, 0.8897, 2.1083, 1.2015, 1.7392, 1.3306, 3.7126, 1.5284, 1.5156,\n",
            "        1.6436], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.9183, 0.7088, 0.8917, 0.7688, 0.8506, 0.7909, 0.9762, 0.8218, 0.8199,\n",
            "         0.8380, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.8240, 0.9031, 0.6738, 0.8781, 0.8335, 0.7657, 0.8546, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[2.5048, 1.2338, 2.2229, 1.4645, 1.9010, 0.2345, 0.0241, 1.7247, 0.1986,\n",
            "         0.1767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [1.7372, 2.3342, 0.3948, 2.1048, 0.1821, 0.2670, 1.9282, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(20.6339, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 20.63\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-9.753737 -9.753736 -9.753737 -9.753736 -9.753737 -9.753736 -9.753737\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        0.        0.      ]\n",
            "Logit =  tensor([-0.9370, -0.9369, -0.9370, -0.9369, -0.9370, -0.9369, -0.9370, -0.0023,\n",
            "        -0.0023, -0.0023], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.2815, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2818, 0.2818, 0.2818, 0.2818, 0.2818, 0.2818, 0.2818, 0.2818, 0.2818,\n",
            "         0.2818, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.3306, 0.3306, 1.2676, 0.3306, 1.2676, 1.2675, 0.3306, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3311, 0.3311, 0.3311, 0.3311, 0.3311, 1.2664, 1.2664, 0.3311, 1.2664,\n",
            "         1.2664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(12.1772, device='cuda:0')\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-8.329143  -8.213823  -8.539909  -9.044027  -8.227356  -7.232357\n",
            " -7.7225485 -8.189601  -7.9616137 -2.9906483  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.       ]\n",
            "Logit =  tensor([-2.2186, -1.8433, -1.8778, -0.7848, -1.7552, -0.6088, -1.3578, -1.4785,\n",
            "        -0.5140, -5.1193], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.0981, 0.1367, 0.1326, 0.3133, 0.1474, 0.3523, 0.2046, 0.1857, 0.3743,\n",
            "         0.0059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1189, 0.1259, 0.1813, 0.2249, 0.6836, 0.1745, 0.2322, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.1032, 0.1469, 0.1423, 0.3758, 0.1595, 1.0432, 1.5867, 0.2054, 0.9828,\n",
            "         5.1253, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1266, 0.1346, 1.7075, 0.2548, 0.3803, 1.7461, 0.2642, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(14.4853, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 14.49\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.174766 -10.174772 -10.174766 -10.174772 -10.174766 -10.174772\n",
            " -10.174766 -10.174772 -10.174766 -10.174772   0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-1.6299, -1.6299, -1.6299, -1.6299, -1.6299, -1.6299, -1.6299, -1.6299,\n",
            "        -1.6299, -1.6299], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.1638, 0.1638, 0.1638, 0.1638, 0.1638, 0.1638, 0.1638, 0.1638, 0.1638,\n",
            "         0.1638, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.1639, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.1789, 0.1789, 0.1789, 0.1789, 0.1789, 1.8088, 1.8088, 0.1789, 1.8088,\n",
            "         1.8088, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1790, 0.1790, 1.8087, 0.1790, 1.8087, 1.8086, 0.1790, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(14.4508, device='cuda:0')\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [ -9.48726    -9.622991   -9.6456175 -10.111011   -8.881751  -10.1116295\n",
            " -10.420464   -9.966334  -10.081986  -10.065923    0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([-1.6044,  0.5309, -1.3388, -2.2159, -2.3850, -2.0162, -1.1377, -1.6522,\n",
            "        -1.3536, -1.8408], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.1674, 0.6297, 0.2077, 0.0983, 0.0843, 0.1175, 0.2427, 0.1608, 0.2053,\n",
            "         0.1370, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4136, 0.0745, 0.2079, 0.1185, 0.0863, 0.1734, 0.0999, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.1832, 0.9934, 0.2328, 0.1035, 0.0881, 2.1412, 1.4158, 0.1753, 1.5834,\n",
            "         1.9881, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5337, 0.0774, 1.5706, 0.1261, 2.4496, 1.7520, 0.1052, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(15.5194, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 15.52\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.314835 -10.31485  -10.314835 -10.31485  -10.314835 -10.31485\n",
            " -10.314835   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-1.3934, -1.3934, -1.3934, -1.3934, -1.3934, -1.3934, -1.3934, -0.0023,\n",
            "        -0.0023, -0.0023], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.1989, 0.1989, 0.1989, 0.1989, 0.1989, 0.1989, 0.1989, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1988, 0.1988, 0.1988, 0.1988, 0.1988, 0.1988, 0.1988, 0.1988, 0.1988,\n",
            "         0.1988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.2217, 0.2217, 1.6152, 0.2217, 1.6152, 1.6151, 0.2217, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 1.6154, 1.6154, 0.2217, 1.6154,\n",
            "         1.6154, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(13.5238, device='cuda:0')\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-10.18583   -10.440411  -10.207792   -9.900442  -10.306921   -9.957336\n",
            " -10.334875  -10.266969   -8.9480915 -10.228553    0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([-1.6978, -1.5512, -0.8952, -2.0264,  0.5879,  0.1648, -1.1993, -1.4131,\n",
            "        -1.6658, -1.5409], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.1547, 0.1749, 0.2900, 0.1165, 0.6429, 0.5411, 0.2316, 0.1957, 0.1590,\n",
            "         0.1764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2591, 0.1324, 0.1536, 0.1389, 0.1225, 0.1035, 0.2156, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.1681, 0.1923, 0.3425, 0.1238, 1.0297, 0.6141, 1.4627, 0.2178, 1.8390,\n",
            "         1.7349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2999, 0.1421, 1.8734, 0.1496, 2.0998, 2.2682, 0.2428, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(14.8008, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 14.80\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.367304 -10.367293 -10.367304 -10.367293 -10.367304 -10.367293\n",
            " -10.367304   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-0.8463, -0.8463, -0.8463, -0.8463, -0.8463, -0.8463, -0.8463, -0.0023,\n",
            "        -0.0023, -0.0023], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002, 0.3002,\n",
            "         0.3002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.3570, 0.3570, 1.2033, 0.3570, 1.2033, 1.2033, 0.3570, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3569, 0.3569, 0.3569, 0.3569, 0.3569, 1.2034, 1.2034, 0.3569, 1.2034,\n",
            "         1.2034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(11.9929, device='cuda:0')\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "X (CLS) totals =  [-10.624947   -9.740051  -10.348192  -10.353505  -11.1183605  -9.992521\n",
            " -11.04931   -10.454474  -10.578121  -11.25824     0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.       ]\n",
            "Logit =  tensor([-0.9794, -0.2534, -1.2413, -1.0685, -0.4505, -1.0490, -0.8437, -0.7323,\n",
            "        -1.3304, -0.2645], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN OUTPUTS: torch.Size([2, 512]) tensor([[0.2730, 0.4370, 0.2242, 0.2557, 0.3892, 0.2594, 0.3008, 0.3247, 0.2091,\n",
            "         0.4343, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2538, 0.2037, 0.3557, 0.2822, 0.2577, 0.2454, 0.3473, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "TRAIN LOSS tensor([[0.3188, 0.5745, 0.2539, 0.2953, 0.4931, 1.3493, 1.2014, 0.3926, 1.5649,\n",
            "         0.8341, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2928, 0.2278, 1.0336, 0.3315, 1.3559, 1.4050, 0.4266, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
            "TRAIN LOSS TOTAL tensor(12.3511, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "  Average training loss: 12.35\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "X (CLS) totals =  [-10.381492 -10.381495 -10.381492 -10.381495 -10.381492 -10.381495\n",
            " -10.381492 -10.381495 -10.381492 -10.381495   0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.      ]\n",
            "Logit =  tensor([-0.4154, -0.4154, -0.4154, -0.4154, -0.4154, -0.4154, -0.4154, -0.4154,\n",
            "        -0.4154, -0.4154], device='cuda:0')\n",
            "VALIDATION OUTPUTS: torch.Size([2, 512]) tensor([[0.3976, 0.3976, 0.3976, 0.3976, 0.3976, 0.3976, 0.3976, 0.3976, 0.3976,\n",
            "         0.3976, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3977, 0.3977, 0.3977, 0.3977, 0.3977, 0.3977, 0.3977, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LABELS: torch.Size([2, 512]) tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='cuda:0')\n",
            "VALIDATION LOSS tensor([[0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.9223, 0.9223, 0.5069, 0.9223,\n",
            "         0.9223, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5069, 0.5069, 0.9222, 0.5069, 0.9222, 0.9222, 0.5069, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]], device='cuda:0')\n",
            "VALIDATION LOSS TOTAL tensor(11.5244, device='cuda:0')\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:04 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULevkUV5bx7"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}