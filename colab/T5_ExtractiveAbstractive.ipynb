{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of T5_ExtractiveAbstractive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d07ba2f057eb4e25ab463f604d72883a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eb79ce14fc1f4de1b7ba7e37b50802ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dab0313e554c4572b53b71573fd8e3b9",
              "IPY_MODEL_02d8556083714875a2a78efed0438f33"
            ]
          }
        },
        "eb79ce14fc1f4de1b7ba7e37b50802ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dab0313e554c4572b53b71573fd8e3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_949e634469f547168e9ad4a40d9d7508",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eabd842864744c9d86dda0a9eb9aec86"
          }
        },
        "02d8556083714875a2a78efed0438f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aabb80b580bb480795ede4e40da2e608",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 3.12kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73453701b6b7468e8e4f724460ee97ba"
          }
        },
        "949e634469f547168e9ad4a40d9d7508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eabd842864744c9d86dda0a9eb9aec86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aabb80b580bb480795ede4e40da2e608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73453701b6b7468e8e4f724460ee97ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45597ea5cbb64ca8b4b7d479d5fabc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b1aba3f71e8f4a2885bdfa8ca67cd882",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86fd0da30a114a6c9c845ce56760b809",
              "IPY_MODEL_38a5dd84d08845729ac0604fc2db195e"
            ]
          }
        },
        "b1aba3f71e8f4a2885bdfa8ca67cd882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86fd0da30a114a6c9c845ce56760b809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f7fe4dc296d240dfbaa6946184b78052",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_839994bd05f04074ab360306f333012e"
          }
        },
        "38a5dd84d08845729ac0604fc2db195e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3445529ef27d4fb5b121ee09910c7096",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:05&lt;00:00, 46.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a69c078d768d4b4e9f75e2a1e89daada"
          }
        },
        "f7fe4dc296d240dfbaa6946184b78052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "839994bd05f04074ab360306f333012e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3445529ef27d4fb5b121ee09910c7096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a69c078d768d4b4e9f75e2a1e89daada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JMpA3Pi4rr9"
      },
      "source": [
        "# T5 Baseline\n",
        "\n",
        "The initial exploration will use T5-small as the pre-training model along with ICSI dataset. When the model is ready, we will expand the dataset and also validation set for other hyperparameter tuning.\n",
        "\n",
        "1. Library Loading  \n",
        "2. Dataset Loading\n",
        "3.   Dataset Transformation\n",
        "4.   Training and Test Splitting\n",
        "5.   Fine Tuning\n",
        "6.   Checkpoint saving\n",
        "7.   Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAat9uX5rJn"
      },
      "source": [
        "## Library Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BODRVvgf5RGZ",
        "outputId": "7bcf5bc5-3ab2-4319-c855-c04ee8cd2e5e"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "#!pip install datasets\n",
        "!pip install nlp\n",
        "#!pip install rouge_score\n",
        "!pip install rouge\n",
        "#!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 5.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 24.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 41.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 51.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 4.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 48.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 11.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 46.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 11.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7MB 5.9MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.8)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7MB 204kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.1.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Installing collected packages: xxhash, pyarrow, nlp\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed nlp-0.4.0 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqwv6NyQ_Wjl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import time\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "# T5ForConditionalGeneration is specific for sequence-to-sequence\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "#from nlp import load_metric\n",
        "import nlp\n",
        "from rouge import Rouge\n",
        "\n",
        "import wandb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8XNh-h8_g0_",
        "outputId": "60716cce-62c3-4873-fc22-f47966ddcb56"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov 29 17:19:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtYB7VvI_jm4"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcXUpiwbe8Cs",
        "outputId": "aef667b7-2495-46a7-e774-bccbee86b375"
      },
      "source": [
        "!wandb login\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqq1uu8hA49j"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "Loaded from GDrive the transformed dataset.\n",
        "\n",
        "This portion is using the dataset from extractive summary to abstractive summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGXLrMD7GC4O"
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_size = 0.8"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvq382j4A3MC",
        "outputId": "45f828e7-de64-4507-c0a1-ea84de745072"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#/content/drive/My Drive/W266/data/ICSI_extrac_abstrac_512token.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DysKSkQoBpfE",
        "outputId": "2e753cc6-9c85-4544-c937-0bb94dc19cd1"
      },
      "source": [
        "#df = pd.read_csv('/content/drive/My Drive/W266/512_tokens/ICSI_extrac_abstrac_512token.csv',encoding='latin-1')\n",
        "#df = df[df['extractive'].notna()][['abstractive','extractive']]\n",
        "train_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_train_cleaned_eax.csv',encoding='latin-1')\n",
        "dev_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_dev_cleaned_eax.csv',encoding='latin-1')\n",
        "test_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_test_cleaned_eax.csv',encoding='latin-1')\n",
        "\n",
        "#train_dataset = train_dataset[train_dataset.abstractive.notna()]\n",
        "#dev_dataset = dev_dataset[dev_dataset.abstractive.notna()]\n",
        "#test_dataset = test_dataset[test_dataset.abstractive.notna()]\n",
        "\n",
        "train_dataset = train_dataset.dropna(subset=['abstractive'])\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "dev_dataset = dev_dataset.dropna(subset=['abstractive'])\n",
        "dev_dataset = dev_dataset.reset_index(drop=True)\n",
        "\n",
        "test_dataset = test_dataset.dropna(subset=['abstractive'])\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "# use the pre-defined \"summarize\" for abstractive summary\n",
        "train_dataset.extractive = 'summarize: ' + train_dataset.extractive\n",
        "dev_dataset.extractive = 'summarize: ' + dev_dataset.extractive\n",
        "test_dataset.extractive = 'summarize: ' + test_dataset.extractive\n",
        "print(train_dataset.head(1))\n",
        "print(len(train_dataset))\n",
        "print(len(dev_dataset))\n",
        "print(len(test_dataset))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     meeting  ...                                        abstractive\n",
            "0  ES2002a.A  ...  The project manager introduced the upcoming pr...\n",
            "\n",
            "[1 rows x 3 columns]\n",
            "291\n",
            "189\n",
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "2Jv3VHyUcENB",
        "outputId": "1e1ddb8d-1510-491c-c4c7-131f7d7c22fb"
      },
      "source": [
        "train_dataset.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meeting</th>\n",
              "      <th>extractive</th>\n",
              "      <th>abstractive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ES2002a.A</td>\n",
              "      <td>summarize: and i like whales . they come in an...</td>\n",
              "      <td>The project manager introduced the upcoming pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ES2002a.B</td>\n",
              "      <td>summarize: this is the kick off meeting for ou...</td>\n",
              "      <td>The project manager introduced the upcoming pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ES2002a.C</td>\n",
              "      <td>summarize: my favourite animal would be a monk...</td>\n",
              "      <td>The project manager introduced the upcoming pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ES2002a.D</td>\n",
              "      <td>summarize: my favourite animal is like   a bea...</td>\n",
              "      <td>The project manager introduced the upcoming pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ES2002b.A</td>\n",
              "      <td>summarize: which is the clunky one , the one o...</td>\n",
              "      <td>The user interface designer presented two exis...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     meeting  ...                                        abstractive\n",
              "0  ES2002a.A  ...  The project manager introduced the upcoming pr...\n",
              "1  ES2002a.B  ...  The project manager introduced the upcoming pr...\n",
              "2  ES2002a.C  ...  The project manager introduced the upcoming pr...\n",
              "3  ES2002a.D  ...  The project manager introduced the upcoming pr...\n",
              "4  ES2002b.A  ...  The user interface designer presented two exis...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENujMiWE70j",
        "outputId": "2dae2627-6b6e-4abc-fd96-e46b23d7bfd4"
      },
      "source": [
        "#train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
        "#test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "#train_dataset = train_dataset.reset_index(drop=True)\n",
        "#print(\"FULL Dataset: {}\".format(df.shape))\n",
        "\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"DEV Dataset: {}\".format(dev_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (291, 3)\n",
            "DEV Dataset: (189, 3)\n",
            "TEST Dataset: (83, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4EXP96GZ8n"
      },
      "source": [
        "## Dataset Transformation\n",
        "\n",
        "Tokenize the input and also perform the attention masking to make sure everything can be done in tensors. \n",
        "\n",
        "Tunable Hyprparam:\n",
        "\n",
        "*   MAX_LEN\n",
        "*   SUMMARY_LEN\n",
        "* TRAIN_BATCH_SIZE\n",
        "* DEV_BATCH_SIZE\n",
        "* TEST_BATCH_SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07SniX-oGTcj"
      },
      "source": [
        "# most code from https://colab.research.google.com/drive/1ypT7oCjtBOTSMJv7J5_1vO7hDYSD_-oU?authuser=2#scrollTo=932p8NhxeNw4\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.abstractive = self.data.abstractive\n",
        "        self.extractive = self.data.extractive\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstractive)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        extractive = str(self.extractive[index])\n",
        "        extractive = ' '.join(extractive.split())\n",
        "\n",
        "        abstractive = str(self.abstractive[index])\n",
        "        abstractive = ' '.join(abstractive.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([extractive], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([abstractive], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOB45_8K6kK"
      },
      "source": [
        "### Training Dataset and Test Dataset \n",
        "\n",
        "# TRAIN Dataset: (1231, 4)\n",
        "# DEV Dataset: (744, 4)\n",
        "# TEST Dataset: (165, 4)\n",
        "\n",
        "MAX_LEN = 1024\n",
        "SUMMARY_LEN= 150\n",
        "\n",
        "# note here only uses the t5-small model.\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "dev_set = CustomDataset(dev_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws6q9x_TMgtP",
        "outputId": "a5826242-9de0-4943-da5e-d7f813f0e8a0"
      },
      "source": [
        "# double checking the result size, only for one point\n",
        "# https://stackoverflow.com/questions/43627405/understanding-getitem-method\n",
        "print(train_set[0]['source_ids'].shape)\n",
        "print(train_set[0]['source_mask'].shape)\n",
        "print(train_set[0]['target_ids'].shape)\n",
        "print(train_set[0]['target_ids_y'].shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024])\n",
            "torch.Size([1024])\n",
            "torch.Size([150])\n",
            "torch.Size([150])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx2e48uKnY3"
      },
      "source": [
        "## Fine Tuning\n",
        "\n",
        "Here we directly use the pre-trained model t5-small and will save checkpoint every 500 steps. \n",
        "\n",
        "Tunable Parameter:\n",
        "* T5ForConditionalGeneration or T5\n",
        "* epoch - train, dev, test\n",
        "* optimizer - LEARNING_RATE, Adam\n",
        "* output: num_beams, length_penalty,early_stopping\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9MmjbqbYjHB"
      },
      "source": [
        "### Training & Validation Functions\n",
        "\n",
        "The training part uses the t5-small pretrained model, didn't make any change to the model layer structures, and fine tune the parameters based on the dataset we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05VKilCKKptn"
      },
      "source": [
        "losslist = []\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "  # put into train mode \n",
        "  model.train()\n",
        "  # enumerate the dataloader for training set into the defined network\n",
        "  for _,data in enumerate(loader, 0):\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      # https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
        "      y_ids = y[:, :-1].contiguous()\n",
        "      lm_labels = y[:, 1:].clone().detach()\n",
        "      lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "      loss = outputs[0]\n",
        "      losslist.append(loss)\n",
        "      if _%10==0:\n",
        "        wandb.log({\"Training Loss\": loss.item()})\n",
        "      if _%500==0:\n",
        "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "      \n",
        "      # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "      # https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VNL1ru0Wvsc"
      },
      "source": [
        "# https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81\n",
        "\n",
        "def dev(epoch, tokenizer, model, device, loader):\n",
        "  #https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  #rouge_metric = load_metric('rouge') \n",
        "  # https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _, data in enumerate(loader, 0):\n",
        "\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask, \n",
        "          max_length=150, \n",
        "          num_beams=12,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "      if _%100==0:\n",
        "          print(f'Completed {_}')\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "      #print(preds)\n",
        "      #print(target)\n",
        "      #rouge_metric.add(preds, target)\n",
        "      \n",
        "    #rouge_results = rouge_metric.compute(rouge_types=[\"rouge2\"]) \n",
        "  return predictions, actuals"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJddO-eY3whv"
      },
      "source": [
        "def compute_rouge_scores(cand_list, ref_list):\n",
        "    \"\"\"\n",
        "    :param cand_list: list of candidate summaries\n",
        "    :param ref_list: list of reference summaries\n",
        "    :return: rouge scores\n",
        "    \"\"\"\n",
        "    rouge = Rouge()\n",
        "    rouge_1_f_score = 0.\n",
        "    rouge_2_f_score = 0.\n",
        "    rouge_L_f_score = 0.\n",
        "\n",
        "    rouge_1_r_score = 0.\n",
        "    rouge_2_r_score = 0.\n",
        "    rouge_L_r_score = 0.\n",
        "\n",
        "    rouge_1_p_score = 0.\n",
        "    rouge_2_p_score = 0.\n",
        "    rouge_L_p_score = 0.\n",
        "\n",
        "    doc_count = len(cand_list)\n",
        "\n",
        "    for cand, ref in zip(cand_list, ref_list):\n",
        "      try:\n",
        "        rouge_scores = rouge.get_scores(cand, ref)[0]\n",
        "        rouge_1_f_score += rouge_scores['rouge-1']['f']\n",
        "        rouge_2_f_score += rouge_scores['rouge-2']['f']\n",
        "        rouge_L_f_score += rouge_scores['rouge-l']['f']\n",
        "\n",
        "        rouge_1_r_score += rouge_scores['rouge-1']['r']\n",
        "        rouge_2_r_score += rouge_scores['rouge-2']['r']\n",
        "        rouge_L_r_score += rouge_scores['rouge-l']['r']\n",
        "\n",
        "        rouge_1_p_score += rouge_scores['rouge-1']['p']\n",
        "        rouge_2_p_score += rouge_scores['rouge-2']['p']\n",
        "        rouge_L_p_score += rouge_scores['rouge-l']['p']\n",
        "      except:\n",
        "        pass\n",
        "    rouge_1_f_score = rouge_1_f_score / doc_count\n",
        "    rouge_2_f_score = rouge_2_f_score / doc_count\n",
        "    rouge_L_f_score = rouge_L_f_score / doc_count\n",
        "\n",
        "    results_dict = {}\n",
        "    results_dict['rouge_1_f_score'] = rouge_1_f_score\n",
        "    results_dict['rouge_2_f_score'] = rouge_2_f_score\n",
        "    results_dict['rouge_l_f_score'] = rouge_L_f_score\n",
        "\n",
        "    return results_dict"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4beVpzOMCoFu"
      },
      "source": [
        "AMI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_AMI_as_dev.csv\"\n",
        "ICSI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_ICSI_as_dev.csv\"\n",
        "\n",
        "amigold = pd.read_csv(AMI_PATH)\n",
        "icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "def rouge_per_document(final_df,gold):\n",
        "  merged_df = pd.concat([dev_dataset.meeting, final_df.Generated_Abstractive_Summary], axis=1)\n",
        "  merged_df[\"meetinglevel\"] = merged_df.meeting.apply(lambda x: x.split(\".\")[0]) \n",
        "\n",
        "  gas_list =[]\n",
        "  meeting_list = []\n",
        "  generated_abstractive = \"\"\n",
        "  for me in set(merged_df.meetinglevel):\n",
        "    for gas in merged_df[merged_df.meetinglevel == me]['Generated_Abstractive_Summary']:\n",
        "      generated_abstractive+= gas + \" \"\n",
        "    gas_list.append(generated_abstractive)\n",
        "    meeting_list.append(me)\n",
        "    generated_abstractive = \" \"\n",
        "  per_doc_summary = pd.DataFrame(\n",
        "    {'Meeting': meeting_list,\n",
        "     'Generated_Abstractive_Summary': gas_list\n",
        "    })\n",
        "  \n",
        "  new_df = pd.merge(per_doc_summary, gold,  how='left', left_on='Meeting', right_on ='meeting')\n",
        "  rouge_results_perdoc = compute_rouge_scores(new_df.Generated_Abstractive_Summary,\n",
        "                                      new_df.abstractive)\n",
        "  return rouge_results_perdoc\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxyXnBnZYex-"
      },
      "source": [
        "### Run Epoch\n",
        "Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y5TwfR5FBIki",
        "outputId": "007c062a-b9f1-487f-da03-546344aca822"
      },
      "source": [
        "id = wandb.util.generate_id()\n",
        "id\n",
        "#dwlkfpg3 AMI 1024\n",
        "#1aei9r6r ICSI 1024\n",
        "#3ugok7an ICSI 512\n",
        "#30e6cuxp AMI 512\n",
        "#3fsv41il ICSI 1024 Cleaned \n",
        "#2knqed4a AMI 1024 Cleaned \n",
        "#3le4t5oh AMI 1024 Cleaned t5-base\n",
        "#3oar0l9l ICSI 1024 t5-base\n",
        "#1foudah4 AMI 1024 eax small\n",
        "#3thp15xs ICSI 1024 eax small\n",
        "#3rc2qh90 ICSI 1024 eax small 0.0001\n",
        "#2ypembrw ICSI 1024 eax small 0.001\n",
        "#3k5y8yms AMI 1024 eax small 0.0001\n",
        "#2volhe8e AMI 1024 eax small 0.001"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2volhe8e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "i8gLtGWbh7AZ",
        "outputId": "f74d10ae-37ca-420d-c3d2-8e18c7e1aa73"
      },
      "source": [
        "#run = wandb.init(project=\"T5_1024_MSFT_AMI_01\",resume=True)\n",
        "run = wandb.init(project=\"T5_1024_MSFT_AMI_02\", id=\"3k5y8yms\", resume=\"allow\")\n",
        "\n",
        "config = wandb.config          # Initialize config\n",
        "config.TRAIN_BATCH_SIZE = 1    # input batch size for training (default: 64)\n",
        "config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n",
        "config.EPOCHS = 50        # number of epochs to train (default: 10)\n",
        "config.LEARNING_RATE = 0.0001   # learning rate (default: 0.01)\n",
        "config.SEED = 42               # random seed (default: 42)\n",
        "config.BEAMS = 12\n",
        "config.MAX_LEN = MAX_LEN\n",
        "config.SUMMARY_LEN = SUMMARY_LEN \n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwuqq09\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Resuming run <strong style=\"color:#cdcd00\">effortless-gorge-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_02\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_02</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_02/runs/3k5y8yms\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_02/runs/3k5y8yms</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201129_172358-3k5y8yms</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5vvXTv0OQLK"
      },
      "source": [
        "# https://deeplizard.com/learn/video/kWVgvsejXsE#:~:text=The%20num_workers%20attribute%20tells%20the,sequentially%20inside%20the%20main%20process\n",
        "# num_workers to default 0\n",
        "# This means that the training process will work sequentially inside the main process. \n",
        "# After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
        "\n",
        "TEST_BATCH_SIZE = 1 \n",
        "\n",
        "train_params = {\n",
        "  'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "  'shuffle': True,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "dev_params = {\n",
        "  'batch_size': config.VALID_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "test_params = {\n",
        "  'batch_size': TEST_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "training_loader = DataLoader(train_set, **train_params)\n",
        "dev_loader = DataLoader(dev_set, **dev_params)\n",
        "test_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKxdX61LW6dY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "d07ba2f057eb4e25ab463f604d72883a",
            "eb79ce14fc1f4de1b7ba7e37b50802ec",
            "dab0313e554c4572b53b71573fd8e3b9",
            "02d8556083714875a2a78efed0438f33",
            "949e634469f547168e9ad4a40d9d7508",
            "eabd842864744c9d86dda0a9eb9aec86",
            "aabb80b580bb480795ede4e40da2e608",
            "73453701b6b7468e8e4f724460ee97ba",
            "45597ea5cbb64ca8b4b7d479d5fabc4f",
            "b1aba3f71e8f4a2885bdfa8ca67cd882",
            "86fd0da30a114a6c9c845ce56760b809",
            "38a5dd84d08845729ac0604fc2db195e",
            "f7fe4dc296d240dfbaa6946184b78052",
            "839994bd05f04074ab360306f333012e",
            "3445529ef27d4fb5b121ee09910c7096",
            "a69c078d768d4b4e9f75e2a1e89daada"
          ]
        },
        "outputId": "1ba27ba6-ea71-427f-b73f-ffc704becfe7"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model = model.to(device)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d07ba2f057eb4e25ab463f604d72883a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45597ea5cbb64ca8b4b7d479d5fabc4f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePGtVuaUXqli"
      },
      "source": [
        "# optimizer \n",
        "# https://pytorch.org/docs/stable/optim.html\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whGS8TXqq5u7",
        "outputId": "6f328ce1-cdce-44ea-cb9e-6d739452adbb"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    eps: 1e-08\n",
              "    lr: 0.0001\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUD8KqP20gmT"
      },
      "source": [
        "# CP_TEMP_NAME = 'epoch10'\n",
        "# CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI512_NoNA/\" + CP_TEMP_NAME +\".pt\"\n",
        "# checkpoint = torch.load(CP_PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmes1VwYbjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912852ff-3d88-4531-bd84-a5afa6cfea18"
      },
      "source": [
        "training_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_AMI_02\"\n",
        "start_train_time = time.time()\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Training =================== ##\n",
        "  print(\"start training epoch\" + str(i))\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_TEMP_PATH = \"/content/drive/My Drive/W266/checkpoints/50EPOCH_Intransit_AMI1024_eax_largelr/\"+ CP_TEMP_NAME +\".pt\"\n",
        "  train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "  torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_epoch': i\n",
        "      }, CP_TEMP_PATH)\n",
        "  training_time = time.time() - start_train_time\n",
        "  print(\"done training epoch\" +str(i))\n",
        "  wandb.log({'epoch_traingTime': training_time,\n",
        "             'epoch': i})\n",
        "  print(\"--- %s seconds ---\" % (training_time))\n",
        "  training_time_log.append(training_time)\n",
        "  i+=1\n",
        "  ## ================= Validation =================== ##\n",
        "  # print(\"strat validation epoch\" + str(i))\n",
        "  # predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  # final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "  #                           'Golden_Abstractive_Text':actuals})\n",
        "  # final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "  # print(\"done validation epoch\" +str(i))\n",
        "\n",
        "  # rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "  #                                      final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  # wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "  #            'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "  #            'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "  #            'epoch': i})\n",
        "  # i+=1\n",
        "\n",
        "#run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "start training epoch0\n",
            "Epoch: 0, Loss:  8.18493366241455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1156: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done training epoch0\n",
            "--- 39.40698170661926 seconds ---\n",
            "start training epoch1\n",
            "Epoch: 1, Loss:  2.565901517868042\n",
            "done training epoch1\n",
            "--- 79.92581033706665 seconds ---\n",
            "start training epoch2\n",
            "Epoch: 2, Loss:  1.5311592817306519\n",
            "done training epoch2\n",
            "--- 119.23822951316833 seconds ---\n",
            "start training epoch3\n",
            "Epoch: 3, Loss:  2.290299892425537\n",
            "done training epoch3\n",
            "--- 161.71458554267883 seconds ---\n",
            "start training epoch4\n",
            "Epoch: 4, Loss:  0.5855029821395874\n",
            "done training epoch4\n",
            "--- 201.07582783699036 seconds ---\n",
            "start training epoch5\n",
            "Epoch: 5, Loss:  0.6651684045791626\n",
            "done training epoch5\n",
            "--- 240.0260841846466 seconds ---\n",
            "start training epoch6\n",
            "Epoch: 6, Loss:  0.5508021712303162\n",
            "done training epoch6\n",
            "--- 281.0104522705078 seconds ---\n",
            "start training epoch7\n",
            "Epoch: 7, Loss:  0.5083840489387512\n",
            "done training epoch7\n",
            "--- 322.24121260643005 seconds ---\n",
            "start training epoch8\n",
            "Epoch: 8, Loss:  0.5727953314781189\n",
            "done training epoch8\n",
            "--- 361.0538308620453 seconds ---\n",
            "start training epoch9\n",
            "Epoch: 9, Loss:  0.5249996781349182\n",
            "done training epoch9\n",
            "--- 400.15883207321167 seconds ---\n",
            "start training epoch10\n",
            "Epoch: 10, Loss:  0.3208249807357788\n",
            "done training epoch10\n",
            "--- 443.1959972381592 seconds ---\n",
            "start training epoch11\n",
            "Epoch: 11, Loss:  0.172369122505188\n",
            "done training epoch11\n",
            "--- 481.86354088783264 seconds ---\n",
            "start training epoch12\n",
            "Epoch: 12, Loss:  0.18810194730758667\n",
            "done training epoch12\n",
            "--- 521.1231179237366 seconds ---\n",
            "start training epoch13\n",
            "Epoch: 13, Loss:  0.07688718289136887\n",
            "done training epoch13\n",
            "--- 562.2599108219147 seconds ---\n",
            "start training epoch14\n",
            "Epoch: 14, Loss:  0.17245171964168549\n",
            "done training epoch14\n",
            "--- 601.1199314594269 seconds ---\n",
            "start training epoch15\n",
            "Epoch: 15, Loss:  0.1400088518857956\n",
            "done training epoch15\n",
            "--- 639.3632507324219 seconds ---\n",
            "start training epoch16\n",
            "Epoch: 16, Loss:  0.1354927122592926\n",
            "done training epoch16\n",
            "--- 677.6610443592072 seconds ---\n",
            "start training epoch17\n",
            "Epoch: 17, Loss:  0.24820487201213837\n",
            "done training epoch17\n",
            "--- 719.8821973800659 seconds ---\n",
            "start training epoch18\n",
            "Epoch: 18, Loss:  0.0024582589976489544\n",
            "done training epoch18\n",
            "--- 757.4980239868164 seconds ---\n",
            "start training epoch19\n",
            "Epoch: 19, Loss:  0.1078198179602623\n",
            "done training epoch19\n",
            "--- 793.9044396877289 seconds ---\n",
            "start training epoch20\n",
            "Epoch: 20, Loss:  0.197984978556633\n",
            "done training epoch20\n",
            "--- 834.844863653183 seconds ---\n",
            "start training epoch21\n",
            "Epoch: 21, Loss:  0.15253500640392303\n",
            "done training epoch21\n",
            "--- 874.2258768081665 seconds ---\n",
            "start training epoch22\n",
            "Epoch: 22, Loss:  0.18962153792381287\n",
            "done training epoch22\n",
            "--- 911.659485578537 seconds ---\n",
            "start training epoch23\n",
            "Epoch: 23, Loss:  0.10354618728160858\n",
            "done training epoch23\n",
            "--- 949.6807377338409 seconds ---\n",
            "start training epoch24\n",
            "Epoch: 24, Loss:  0.1911626160144806\n",
            "done training epoch24\n",
            "--- 991.0482783317566 seconds ---\n",
            "start training epoch25\n",
            "Epoch: 25, Loss:  0.0378163643181324\n",
            "done training epoch25\n",
            "--- 1030.566145658493 seconds ---\n",
            "start training epoch26\n",
            "Epoch: 26, Loss:  0.1961096227169037\n",
            "done training epoch26\n",
            "--- 1068.4136128425598 seconds ---\n",
            "start training epoch27\n",
            "Epoch: 27, Loss:  0.15858618915081024\n",
            "done training epoch27\n",
            "--- 1107.1085932254791 seconds ---\n",
            "start training epoch28\n",
            "Epoch: 28, Loss:  0.07361144572496414\n",
            "done training epoch28\n",
            "--- 1146.4772889614105 seconds ---\n",
            "start training epoch29\n",
            "Epoch: 29, Loss:  0.08861186355352402\n",
            "done training epoch29\n",
            "--- 1183.5512988567352 seconds ---\n",
            "start training epoch30\n",
            "Epoch: 30, Loss:  0.05785311758518219\n",
            "done training epoch30\n",
            "--- 1224.7406196594238 seconds ---\n",
            "start training epoch31\n",
            "Epoch: 31, Loss:  0.01784336194396019\n",
            "done training epoch31\n",
            "--- 1263.1803095340729 seconds ---\n",
            "start training epoch32\n",
            "Epoch: 32, Loss:  0.05470363050699234\n",
            "done training epoch32\n",
            "--- 1300.6849617958069 seconds ---\n",
            "start training epoch33\n",
            "Epoch: 33, Loss:  0.07704196870326996\n",
            "done training epoch33\n",
            "--- 1339.9934220314026 seconds ---\n",
            "start training epoch34\n",
            "Epoch: 34, Loss:  0.08265557885169983\n",
            "done training epoch34\n",
            "--- 1379.2015335559845 seconds ---\n",
            "start training epoch35\n",
            "Epoch: 35, Loss:  0.0022968731354922056\n",
            "done training epoch35\n",
            "--- 1418.0294575691223 seconds ---\n",
            "start training epoch36\n",
            "Epoch: 36, Loss:  0.13967396318912506\n",
            "done training epoch36\n",
            "--- 1455.7145738601685 seconds ---\n",
            "start training epoch37\n",
            "Epoch: 37, Loss:  0.09299590438604355\n",
            "done training epoch37\n",
            "--- 1494.9724493026733 seconds ---\n",
            "start training epoch38\n",
            "Epoch: 38, Loss:  0.09688428789377213\n",
            "done training epoch38\n",
            "--- 1535.356689453125 seconds ---\n",
            "start training epoch39\n",
            "Epoch: 39, Loss:  0.05563328042626381\n",
            "done training epoch39\n",
            "--- 1573.1366350650787 seconds ---\n",
            "start training epoch40\n",
            "Epoch: 40, Loss:  0.006356159690767527\n",
            "done training epoch40\n",
            "--- 1611.3821394443512 seconds ---\n",
            "start training epoch41\n",
            "Epoch: 41, Loss:  0.06615849584341049\n",
            "done training epoch41\n",
            "--- 1652.1712882518768 seconds ---\n",
            "start training epoch42\n",
            "Epoch: 42, Loss:  0.13529683649539948\n",
            "done training epoch42\n",
            "--- 1691.3420643806458 seconds ---\n",
            "start training epoch43\n",
            "Epoch: 43, Loss:  0.037566810846328735\n",
            "done training epoch43\n",
            "--- 1728.84499335289 seconds ---\n",
            "start training epoch44\n",
            "Epoch: 44, Loss:  0.06989608705043793\n",
            "done training epoch44\n",
            "--- 1770.3338479995728 seconds ---\n",
            "start training epoch45\n",
            "Epoch: 45, Loss:  0.1373133808374405\n",
            "done training epoch45\n",
            "--- 1809.0081627368927 seconds ---\n",
            "start training epoch46\n",
            "Epoch: 46, Loss:  0.03402138873934746\n",
            "done training epoch46\n",
            "--- 1846.817186832428 seconds ---\n",
            "start training epoch47\n",
            "Epoch: 47, Loss:  0.045994725078344345\n",
            "done training epoch47\n",
            "--- 1884.4111692905426 seconds ---\n",
            "start training epoch48\n",
            "Epoch: 48, Loss:  0.031193207949399948\n",
            "done training epoch48\n",
            "--- 1926.2722771167755 seconds ---\n",
            "start training epoch49\n",
            "Epoch: 49, Loss:  0.17074629664421082\n",
            "done training epoch49\n",
            "--- 1965.8508989810944 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tVOuLscBjxq",
        "outputId": "d2712137-184b-452d-e41c-8c6b1b095d12"
      },
      "source": [
        "validation_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_AMI_02_smalllr\"\n",
        "start_validation_time = time.time()\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Validation =================== ##\n",
        "  print(\"strat validation epoch\" + str(i))\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "  model = model.to(device)\n",
        "  # optimizer \n",
        "  # https://pytorch.org/docs/stable/optim.html\n",
        "  optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_PATH = \"/content/drive/My Drive/W266/checkpoints/50EPOCH_Intransit_AMI1024_eax_smalllr/\" + CP_TEMP_NAME +\".pt\"\n",
        "  checkpoint = torch.load(CP_PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  wandb.watch(model, log='all')\n",
        "\n",
        "  predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "                            'Golden_Abstractive_Text':actuals})\n",
        "  final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "\n",
        "  rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "                                       final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  validation_time = time.time() - start_validation_time\n",
        "  validation_time_log.append(validation_time)\n",
        "\n",
        "  # amigold = pd.read_csv(AMI_PATH)\n",
        "  # icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "  rouge_results_perdoc = rouge_per_document(final_df,amigold)\n",
        "  wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "            'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "            'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "            'rouge1_doclevel': rouge_results_perdoc.get(\"rouge_1_f_score\"), \n",
        "            'rougeL_doclevel': rouge_results_perdoc.get(\"rouge_l_f_score\"),  \n",
        "            'rouge2_doclevel': rouge_results_perdoc.get(\"rouge_2_f_score\"),\n",
        "            'epoch_validationTime': validation_time,\n",
        "            'epoch': i})\n",
        "  print(\"done validation epoch\" +str(i))\n",
        "  i+=1\n",
        "\n",
        "\n",
        "run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "strat validation epoch0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch0\n",
            "strat validation epoch1\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch1\n",
            "strat validation epoch2\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch2\n",
            "strat validation epoch3\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch3\n",
            "strat validation epoch4\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch4\n",
            "strat validation epoch5\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch5\n",
            "strat validation epoch6\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch6\n",
            "strat validation epoch7\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch7\n",
            "strat validation epoch8\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch8\n",
            "strat validation epoch9\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch9\n",
            "strat validation epoch10\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch10\n",
            "strat validation epoch11\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch11\n",
            "strat validation epoch12\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch12\n",
            "strat validation epoch13\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch13\n",
            "strat validation epoch14\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch14\n",
            "strat validation epoch15\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch15\n",
            "strat validation epoch16\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch16\n",
            "strat validation epoch17\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch17\n",
            "strat validation epoch18\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch18\n",
            "strat validation epoch19\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch19\n",
            "strat validation epoch20\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch20\n",
            "strat validation epoch21\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch21\n",
            "strat validation epoch22\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch22\n",
            "strat validation epoch23\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch23\n",
            "strat validation epoch24\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch24\n",
            "strat validation epoch25\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch25\n",
            "strat validation epoch26\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch26\n",
            "strat validation epoch27\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch27\n",
            "strat validation epoch28\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch28\n",
            "strat validation epoch29\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch29\n",
            "strat validation epoch30\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch30\n",
            "strat validation epoch31\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch31\n",
            "strat validation epoch32\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch32\n",
            "strat validation epoch33\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch33\n",
            "strat validation epoch34\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch34\n",
            "strat validation epoch35\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch35\n",
            "strat validation epoch36\n",
            "Completed 0\n",
            "Completed 100\n",
            "done validation epoch36\n",
            "strat validation epoch37\n",
            "Completed 0\n",
            "Completed 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzS-XNA3z9wg",
        "outputId": "0de5425c-11a0-44d1-c366-558df8300b9a"
      },
      "source": [
        "  len(final_df)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aOhtnmG0DRk",
        "outputId": "56a0ee0a-4d98-42db-fdc5-e47a76de86ad"
      },
      "source": [
        "final_df_exam = final_df.dropna(subset=['Generated_Abstractive_Summary'])\n",
        "final_df_exam = final_df_exam.reset_index(drop=True)\n",
        "len(final_df_exam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyhLFo3N0SD0"
      },
      "source": [
        "rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "                                     final_df.Golden_Abstractive_Text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5lBVnro12G0"
      },
      "source": [
        "rouge_results_perdoc = rouge_per_document(final_df,amigold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBu3Byul13bM",
        "outputId": "d313fc37-41a5-4e8c-c7b9-fdb6b9ac41bc"
      },
      "source": [
        "rouge_results_perdoc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge_1_f_score': 0.37213286129867573,\n",
              " 'rouge_2_f_score': 0.10793264420054217,\n",
              " 'rouge_l_f_score': 0.29351381850775093}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL3L1l4h15lA",
        "outputId": "f44933a6-c6c4-48ae-c9ff-82642ecbbe14"
      },
      "source": [
        "i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptlFcGar18hF",
        "outputId": "eccc2ea4-d9aa-4439-facc-ec5c07366576"
      },
      "source": [
        "validation_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2802.2264659404755"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiml8KGSeSGB"
      },
      "source": [
        "#### Checkpoint \n",
        "\n",
        "Remember to change the CP_NAME to a new model pt name.\n",
        "\n",
        "The model is then saved as checkpoints to Google Drive with the related tunable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVgDGjccbZQY"
      },
      "source": [
        "# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
        "# Checkpoint Saving\n",
        "CP_NAME = MODEL_NAME\n",
        "\n",
        "CP_TRAIN_EPOCHS = TRAIN_EPOCHS\n",
        "CP_DEV_EPOCHS = DEV_EPOCHS\n",
        "CP_LEARNING_RATE = LEARNING_RATE\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/\"+ CP_NAME +\".pt\"\n",
        "CP_MAX_LEN = MAX_LEN\n",
        "CP_SUMMARY_LEN = SUMMARY_LEN\n",
        "CP_TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
        "CP_DEV_BATCH_SIZE = DEV_BATCH_SIZE\n",
        "CP_MODEL = 'T5ForConditionalGeneration,t5-small'\n",
        "CP_OPTIMIZER_OPTION = 'Adam'\n",
        "CP_LOSSLIST = losslist\n",
        "CP_TEST_OPTIONS = {\n",
        "    \"num_beams\":          12,\n",
        "    \"repetition_penalty\": 2.5, \n",
        "    \"length_penalty\":     1.0, \n",
        "    \"early_stopping\":     True\n",
        "}\n",
        "CT_TRAIN_TIME = training_time\n",
        "#CT_EVALUATE_TIME = evaluating_time\n",
        "\n",
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_epoch': CP_TRAIN_EPOCHS,\n",
        "            'dev_epoch': CP_DEV_EPOCHS,\n",
        "            'learning_rate': CP_LEARNING_RATE,\n",
        "            'max_source_length':CP_MAX_LEN,\n",
        "            'max_target_length':CP_SUMMARY_LEN,\n",
        "            'train_batch_size':CP_TRAIN_BATCH_SIZE,\n",
        "            'dev_batch_size':CP_DEV_BATCH_SIZE,\n",
        "            'model_option':CP_MODEL,\n",
        "            'optimizer_option':CP_OPTIMIZER_OPTION,\n",
        "            'losslist': CP_LOSSLIST,\n",
        "            'training_time': CT_TRAIN_TIME,\n",
        "            #'evaluating_time': CT_EVALUATE_TIME,\n",
        "            'test_option': CP_TEST_OPTIONS\n",
        "            }, CP_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-EORf2EbxV3",
        "outputId": "7db9540d-f6a2-41a4-fd93-417f63e01b10"
      },
      "source": [
        "MODEL_NAME = \"epoch61\"\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/\" + MODEL_NAME +\".pt\"\n",
        "print(CP_PATH)\n",
        "checkpoint = torch.load(CP_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# train_epoch = checkpoint['train_epoch']\n",
        "# dev_epoch = checkpoint['dev_epoch']\n",
        "# losslist = checkpoint['losslist']\n",
        "# learning_rate = checkpoint['learning_rate']\n",
        "# max_source_length = checkpoint['max_source_length']\n",
        "# max_target_length = checkpoint['max_target_length']\n",
        "# train_batch_size = checkpoint['train_batch_size']\n",
        "# dev_batch_size = checkpoint['dev_batch_size']\n",
        "# optimizer_option = checkpoint['optimizer_option']\n",
        "# test_option = checkpoint['test_option']\n",
        "# training_time = checkpoint['training_time']\n",
        "\n",
        "\n",
        "# evaluating_time = checkpoint['evaluating_time']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/epoch61.pt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}