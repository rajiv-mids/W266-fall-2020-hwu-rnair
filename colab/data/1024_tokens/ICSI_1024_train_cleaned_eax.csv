meeting,extractive,abstractive
Bdb001.A,"you 're essentially defining a lattice . that 's actually very nicely handled here because you could all you 'd have to change is the , time stamps in the time line without , changing the i ds . except the time line is gonna be huge . suppose you have a phone level alignment . do they already have something that 's that would be useful for us in place ? if it 's conceptually close , and they already have or will have tools that everybody else will be using , it would be crazy to do something s separate that it seems to me you want to keep the frame level separate . now how would you represent , multiple speakers in this framework ? but how in the nist format do we express a hierarchical relationship between , say , an utterance and the words within it ? f say , one segmentation is in terms of , sentences . and another segmentation is in terms of , i don't know , prosodic phrases . and let 's say that they don't nest . but , when it comes to annotations , you often find yourself in the situation where you have different annotations of the same , say , word sequence . and sometimes the word sequences even differ slightly because they were edited s at one place but not the other . but , see , if you 'd annotate dialogue acts , you don't necessarily want to or topics you don't really want to be dealing with time marks . and you want some tool , that is able to merge these different annotations back into a single , version . but once you have a file format , imagine writing not personally , but someone writing a tool that is essentially an alignment tool , that mediates between various versions , ",
Bdb001.B,,
Bdb001.C,"we i already have developed an xml format for this tha it has a single time line , and then you have a bunch of times . and then , i also wanted to be i to be able to not specify specifically what the time was and just have a stamp . and then somewhere la further down you might have something like an utterance tag which has "" start equals t seventeen , what that 's saying is , we know it starts at this particular time . you could have some other tag later in the file that would be something like , i don't know , "" noise type equals door slam "" . and then , you could either say "" time equals a particular time mark "" or you could do other sorts of references . it 's parsing limitations . i don't want to have this text file that you have to read in the whole thing to do something very simple for . for word level , this would be for lower than word level , you 're talking about much data that don't know . or any frame level would use p file . it 's ics icsi has a format for frame level representation of features . or there 's a particular way in xml to refer to external resources . more compact , because you have a two gigabyte limit on most o ss . think it 's debatable whether you want to do phone level in the same thing . but a anything at frame level , even p file , is too verbose . it 's whatever you want , actually . built into it is the concept of frames , utterances , sentences , that thing , that structure . and then also attached to it is an arbitrary vector of values . and we have a lot of tools already to deal with it . man pages and , source code , and me . it 's something that we developed at icsi . and , we have a configured system that you can distribute for free , we should look at atlas , the nist thing , th there are two choices . your file format can know about know that you 're talking about language and speech , which is what i chose , and time , or your file format can just be a graph representation . and then the application has to impose the structure on top . what it looked like atlas chose is , they chose the other way , which was their file format is just nodes and links , and you have to interpret what they mean yourself . and it was better if you 're looking at a raw file to be t for the tags to say "" it 's an utterance "" , as opposed to the tag to say "" it 's a link "" . i chose this for a couple reasons . one of them is that it 's easy to parse .  what nist would say is that instead of doing this , you would say something like "" link start equals , some node id , end equals some other node id "" , and then "" type "" would be "" utterance "" . they 're developing a big infrastructure . the last time i looked at it was a while ago , probably a year ago , and specifically they didn't have any external format representation at that time . they just had the conceptual node annotated transcription graph , since then , they 've developed their own external file format , and they 've also developed a lot of tools , but i haven't looked at them . should . th what would what would worry me is that we might miss a little detail 'll take a closer look at it . the other thing the other way that i established this was as easy translation to and from the transcriber format . with this , though , is that you can't really add any supplementary information . there 's a spea speaker tag up at the top which identifies them the way i had it is each turn or each utterance , i don't even remember now , had a speaker id tag attached to it . you would have another structure lower down than this that would be saying they 're all belonging to this id . 'm i had better look at it again i 'm pretty that you can do that , but i 'm forgetting the exact level of nesting . there 's a standard again in xml , specifically for searching xml documents structured x xml documents , where you can specify both the content and the structural position . it 's you would use that to build your tool to do that search . no matter what format you choose , you 're gonna have the trou you 're gonna have the difficulty of relating the frame level features one of the things that atlas is doing is they 're trying to define an api which is independent of the back store , that , you could define a single api and the storage could be flat xml files or a database . my opinion on that is for the s that we 're doing , i suspect it 's overkill to do a full relational database , that , just a flat file and , search tools i bet will be enough . 'm just a little hesitant to try to go whole hog on the whole framework that nist is talking about , with atlas and a database and all that cuz it 's a big learning curve , just to get going . if you wanted to use the structured xml query language , that 's a different thing . what you would do is , someone would build a tool that used that as a library . ","On the one hand, a bespoke XML structure that connects transcriptions and annotations (down to the word-level) to a common timeline. The respective frame-level representation can be handled by P-files, a technology developed at ICSI, which also comes with a library of tools. Separation of levels of analysis makes files more compact and manageable. Phone-level analysis can be included in the same structure, or in a separate, linked file. Two main options were discussed as to the organisation of the collected data. On the other hand, the ATLAS (NIST) technology offers a very similar, but more generic organisational scheme based on nodes and links. Its advantages are that it is easier to read, parse, map onto the Transcriber format and to expand with extra features. These are labeled with domain specific types, like ""utterance"" or ""speaker"". This option offer well-developed infrastructure and flexibility as to the type of data storage (flat XML files or relational database). XML standards offer libraries that can be used for the development of search tools. "
Bdb001.C,"i have to look at it again to see whether it can really do what we want , but if we use the atlas external file representation , it seems like it 's rich enough that you could do quick tools just as i said in perl , and then later on if we choose to go up the learning curve , we can use the whole atlas inter infrastructure , th the reason i like p file is i 'm already familiar with it , but , it is just something we developed at icsi . there 's an api for it . a bunch of libraries , p file utilities . i don't see any way that file formats are gonna help us with that . the hard part isn't the file format . the hard part is specifying what by "" merge "" . ","The respective frame-level representation can be handled by P-files, a technology developed at ICSI, which also comes with a library of tools. "
Bdb001.D,"but what 's the advantage of doing that versus just putting it into this format ? p file for each frame is storing a vector of cepstral or plp values , what about , the idea of using a relational database to , store the information from the xml ? which allows you to do all kinds of good search things in there . ",
Bdb001.E,"if we tie the overlap code to the first word in the overlap , then you 'll have a time marking . it won't it 'll be independent of the time bins , however these e evolve , shrink , or whatever , ",
Bdb001.F,"and the main thing that i was gonna ask people to help with today is to give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts , anything that transcribers or discourse coders or whatever put in the signal , with time marks for words and phone boundaries and all the we get out of the forced alignments and the recognizer . and what do you do if you just conceptually , if you get , transcriptions where the words are staying but the time boundaries are changing , cuz you 've got a new recognition output , or s what 's the , sequence of going from the waveforms that stay the same , the transcripts that may or may not change , and then the utterance which where the time boundaries that may or may not change ? that you could call that you would tie into this representation with like an id . these are long meetings these are big files . but something standard enough that , if we start using this we can give it out , other people can work on it , but it would be good to get something that we can that other people can use or adopt for their own kinds of encoding . and especially for the prosody work , what it ends up being is you get features from the signal , you want to recompute your features , and then keep the database up to date . or you change a word , or you change a utterance boundary segment , which is gonna happen a lot . and wanted something where all of this can be done in a elegant way and that if somebody wants to try something or compute something else , that it can be done flexibly . it just has to be , easy to use , and can you but you can add to those structures if you you have to make a different type . we would be taking the format and enriching it with things that we wanna query in relation to the words that are already in the file , the two goals . one that chuck mentioned is starting out with something that we don't have to start over , that we don't have to throw away if other people want to extend it for other kinds of questions , and being able to at least get enough , information out on where we condition the location of features on information that 's in the file that you put up there . ","Two main options were discussed as to the organisation of the collected data. In either case, it is important for the chosen format to allow for fast searches, flexible updates and, if possible, be reusable in future work. Its advantages are that it is easier to read, parse, map onto the Transcriber format and to expand with extra features. "
Bmr001.A,,
Bmr001.B,,
Bmr001.C,,
Bmr002.A,,
Bmr002.B,,
Bmr002.C,,
Bmr002.D,,
Bmr008.A,,
Bmr008.B,,
Bmr008.C,,
Bmr008.D,,
Bmr008.E,,
Bmr008.F,,
Bmr009.A,"and i was just going to say that right now we 're just exploring . what you would imagine eventually , is that you 'll feed all of these features into some discriminative system . and even if one of the features does a good job at one type of overlap , another feature might do a good job at another type of overlap . this was the problem with these categories , i picked those categories from timit . 'm not what to do about the region field for english variety . but i don't know how to i don't know how to categorize them . did you guys get my email on the multitrans ? i have a version also which actually displays all the channels . the what the ones i applied , that you can actually do are dan 's , because it doesn't slow it down . no , the one that 's installed is fine . it 's not slow i wrote another version . which , instead of having the one pane with the one view , it has multiple panes with the views . but the problem with it is the drawing of those waveforms is slow that every time you do anything it just crawls . just about anything , and it was slow it was not usable . that the one dan has is usable enough . it doesn't display the others . it displays just the mixed signal . but you can listen to any of them . not if we 're going to use tcl tk at least not if we 're going to use snack . and it 's really it 's not too bad to find places in the stream where things are happening . if one of us sat down and coded it , that it could be displayed fast enough i 'm they would be quite willing to incorporate it . but it 's not a trivial task . ","Participants also reviewed the latest iteration of speaker forms, and discussed recent changes to the Transcriber tool. "
Bmr009.B,"when i presented my results about the distribution of overlaps and the speakers and the profiles of the speakers , at the bottom of that i did have a proposal , and i had plan to go through with it , of co coding the types of overlaps that people were involved in s just with reference to speaker style with reference that it 's like people may have different amounts of being overlapped with or overlapping but that in itself is not informative without knowing what types of overlaps they 're involved in was planning to do a taxonomy of types overlaps with reference to that . and this 'll be a hav having the multiwave will be a big help ",Participants discussed alternate strategies for examining energy and the importance of categorizing types of speaker overlap. 
Bmr009.C,"but jose and i were just talking about the speech e energy thing , right now , that he 's not really showing any distinction , but but we discussed a couple of the possible things that he can look at . and one is that this is all in log energy and log energy is compressing the distances between things . another is that he needs to play with the different temporal sizes . he was taking everything over two hundred milliseconds and he 's going to vary that number and also look at moving windows , as we discussed before . and and the other thing is that the doing the subtracting off the mean and the variance in the and dividing it by the standard deviation in the log domain , may not be the right thing to do . it 's between the pauses for some segment . and his he 's making the constraint it has to be at least two hundred milliseconds . and then he 's measuring at the frame level and then just normalizing with that larger amount . and but one thing he was pointing out is when he looked at a bunch of examples in log domain , it is actually pretty hard to see the change . what i was suggesting to him is that actually , a pdf . but also u good first indicator is when the researcher looks at examples of the data and can not see a change in how big the signal is , when the two speaker and when he 's looking in the log domain he 's not really seeing it . and when he 's looking in straight energy he is , that 's a good place to start . there 's a good chance then given that different people do talk different amounts that there is still a lot more to be gained from gain norm normalization with some sort but we were that in addition to that there should be s related to pitch and harmonics and forth . actually , you do have some distributions here , for these cases . and they don't look very separate . separated . before we get complicated , let 's start with the most basic wh thing , which is we 're arguing that if you take energy if you look at the energy , that , when two people are speaking at the same time , usually there 'll be more energy than when one is that 's that hypothesis . is that you would just take a look at the distribution of those two things , but i had made it too complicated by suggesting early on , that you look at scatter plots let 's start off just in one , with this feature . and then we w we 're that pitch related things are going to be a really likely candidate to help . but since your intuition from looking at some of the data , is that when you looked at the regular energy , that it did usually go up , when two people were talking , that 's you should be able to come up with a measure which will match your intuition . if you generated something like that just for the energy and see , and then , a as liz says , when they g have smaller more coherent groups to look at , that would be another interesting thing later . and then that should give us some indication between those , should give us some indication of whether there 's anything to be achieved f from energy and then you can move on to the more pitch related would just look at the energy and then get into the harmonicity as a suggestion . ","The Berkely Meeting Recorder group discussed efforts by speaker mn005 to measure energy levels in cases of speaker overlap in which the time window analyzed was 200 milliseconds or greater. Preliminary results were presented showing that log domain analyses did not reveal a significant difference in mean energy levels for windows of overlapping versus non-overlapping speech. In contrast, raw energy analyses were successful in showing the two groups to be distinct. "
Bmr009.D,"are these the long term means ? but it is definitely true that we need to have the time marks , and i was assuming that will be inherited because , if you have the words and they 're roughly aligned in time via forced alignment or whatever we end up using , then this student and i would be looking at the time marks we wouldn't be able to do any work without a forced alignment anyway , somehow if once he gets going we 're gonna hafta come up with one is there any hope for actually displaying the wave form ? ",
Bmr009.E,"this is the thing i comment with you before , that we have a great variation of th situation of overlapping . this is a good idea . not consider the log energy . another parameter we c we can consider is the duration . because is possible some s some classes has type of a duration , that you have you have a backchannel , you have a overlapping zone very short that the e effect of the normalization with the mean and the variance is different that if you consider only a window compared with the n the duration of overlapping . ",
Bmr010.A,"right . this would work for , pauses and utterance boundaries and things like that . we should get it and if it 's good enough we 'll arrange windows machines to be available . no , no . praat isn't praat 's multi platform . what our decision was is that we 'll go ahead with what we have with a not very fine time scale on the overlaps . no . they can only display one , we should definitely get with them then , agree upon a format . cross correlation . and i hope that if we do a forced alignment with the close talking mike , that will be enough to recover at least some of the time information of when the overlap occurred . one thing is that i did look on sony 's for a replacement for the mikes for the head m head worn ones cuz they 're uncomfortable . it seemed to me when i was using dragon that it was really microphone placement helped an in , an enormous amount . you want it enough to the side that when you exhale through your nose , it doesn't the wind doesn't hit the mike . and then just close enough that you get good volume . it would also be interesting to have , a couple of the meetings have more than one transcriber do , cuz i 'm curious about inter annotator agreement . right . if we fed the hand segmentation to javier 's and it doesn't work , then we know something 's wrong . that 's probably worthwhile doing . ","The Berkeley Meeting Recorder group talked about the ongoing transcription effort and issues related to the Transcriber tool, which despite its limitations for capturing tight time markings for overlapping speech, will continue to remain in use. Recording equipment and procedures were discussed, with a focus on audible breathing and the need for standards in microphone wear and use. "
Bmr010.B,"anything to reduce breathing is a good thing . one more remark , concerning the sri recognizer . it is useful to transcribe and then ultimately train models for things like breath , and also laughter is very , very frequent and important to model . the thing that you is hard to deal with is whe when they speak while laughing . ","Recording equipment and procedures were discussed, with a focus on audible breathing and the need for standards in microphone wear and use. "
Bmr010.C,"what we did far was using the mixed file to detect s speech or nonspeech portions in that . and what i did far is used our old munich system , which is an ba based system with gaussian mixtures for s speech and nonspeech . and it was a system which used only one gaussian for silence and one gaussian for speech . and now i added , multi mixture possibility for speech and nonspeech . and i did some training on one dialogue , which was transcribed by we did a nons s speech nonspeech transcription . and i did some pre segmentations for jane . and i 'm not how good they are or what the transcribers say . saw that there were loud loudly speaking speakers and quietly speaking speakers . and did two mixtures , one for the loud speakers and one for the quiet speakers . it 's just our old munich , loudness based spectrum on mel scale twenty critical bands and then loudness . and four additional features , which is energy , loudness , modified loudness , and zero crossing rate . you can specify the minimum length of speech or and silence portions which you want . changing the minimum length for s for silence to have more or less , silence portions in inserted . no . w we originally we did that for our recognizer in munich we saw that w it 's not it 's not necessary . it works as with without , a lda susanne bur burger , who is at se cmu , he wa who was formally at in munich and w and is now at with cmu , she said she has something which she uses to do eight channels , trans transliterations , about , adding , another class too . ",Speaker mn014 explained his efforts to pre-segment the signal into speech and non-speech portions for facilitating transcriptions. 
Bmr010.D,"what are the different , classes to code , the overlap , you will use ? because , i have the results , of the study of different energy without the law length . the other , the last w meeting we have problem to with the parameter with the representations of parameter , because the valleys and the peaks in the signal , look like , it doesn't follow to the energy in the signal . and it was a problem , with the scale . and i change the scale and we can see the variance . and , i have prepared the pitch tracker now . and i hope the next week i will have , some results and we will show we will see , the parameter the pitch , tracking in with the program . but , what did you think about the possibility of using the javier software ? using the mark , by hand , to distinguish be to train overlapping zone and speech zone . but it 's possible with my segmentation by hand that we have information about the overlapping , ","And, finally, it was determined that speaker mn005's efforts to detect speaker overlap using energy should instead be focussed on pitch- and harmonicity-related features or be guided by a non-featural, statistical approach, i.e. via the use of Markov models. "
Bmr010.E,,
Bmr010.F,"give you an update on the transcription effort . raise the issue of microphone , procedures with reference to the cleanliness of the recordings . the we have great , p steps forward in terms of the nonspeech speech pre segmenting of the signal . it 's a big improvement . they think it 's a terrific improvement . but it saves much time the transcribers because at present , because of the limitations of th the interface we 're using , overlaps are , not being encoded by the transcribers in as complete and , detailed a way as it might be , we don't have start and end points at each point where there 's an overlap . we just have the overlaps encoded in a simple bin . @ the limits of the over of the interface are such that we were at this meeting we were entertaining how we might either expand the interface or find other tools which already do what would be useful . because what would ultimately be , ideal in my view and mean , i had the sense that it was consensus , is that , a thorough going musical score notation would be the best way to go . because you can have multiple channels , there 's a single time line , it 's very clear , flexible , and all those things . this is called praat , which means spee speech in dutch and i was just thinking that , if it were possible to bring that in this week , then when they 're encoding the overlaps it would be for them to be able to specify when the start points and end points of overlaps . th they 're making really quick progress . and , dan ellis 's hack already allows them to be able to display different waveforms to clarify overlaps and things , yes , but what is that , from the transcriber 's perspective , those two functions are separate . and dan ellis 's hack handles the , choice the ability to choose different waveforms from moment to moment . the hack to preserve the overlaps better would be one which creates different output files for each channel , which then would also serve liz 's request of having , separable , cleanly , easily separable , transcript tied to a single channel , audio . i 'm keeping the conventions as simple as possible . and dan gel and dave gelbart is interested in pursuing the aspect of using amplitude as a as a basis for the separation . cuz there is one thing that we don't have right now and that is the automatic , channel identifier . that , that would g help in terms of encoding of overlaps . the transcribers would have less , disentangling to do if that were available . types of overlap ? it i the it 's two tiered structure where the first one is whether the person who 's interrupted continues or not . and then below that there 're subcategories , that have more to do with , is it , simply backchannel or is it , someone completing someone else 's thought , or is it someone in introducing a new thought . they 're putting in curly brackets they put "" inhale "" or "" breath "" . it they and then in curly brackets they say "" laughter "" . and i also thought , y liz has this , and i do also , this interest in the types of overlaps that are involved . these people would be great choices for doing coding of that type if we wanted , ","The Berkeley Meeting Recorder group talked about the ongoing transcription effort and issues related to the Transcriber tool, which despite its limitations for capturing tight time markings for overlapping speech, will continue to remain in use. Recording equipment and procedures were discussed, with a focus on audible breathing and the need for standards in microphone wear and use. Speaker mn014 explained his efforts to pre-segment the signal into speech and non-speech portions for facilitating transcriptions. "
Bmr010.G,"is there a transformation , like principal components transformation have , folks from nist been in contact with you ? they seem to want to get clear on standards for transcription standards and forth with us . i had mentioned this a couple times before , the c the commercial devices that do , voice , active miking , look at the amp at the energy at each of the mikes . and you compare the energy here to some function of all of the mikes . by doing that , rather than setting any , absolute threshold , you actually can do pretty good , selection of who 's talking . let 's why don't we talk about microphone issues ? but that it doesn't hurt , the naturalness of the situation to try to have people wear the microphones properly , if possible , the other thing we could do , actually , is , use them for a more detailed analysis of the overlaps . but the bottom line is it 's still not , separating out very but you don't want to keep , keep knocking at it if it 's if you 're not getting any result with that . but , the other things that we talked about is , pitch related things and harmonicity related things , a completely different tack on it wou is the one that was suggested , by your colleagues in spain , which is to say , don't worry much about the , features . that is to say , use , as you 're doing with the speech , nonspeech , use some very general features . and , then , look at it more from the aspect of modeling . have a couple markov models and , try to indi try to determine , w when is th when are you in an overlap , when are you not in an overlap . and let the , statistical system determine what 's the right way to look at the data . you have , nonspeech , single person speech , and multiple person speech ? and then you have a markov model for each ? far , jose has been the has , been exploring , e largely the energy issue as with a lot of things , it is not like this , it 's not as simple as it sounds . and then there 's , is it energy ? is it log energy ? is it lpc residual energy ? is it is it , delta of those things ? should there be a long window for the normalizing factor and a short window for what you 're looking at ? and far at least has not come up with any combination that really gave you an indicator . but it may be given that you have a limited time here , it just may not be the best thing to focus on for the remaining of it . but it seems like if we just wanna get something to work , that , their suggestion of th they were suggesting going to markov models , but in addition there 's an expansion of what javier did . and one of those things , looking at the statistical component , even if the features that you give it are not ideal for it , it 's just this general filter bank ","Recording equipment and procedures were discussed, with a focus on audible breathing and the need for standards in microphone wear and use. And, finally, it was determined that speaker mn005's efforts to detect speaker overlap using energy should instead be focussed on pitch- and harmonicity-related features or be guided by a non-featural, statistical approach, i.e. via the use of Markov models. "
Bmr011.A,"and actually in addition to that , that the close talking mikes are worn in such a way as to best capture the signal . it adds this extra , vari variable for each speaker to deal with when the microphones aren't similar . but if we could actually standardize , the microphones , as much as possible that would be really helpful . it seems to me that there 's there are good political reasons for doing this , it 'd be if we can have at least , make use of the data that we 're recording as we go since it 's this is the first site that has really collected these really impromptu meetings , and just have this other information available . if we can get the investment in just for the infra infrastructure it 'd be g it 'd be good to have the recording . but it definitely in the case of microphone arrays , since if there was a community interested in this , then but i know there is interest from other places that are interested in looking at meeting data and having the video . there are a lot of words that are reduced phonetically that make sense when what the person was saying before . for the sri front end , we really need to chop things up into pieces that are f not too huge . but second of all , in general because some of these channels , some of the segments have a lot of cross talk . it 's good to get short segments if you 're gonna do recognition , especially forced alignment . we have to normalize the front end and forth , and have these small segments . we 've taken that and chopped it into pieces based always on your , cuts that you made on the mixed signal . and that every speaker has the same cuts . the problem is if we have no time marks , then for forced alignment we actually don't know where in the signal the transcriber heard that word . th but there 's going to be a real problem , even if we chop up based on speech silence these , the transcripts from i b m , we don't actually know where the words were , which segment they belonged to . wanted to , make a pitch for trying to collect more meetings . because it 'd be good if we can get a few different non internal types of meetings the other thing is that there was a number of things at the transcription side that , transcribers can do , like dialogue act tagging , disfluency tagging , things that are in the speech that are actually something we 're y working on for language modeling . but that , it would be helpful if stay in the loop somehow with , people who are doing any post processing , all i meant is just that as as this pipeline of research is going on we 're also experimenting with different asr , techniques . r right , although if they 're not talking , using the inhouse transcriptions , were k we try to find as close of start and end time of as we can to the speech from an individual speaker , because then we 're more guaranteed that the recognizer will for the forced alignment which is just to give us the time boundaries , because from those time boundaries then the plan is to compute prosodic features . and the more space you have that isn't the thing you 're trying to align the more errors we have . because of the fact that there 's enough acoustic signal there t for the recognizer to eat , as part of a word . but we probably will have to do something like that in addition . because that 'll really help us . ","The discussion was largely focused on efforts to facilitate transcriptions, including the improvement of strategies for transcribing overlapping speech, and achieving greater uniformity in the type of equipment used during recordings and the manner in which recording devices are worn by speakers. The Berkeley Meeting Recorder group discussed recording equipment and setup issues, recent developments in the transcription effort, other potential types of tagging to be assigned to transcribers, and the post-processing of waveforms. "
Bmr011.B,"the one issue was that the , lapel mike , isn't as good as you would like . and it 'd be better if we had close talking mikes for everybody . one thing i was gonna say was that , i we could get more , of the head mounted microphones why don't we just go out and get an order of i 'd just get a half dozen of these things . we should go out to our full complement of whatever we can do , but have them all be the same mike . the original reason that it was done the other way was because , it w it was an experimental thing and i don't think anybody knew whether people would rather have more variety or , more uniformity , dave gelbart and i will be , visiting with john canny who i is a cs professor , who 's interested in ar in array microphones . they 'd wanna stick an array mike here when we 're doing things but they might wanna just , you could imagine them taking the four signals from these table mikes and trying to do something with them if i 'm speaking , or if you 're speaking , or someone over there is speaking , it if you look at cross correlation functions , you end up with a if someone who was on the axis between the two is talking , then you get a big peak there . and then , it even looks different if th t if the two people on either side are talking than if one in the middle . it actually looks somewhat different , they wanted to do it think they 've instrumented a room but i don't think they haven't started recordings yet . all i know is that they 've been talking to me about a project that they 're going to start up recording people meet in meetings . and , it is related to ours . they were interested in ours . they wanted to get some uniformity with us , about the transcriptions and on . but one , difference from the audio side was that they are interested in using array mikes . the reason i didn't go for that here was because , the focus , both of my interest and of adam 's interest was in impromptu situations . we 're not recording a bunch of impromptu situations but the thing we ultimately wanted to aim at was a situation where you were talking with , one or more other people i in an p impromptu way , where you didn't actually the situation was going to be . and therefore it would not it 'd be highly unlikely that room would be outfitted with some very carefully designed array of microphones . it 's a good thing to do , but it doesn't solve the problem of how do you solve things when there 's one mike or at best two mikes in this imagined pda that we have . we 'll do some more of it . and think we could get a microphone array in here pretty easily and , have it mixed to one channel of some sort . but , e for for maximum flexibility later you really don't want to end up with just one channel that 's pointed in the direction of the p the person with the maximum energy like that . you want actually to have multiple channels being recorded that you can i 'm not much worried about disk space actually . but the real issue is that , there is no way to do a recording extended to what we have now with low skew . but if you 're d i the person who 's doing array processing you actually care about funny little times . and you actually wou would want to have a completely different set up than we have , one that would go up to thirty two channels i 'm kinda skeptical , but what we could do is if there was someone else who 's interested they could have a separate set up which they wouldn't be trying to synch with ours which might be useful for them . we can o offer the meetings , and the physical space , and the transcripts , and on . there 's this human subjects problem . once you actually have serious interest in any of these things then you actually have to put a lot of effort in . think nist or ldc , or somebody like that is much better shape to do all that . some point ago we thought that it "" boy , we 'd really have to ramp up to do that "" , and , here 's , a , collaborating institution that 's volunteered to do it . that was a contribution they could make . in terms of time , money , that they are , and we have to have a dialogue with them about it . but how do we step out the recorded meetings ? and the other one is , is there some good use that we can make of the transcribers to do other things ? right , think we talking about three level three things . one was we had s had some discussion in the past about some very high level labelings , types of overlaps , and forth that someone could do . second was , somewhat lower level just doing these more precise timings . and the third one is , just a completely wild hair brained idea that i have which is that , if , if we have time and people are able to do it , to take some subset of the data and do some very fine grained analysis of the speech . marking in some overlapping potentially overlapping fashion , the value of , ar articulatory features . ","The discussion was largely focused on efforts to facilitate transcriptions, including the improvement of strategies for transcribing overlapping speech, and achieving greater uniformity in the type of equipment used during recordings and the manner in which recording devices are worn by speakers. The Berkeley Meeting Recorder group discussed recording equipment and setup issues, recent developments in the transcription effort, other potential types of tagging to be assigned to transcribers, and the post-processing of waveforms. "
Bmr011.B,"and then this would give some more ground work for people who were building statistical models that allowed for overlapping changes , different timing changes as opposed to just "" click , no , it 's for that purpose i 'm just viewing meetings as being a neat way to get people talking naturally . and then you have i and then it 's natural in all senses , in the sense that you have microphones that are at a distance that one might have , and you have the close mikes , and you have people talking naturally . and the overlap is just indicative of the fact that people are talking naturally , think , if he 's thinking in terms of recognition technology he would probably want , american english , ",
Bmr011.C,"what i 'm doing right now is i 'm trying to include some information about which channel , there 's some speech in . i 'm just trying to do this by comparing energies , normalizing energies and comparing energies of the different channels . to give the transcribers some information in which channel there 's speech in addition to the thing we did now which is just , speech nonspeech detection on the mixed file . 'm relying on the segmentation of the mixed file but i 'm trying to subdivide the speech portions into different portions if there is some activity in different channels . ",
Bmr011.D,,
Bmr011.E,"that reminds me , i had a thought of an interesting project that somebody could try to do with the data from here , and that is to try to construct a map of where people were sitting , and you could plot out who was sitting next to who is there an interest in getting video recordings for these meetings ? there 's not enough interest to overcome all of but there could be problems , right ? with that . given all of the effort that is going on here in transcribing why do we have i b m doing it ? why not just do it all ourselves ? the problem is like , on the microphone of somebody who 's not talking they 're picking up signals from other people and that 's causing problems ? ",
Bmr011.F,,
Bmr011.G,"it 's the equipment and also how it 's worn . it 's really it makes a big difference from the transcribers ' point of view and , in terms of the multi trans , that 's being modified by dave gelbart to , handle multi channel recording . and , that 's that will enable us to do tight time marking of the beginning and ending of overlapping segments . at present it 's not possible with limitations of the , original design of the software . in terms of pre segmentation , that continues to be , a terrific asset to the transcribers . first of all , i 've got eight transcribers . seven of them are linguists . one of them is a graduate student in psychology . and , meetings , that they 're they go as long as a almost two hours in some cases . but the pre segmentation really helps a huge amount . and , also dan ellis 's innovation of the , the multi channel to here really helped a r a lot in terms of clearing up h hearings that involve overlaps . just out of curiosity i asked one of them how long it was taking her , one of these two who has already finished her data set . she said it takes about , sixty minutes transcription for every five minutes of real time . which is what we were thinking . it 's in the range . but it 's word level , speaker change , the things that were mentioned . now i wanted to mention the , teleconference i had with , jonathan fiscus . he , he in indicated to me that they 've that he 's been , but spending a lot of time with the atlas system . but it looks to me like that 's the name that has developed for the system that bird and liberman developed for the annotated graphs approach . and what we will do and is to provide them with the u already transcribed meeting for him to be able to experiment with in this atlas system . and that he wants to experiment with taking our data and putting them in that format , and see how that works out . now i also wanted to say in a different direction is , brian kingsbury . i told him he could ssh on and use multi trans , and have a look at the already done , transcription . and he did . and what he said was that , what they 'll be providing is will not be as fine grained in terms of the time information . he downloaded from the cd onto audio tapes . and he did it one channel per audio tape . each of these people is transcribing from one channel . except say that my transcribers use the mixed signal mostly unless there 's a huge disparity in terms of the volume on the mix . in which case , they wouldn't be able to catch anything except the prominent channel , then they 'll switch between . now wasn't that one of the proposals was that ibm was going to do an initial forced alignment , which is , jonathan fiscus expressed primar a major interest in having meetings which were all english speakers . ","The discussion was largely focused on efforts to facilitate transcriptions, including the improvement of strategies for transcribing overlapping speech, and achieving greater uniformity in the type of equipment used during recordings and the manner in which recording devices are worn by speakers. "
Bmr011.H,"it 's towards the corner of your mouth that breath sounds don't get on it . and then just about , a thumb or a thumb and a half away from your mouth . that the point of doing the close talking mike is to get a good quality signal . we 're not doing research on close talking mikes . we might as get it as uniform as we can . as i said , we 'll do a field trip and see if we can get all of the same mike that 's more comfortable than these things , which are horrible . dan had worked on that . dan ellis , that 's the cross correlation was doing b beam forming . nist has done a big meeting room instrumented meeting room with video and microphone arrays , and very elaborate software . to actually get a microphone array and do that ? yes , but it 's exactly the same problem , that you have an infrastructure problem , you have a problem with people not wanting to be video taped , and you have the problem that no one who 's currently involved in the project is really hot to do it . the one the what you 're referring to , they have this concept of an annotated transcription graph representation . and it 's a data representation and a set of tools for manipulating transcription graphs of various types . but we had this we 've had this discussion many times . and the answer is we don't actually know the answer because we haven't tried both ways . liz , with the sri recognizer , can it make use of some time marks ? ","The discussion was largely focused on efforts to facilitate transcriptions, including the improvement of strategies for transcribing overlapping speech, and achieving greater uniformity in the type of equipment used during recordings and the manner in which recording devices are worn by speakers. "
Bmr012.A,"suddenly the overall error rate when we first ran it was like eighty percent but i looking at the first sentences looked much better than that and then suddenly it turned very bad and then we noticed that the reference was always one off with the we have everything recognized but we scored only the first whatever , up to that time to there are a fair number of errors that are , where got the plural s wrong or the inflection on the verb wrong .  i should say that the language model is not just switchboard there 's actually more data is from broadcast news but with a little less weight our complete system starts by doing ge a gender detection and it might be reassuring for everybody to know that it got all the genders right . clearly there are with just a small amount of actual meeting transcriptions thrown into the language model you can probably do quite a bit better we talked about setting up the sri recognizer here . that 's if there are more machines here plus people can could run their own variants of the recognition runs the other thing we should try is to just take the original wave forms , segment them but not downsample them . and feed them to the sri recognizer and see if the sri front end does something . and if for some reason we see that it works better then we might investigate why mel cepstrum . ",
Bmr012.B,"the acoustic models were also from switchboard or certainly i 'd like to see as soon as we could , get some of the glitches out of the way but soon as we could how it does with say with the p z ms or even one of the we just have to go through this process of having people approve the transcriptions , are you using mel cepstrum or plp over there ? this is being recorded at forty eight kilohertz . which is more that anybody needs sixteen is more common for broadband that isn't that isn't music and isn't telephone , but is this thing of two eight channel boards a maximum for this setup or could we go to a third board ? already we have a group of people in this room that cannot all be miked just order it . if we can't get another board and even if we can i have a feeling they 'll be some work . you have these mikes with a little antenna on the end but that 's a great idea and then just have that as the and then you can have groups of twenty people or whatever we could they could have a meeting more or less without us that to do this and we should record it ","It was decided that close-talking data should be downsampled and fed to the SRI recognizer to compare recognition performance, and that data from the far-field microphones should be tested on the recognizer as soon as possible. "
Bmr012.C,"with andreas ' help andreas put together a no frills recognizer which is gender dependent but like no adaptation , no cross word models , no trigrams a bigram recognizer and that 's trained on switchboard which is telephone conversations . don took the first meeting that jane had transcribed and separated used the individual channels we segmented it in into the segments that jane had used anyway it 's twenty minutes and i actually it was a complicated bug because they were sometimes one off and then sometimes random but that will be completely gone if this synch time problem the synch time the synch numbers have more significant digits than they should , there 's things that are l in smaller increments than a frame . anyway these are just the ones that are the prebug for one meeting . this is really encouraging cuz this is free recognition , a lot of the errors are out of vocabulary , the bottom line is even though it 's not a huge amount of data it should be reasonable to actually run recognition and be like within the scope of r reasonable s switchboard this is like h about how we do on switchboard two data with the switchboard one trained mostly trained recognizer and switchboard two is got different population of speakers and a different topic and actually we actually used switchboard telephone bandwidth models and we 'll have more data and we can also start to adapt the language models once we have enough meetings . this is only twenty minutes of one meeting with no tailoring and this is good news because that means the force alignments should be good but if the force alignments are good we can get all kinds of information . about , prosodic information and speaker overlaps and forth directly from the aligned times . actually in order to assess the forced alignment we need s some linguists or some people to look at it and say are these boundaries in about the right place . this would be like if you take the words and force align them on all the individual close talk close talking mikes then how good are these in reality but it 's gotta be pretty good because otherwise the word recognition would be really b crummy . it 's not the case that the end of one utterance is in the next segment and things like that which we had more problems with in switchboard we could try that with this particular twenty minutes of speech and see if there 's any differences . and but jerry also we 're starting on and it 's a meeting on even deeper understanding , edu , and one of the things that i was realizing is it would be really great if anyone has any ideas on some time synchronous way that people in the meeting can make a comment to the person whose gonna transcribe it is there any way we can have like a wireless microphone that you pass around to the people who the extra people for the times they wanna talk that that 's a great idea . ","The Berkeley Meeting Recorder group discussed recognition results generated for 20 minutes of close-talking microphone data. Recognition performance was very good, indicating promising results for forced alignment procedures and the ability to analyze other important signal information, e.g. prosody and overlapping speech. The collection of Meeting Recorder data is ongoing, and will include meetings by the Berkeley Even Deeper Understanding research group and, possibly, an organized discussion by members of the transcriber pool. A tentative decision was also made to integrate the use of a hand-held wireless microphone to help compensate for the lack of available close-talking microphones. "
Bmr012.D,,
Bmr012.E,"have i wanna talk about new microphones and wireless and i 'm liz and andreas wanna talk about recognition results . there there 's an acoustic glitch that occurs where the channels get slightly asynchronized the that problem has gone away in the original driver believe it or not when the ssh key gen ran the driver paused for a fraction of a second i do remember seeing once the transcriber produce an incorrect xml file where one of the synch numbers was incorrect . and i have to try it on the far field mike we need to work at a system for doing that approval that we can send people the transcripts there 's one level that 's already happening right here . and it gets downsampled to sixteen . we could do it with the cross pads . that it 's the maximum we can do without a lot of effort because it 's one board with two digital channels . it takes two fibers in to the one board . and if we wanna do that more than that we 'd have to have two boards , and then you have the synchronization issue . the oth topic is getting more mikes and different mikes , we can fit we have room for one more wireless why don't we get one of these with the crown with a different headset ? i 'd like to hear what they s say . ","The group also discussed recording setup and equipment issues. The Berkeley Meeting Recorder group discussed recognition results generated for 20 minutes of close-talking microphone data. It was decided that close-talking data should be downsampled and fed to the SRI recognizer to compare recognition performance, and that data from the far-field microphones should be tested on the recognizer as soon as possible. "
Bmr012.F,"i was pretty certain that it worked up until that time , there are lots of w there are lots of ways to do the downsampling different filters to put on , ",
Bmr012.G,"one question which is i had the impression from this meeting that w that i transcribed that that there was already automatic downsampling occurring , one of them is to give you a status in terms of the transcriptions far . as of last night 'd assigned twelve hours and they 'd finished nine that by tomorrow we 'll have ten . and then also an idea for another meeting , which would be to have the transcribers talk about the data ","The collection of Meeting Recorder data is ongoing, and will include meetings by the Berkeley Even Deeper Understanding research group and, possibly, an organized discussion by members of the transcriber pool. "
Bmr012.H,,
Bmr012.J,my name is espen eriksen . i 'm a norwegian . with the help of dan ellis i 'm gonna do small project associated to this . what i 'm gonna try to do is use ech echo cancellation to to handle the periods where you have overlapping talk . ,
Bmr016.A,"no , with the transcriber tool , it 's no problem . that 's the training data . thought we sh perhaps we should try to start with those channelized versions give it give one tr transcriber the channelized version of my speech nonspeech detection and look if that 's helpful for them , ",
Bmr016.B,"you said there 's like ten different groupings ? if there was a segment of speech this long the sri front end won't take a an a large audio file name and then a list of segments to chop out from that large audio file ? th probably the way it 'll go is that , when we make this first general version and then start working on the script , that script @ @ that will be ma primarily come from what you 've done , we 'll need to work on a channelized version of those originals . and then probably what will happen is as the transcribers finish tightening more and more , that original version will get updated and then we 'll rerun the script and produce better versions . but the the ef the effect for you guys , because you 're pulling out the little wave forms into separate ones , that would mean these boundaries are constantly changing you 'd have to constantly re rerun that , ",
Bmr016.C,"those are gonna be kept . have those e the vis the ten hours that have been transcribed already , have those been channelized ? no i know that adjusting those things are gonna is gonna make it better . but do you have like a time frame when you can expect like all of it to be done , ",
Bmr016.D,"we 're out of digits . just have to go through them and pick out the ones that have problems , and either correct them or have them re read . we have about two hours worth . what that means is we have about an hour of transcribed digits that we can play with . it 's just a question of a little hand editing of some files and then waiting for more people to turn in their speaker forms . and i sent mail to everyone who hadn't filled out a speaker form , and they 're slowly s trickling in . it 's for labeling the extracted audio files . by speaker id and microphone type . the other topic with digits is liz would like to elicit different prosodics , and we tried last week with them written out in english . and it just didn't work because no one grouped them together . just let them read it how they read it . can just add to the instructions to read it as digits and which group appears is picked randomly , and what the numbers are picked randomly . unlike the previous one , which i d simply replicated ti digits , this is generated randomly . if we wanted to do that we would do it as a separate session , we have asr results from liz , transcript status from jane , and disk space and storage formats from don . and someone said "" in the front in the middle . don , you had disk space and storage formats . and that 's the whole point about the naming conventions on the glosses for numbers , it seems like there are lots of different ways it 's being done . "" nums "" , you have that data don't you ? ","Approximately two hours of digits have been recorded, half of which have been extracted. The Berkeley Meeting Recorder group discussed digits data, recent ASR results, the status of transcriptions, and disk space and storage format issues. "
Bmr016.E,"the spaces already bias it toward being separated . alright first of all , there was a an interest in the transcribe transcription , checking procedures and the first thing i did was spell check . and then , in addition to that , i did an exhaustive listing of the forms in the data file , which included n detecting things like f faulty punctuation and things and th and then another check involves , being that every utterance has an identifiable speaker . then there 's this issue of glossing s w called "" spoken forms "" . mo for the most part , we 're keeping it standard wo word level transcription . however , things like "" cuz "" where you 're lacking an entire very prominent first syllable , those are r reasons f for those reasons i kept that separate , and used the convention of using "" cuz "" for that form , however , glossing it that it 's possible with the script to plug in the full orthographic form for that one , and the way i tag ac pronounced acronyms is that i have underscores between the components . the things in curly brackets are viewed as comments . there 're comments of four types . one of them is , the gloss type we just mentioned . then you have if it 's there 're a couple different types of elements that can happen that aren't really properly words , and then the third type right now , is m things that fall in the category of comments about what 's happening . i have frequencies . you 'll see how often these different things occur . but , the very front page deals with this , final c pa aspect of the standardization which has to do with the spoken forms like "" and "" ha "" and "" and all these different types . right now i 've standardized across all the existing data with these spoken forms . qual qualifier . comment or contextual comment . which means this is part of the numbers task . these are all these , the "" nums point "" , this all where they 're saying "" point "" something or other . and the other thing too is for readability of the transcript . and this is just really a way of someone who would handle th the data in a more discourse y way to be able to follow what 's being said . where we 're gonna have a master file of the channelized data . there will be scripts that are written to convert it into these t these main two uses and th some scripts will take it down th e into a f a for ta take it to a format that 's usable for the recognizer an other scripts will take it to a form that 's usable for the for linguistics an and discourse analysis . and , the implication that i have is that th the master copy will stay unchanged . that , thilo requested , that we ge get some segments done by hand to e s reduce the size of the time bins and he requested that there be , similar , samples done for five minute stretches c involving a variety of speakers and overlapping secti sections . if we could do the more fine grained tuning of this , using an algorithm , that would be much more efficient . yes , they have . the problem is that some of the adjustments that they 're making are to bring are to combine bins that were time bins which were previously separate . and the reason they do that is sometimes there 's a word that 's cut off . and i it 's true that it 's likely to be adjusted in the way that the words are more complete . it takes a couple hours t to do , ten minutes . ","Transcription checking procedures were reviewed, and efforts to coordinate the channelization and presegmention of data with the tightening of time bins were discussed. "
Bmr016.F,"but you think we 'll be able to retrieve the other hour , reasonably ? the relevance of the speaker form here , s i was just gonna say , we have in the vicinity of forty hours of recordings now . and you 're saying two hours , is digits , something like twenty to one . like you say , couple hours for a for a test set 's but the truth is i 'm hoping that we through the that you guys have been doing as you continue that , we get , the best we can do on the spontaneous nearfield , and then we do a lot of the testing of the algorithms on the digits for the farfield , and at some point when we feel it 's mature and we understand what 's going on with it then we have to move on to the spontaneous data with the farfield . but having a little bit might at least be fun for somebody like dan to play around with , it just to r to remove cross talk . l over all our data , we want to not downsample . as as long as there is a form that we can come from again , that is not downsampled , then , what 's qual ? ","Approximately two hours of digits have been recorded, half of which have been extracted. "
Bmr016.G,"but it seems to me that , at least for us , we can learn to read them as digits don't think that 'd be that hard to read them as single digits . because the digits are easier to recognize . they 're better trained than the numbers . thought we 're if we 're collec collecting digits , and adam had said we were running out of the ti forms , it 'd be to have them in groups , and probably , all else being equal , it 'd be better for me to just have single digits and also w we can just let them choose "" zero "" versus "" o "" as they like right , just groupings in terms of number of groups in a line , and number of digits in a group , and the pattern of groupings . roughly looked at what kinds of digit strings are out there , and they 're usually grouped into either two , three , or four , four digits at a time . i purposely didn't want them to look like they were in any pattern . and , it 's to have the digits replicated many times . especially for speakers that don't talk a lot . we have a problem with acoustic adaptation , but we don't have any overlapping digits . but the people who wanna work on it we should talk to them . it 's pretty preliminary in terms of asr results like it 's not the speaking style that differs , it 's the fact that there 's overlap that causes recognition errors . and then , the fact that it 's almost all insertion errors , but you might also think that in the overlapped regions you would get substitutions and forth , leads us to believe that doing a better segmentation , like your channel based segmentation , or some echo cancellation to get back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the close talking mikes . the other neat thing is it shows for that the lapel , within speaker is bad . and it 's bad because it picks up the overlapping speech . and the rec there 're error rates because of insertion insertions aren't bounded , with a one word utterance and ten insertions you got huge error rate . this is what we expected , and also wanted to mention briefly that , andreas and i called up dan ellis who 's still stuck in switzerland , and we were gonna ask him if there 're what 's out there in terms of echo cancellation and things like that . because right now we 're not able to actually report on recognition in a real paper , because it would look premature . the idea is to run this on the whole meeting . and get the locations , which gives you also the time boundaries of the individual speak right . except that there are many techniques for the kinds of cues , that you can use to do that . that 's really the next step because we can't do too much , on term in terms of recognition results knowing that this is a big problem until we can do that processing . at this point we can finalize the naming , and forth , and we 're gonna re rewrite out these waveforms that we did the r the front end on the sri recognizer just downsamples them on the fly , we can't shorten them , but we can downsample them . that 's why we need more disk space it 's better if they 're chopped out , cuz you can run them , in different orders . you can't load an hour of speech into x waves . you need to s have these small files , it 's actually probably good for us to know the difference between the real "" and the one that 's just like "" or transcribed "" aaa "" we have this there is this thing i was gonna talk to you about at some point about , what do we do with the dictionary as we 're up updating the dictionary , but if there 's things that we change later , then we always have to keep our the dictionary up to date . now why do we what 's the reason for having like the point five have the "" nums "" on it ? if you b merge two things , then that it 's the sum of the transcripts , but if you split inside something , you don't where the word which words moved . the cutting of the waveforms is pretty trivial . ",Researchers doing ASR are looking into methods for generating a better channel-based segmentation to improve recognition results for close-talking microphone data. 
Bmr021.A,,
Bmr021.B,,
Bmr021.C,,
Bmr021.D,,
Bmr021.E,,
Bmr021.F,,
Bmr022.A,,
Bmr022.B,,
Bmr022.C,,
Bmr022.D,,
Bmr022.E,,
Bmr022.F,,
Bmr023.A,"we the transcribers have continued to work past what i 'm calling "" set one "" , but , they 've gotten five meetings done in that set . i hired two transcribers today . i 'm thinking of hiring another one , which will because we 've had a lot of attrition . the pre segmentations are much are s extremely helpful . but actually i it 's correct for much of the time , that it 's an enormous time saver and it just gets tweaked a little around the boundaries . wha what you 'd really like is that they started with pre segmented and were pre segmented all the way through . unfortunately , in the sign thing that they signed , it says "" transcripts "" . "" you 'll be provided the transcripts when they 're available . "" ","Specifically, the group would like to have transcripts available, which would mean resolving legal issues for data use and on the basis of feedback from IBM get more transcription underway. "
Bmr023.B,"like , there 's a lot of different features you could just pull out . ",
Bmr023.C,"it looks like the vocal tract length normalization is working beautifully , actually , because in all our previous experiments , we had the we were essentially cheating by having the , the h the hand segmentations as the basis of the recognition . and now with thilo 's segmenter working we should consider doing a we should consider doing some extra things retraining or adapting the models for background noise to the to this environment , and they don't have to approve , th an edited version , they can just give their approval to whatever version but th the editing will continue . presumably if s errors are found , they will be fixed , that wouldn't be cheating because you can detect pause pretty within the time . chuck and i talked and the @ @ next thing to do is probably to tune the the size of the gaussian system , @ @ to this feature vector , which we haven't done we just used the same configuration as we used for the standard system . and , dan @ @ dan just sent me a message saying that cmu used , something like ten gaussians per cluster each mixture has ten gaussians that 's big difference and give very poorly trained , gaussians that way , the turn around time on the training when we train only the a male system with , our small training set , is less than twenty four hours , but the plp features work continue to improve the , as i said before , the using dan 's , vocal tract normalization option works very @ @ i ran one experiment where we 're just did the vocal tract le normalization only in the test data , didn't bother to retrain the models which is about what we get with with , just @ @ actually doing both training and test normalization , with , the , with the standard system . in a few hours we 'll have the numbers for the for retraining everything with vocal tract length normalization it looks like the p l fea p features do very now with after having figured out all these little tricks to get it to work . right . and what that suggests also is that the current switchboard mlp isn't trained on very good features . but if you add them all up you have , almost five percent difference now . at this point i 'm as i 'm wondering is it can we expect , a tandem system to do better than a properly trained a gaussian system trained directly on the features with , the right ch choice of parameters ? but there the main point is that , it took us a while but we have the procedure for coupling the two systems debugged now and there 's still conceivably some bug somewhere in the way we 're feeding the tandem features either generating them or feeding them to this to the sri system , that that 's this that 's essentially the same as we use with the ce with the p l p fe features . ","PLP results for the front-end look good, with the group also reporting progress in segmentation: Thilo's segmenter will now be used and ways of improving performance investigated; Work on the front end continues, with improvements of 3-5% being made. "
Bmr023.D,"now the you saw the note that the plp now is getting the same as the mfcc . and he talked it over with the transcriber and the transcriber thought that the easiest thing for them would be if there was a beep and then the nu a number , a digit , and then a beep , at the beginning of each one and , adam wrote a little script to generate those style , beeps we 're gonna send them one more sample meeting , and thilo has run his segmentation . adam 's gonna generate the chunked file . and then , i 'll give it to brian and they can try that out . and when we get that back we 'll see if that fixes the problem we had with , too many beeps in the last transcription . the last one seemed like it took a couple of weeks . even three . that 's just the i b m side . jane and adam and i had a meeting where we talked about the reorganization of the directory structure for all of the meeting for all the meeting recorder data . and then , jane also s prepared a started getting all of the meetings organized , she prepared a spreadsheet , which i spent the last couple of days adding to . been putting it into , a spreadsheet with start time , the date , the old meeting name , the new meeting name , the number of speakers , the duration of the meeting , comments , what its transcription status is , all that and the idea is that we can take this and then export it as html and put it on the meeting recorder web page we can keep people updated about what 's going on . but far , as of monday , the fourteenth , we 've had a total number of meeting sixty two hours of meetings that we have collected . some other interesting things , average number of speakers per meeting is six . and i 'm gonna have on here the total amount that 's been transcribed far , and it 'll also list , like under the status , if it 's at ibm or if it 's at icsi , or if it 's completed or which ones we 're excluding could it have to do with the lower frequency cut off on the switchboard ? we 're using sixty four , there could be a bug in the somewhere before that . it 's hard with features , cuz you don't they should look like . you can't just print the values out in ascii and , look at them , see if they 're ","PLP results for the front-end look good, with the group also reporting progress in segmentation: Thilo's segmenter will now be used and ways of improving performance investigated; Specifically, the group would like to have transcripts available, which would mean resolving legal issues for data use and on the basis of feedback from IBM get more transcription underway. "
Bmr023.E,it should @ @ be finished today ,
Bmr023.F,"i was gonna ask adam to , say if he thought anymore about the demo because it occurred to me that this is late may and the darpa meeting is in mid july . i know that we were gonna do something with the transcriber interface is one thing , is there that 's happened about , the sri recognizer et cetera , y you guys were doing a bunch of experiments with different front ends and then with do w do what do you have any idea of the turn around on those steps you just said ? e u the reason i 'm asking is because , jane and i have just been talking , and she 's just been doing . e a , further hiring of transcribers . and we don't really know exactly what they 'll be doing , how long they 'll be doing it , and forth , because right now she has no choice but to operate in the mode that we already have working . in particular i would really hope that when we do this darpa meeting in july that we have we 're into production mode , somehow that we actually have a stream going and we know how it does and how it operates . but if we hire f f we have five on staff five or six on staff at any given time , then it 's a small enough number we can be flexible either way . le let me put in another milestone as i did with the , the pipeline . we are gonna have this darpa meeting in the middle of july , given that we 've been we 've given a couple public talks about it already , spaced by months and months , it 'd be pretty bad if we continued to say none of this is available . right . we can s we wanna be able to say "" here is a subset that is available right now "" and that 's has been through the legal issues and forth . in principle , yes . but , i if somebody actually did get into some legal issue with it then we i it there is a point at which i agree it becomes ridiculous let me just suggest that off line that , the people involved figure it out and take care of it before it 's july . has , we just just talked about this the other day , but h has anybody had a chance to try changing , insertion penalty things with the , using the tandem system input for the ? and agree with you that if we fixed lots of different things and they would all add up , we would probably have a competitive system . but not that much of it is due to the front end per se . couple percent of it is , as far as see from this . i would actually double check with stephane at this point , not unless you had a lot of time the other thing , just to mention that stephane this was an innovation of stephane 's , which was a pretty neat one , and might particularly apply here , given all these things we 're mentioning . stephane 's idea was that , discriminant , approaches are great . even the local ones , given , these potential outer loops which , you can convince yourself turn into the global ones . however , there 's times when it is not good . when something about the test set is different enough from the training set that , the discrimination that you 're learning is not a good one . his idea was to take as the input feature vector to the , gaussian mixture system , a concatenation of the neural net outputs and the regular features . it 's it 's not it 's not gonna work out but we could just , see if we find a rhythm , ","Additionally they would also like to have the question answering mock-up and transcriber interface ready for then. A pressing concern for the group is the DARPA meeting in July, which is only a short time away, and for which they would like to have some progress. PLP results for the front-end look good, with the group also reporting progress in segmentation: Thilo's segmenter will now be used and ways of improving performance investigated; Specifically, the group would like to have transcripts available, which would mean resolving legal issues for data use and on the basis of feedback from IBM get more transcription underway. Work on the front end continues, with improvements of 3-5% being made. "
Bmr023.G,"we were gonna do a mock up question answering and even the good thing is that since you , have high recall , even if you have low precision cuz you 're over generating , that 's good but i know that if we run recognition unconstrained on a whole waveform , we do very poorly because we 're getting insertions in places what that you may be cutting out . we do need some pre segmentation . and , using thilo 's , posteriors or some or as long as we have a record , of the original automatic one , we can always find out how we would do fr from the recognition side by using those boundaries . because we couldn't use the non native all non native meetings and it 's , probably below threshold on enough data for us for the things we 're looking at because the prosodic features are very noisy and you need a lot of data in order to model them . we 're starting to see some patterns but we did find that some of the features that , i gue jane would know about , that are expressing the distance of , boundaries from peaks in the utterance and some local , range pitch range effects , like how close people are to their floor , are showing up in these classifiers , which are also being given some word features that are cheating , cuz they 're true words . these are based on forced alignment . spurts is not cheating except that the real words , you reported on some te punctuation type finding sentence boundaries , finding disfluency boundaries , and then i had done some work on finding from the foreground speech whether or not someone was likely to interrupt , where if i 'm talking now and someone and andreas is about to interrupt me , is he gonna choose a certain place in my speech , either prosodically or word based . and there the prosodic features actually showed up even though the word features were available . and a neat thing there too is i tried some putting the speaker i gave everybody a short version of their name . that means that overall , it wasn't just modeling morgan , or it wasn't just modeling a single person , the other thing that was interesting to me is that the pitch features are better than in switchboard . and that really is from the close talking mikes , cuz the pitch processing that was done has much cleaner behavior than the switchboard telephone bandwidth . first of all , the pitch tracks are m have less , halvings and doublings than switchboard and there 's a lot less dropout , if you ask how many regions where you would normally expect some vowels to be occurring are completely devoid of pitch information , in other words the pitch tracker just didn't get a high enough probability of voicing for words ma the tele we had telephone bandwidth for switchboard and we had the an annoying telephone handset movement problem that may also affect it . we 're just getting better signals in this data . to try to get a non cheating version of how all this would work . are we trying to do them in synchrony ? that might be fun . ","Additionally they would also like to have the question answering mock-up and transcriber interface ready for then. PLP results for the front-end look good, with the group also reporting progress in segmentation: Thilo's segmenter will now be used and ways of improving performance investigated; The classifier segmentation is progressing well, especially in the use of prosody for identifying interruption. "
Bmr024.A,"and crucial part of that is the idea of not wanting to do it until right before the next level zero back up that there won't be huge number of added , i hire i 've hired two extra people already , expect to hire two more . which are now being edited by my head transcriber , in terms of spelling errors and all that . she 's also checking through and mar and monitoring , the transcription of another transcriber . and , i 've moved on now to what i 'm calling set three . if i do it in sets groups of five , then have parallel processing through the current . and you indicated to me that we have a g a goal now , for the for the , the , darpa demo , of twenty hours . i 'm gonna go up to twenty hours , be that everything gets processed , and released , and that 's what my goal is . i realize that , w i we 're using the pre segmented version , and , the pre segmented version is extremely useful , and wouldn't it be , useful also to have the visual representation of those segments ? and 've i , i 've trained the new one the new the newest one , to , use the visual from the channel that is gonna be transcribed at any given time . because what happens then , is you scan across the signal and once in a while you 'll find a blip that didn't show up in the pre segmentation . and , that we 're gonna end up with , better coverage of the backchannels , but at the same time we 're benefitting tremendously from the pre segmentation because we have meetings that have a reason . and those and this sounds like it 's more of an experimental setup . but you can have it nw archive to you can have , a non backed up disk nw archived , ","In particular, the group discuss their preparation of materials for the transcriptions of digits by IBM, and also the human transcribers who are working towards preparing the set of 20 for the DARPA meeting. A number of issues regarding the management of data are addressed by the group: The inclusion of different data types in the corpus, and the storage and back-up of the group's data. "
Bmr024.B,"and the main thing will be if we can align what they give us with what we sent them . n i 'm successfully , increasing the error rate . 'm just playing with , the number of gaussians that we use in the recognizer , and ","Other discussion focuses on the re-evaluation of recognition without cheating on segmentation, and also how SRI recognition can be improved, especially for the female group. "
Bmr024.C,,
Bmr024.D,"but , probably , if we had to pick something that we would talk on for ten minutes or while they 're coming here . or it would be , you think , reorganization status , since that was a pretty short one , we should talk about the ibm transcription status . but the other thing is that , that 's kinda twenty hours asap because the longer before the demo we actually have the twenty hours , the more time it 'll be for people to actually do things with it . the the difference if , if the ibm works out , the difference in the job would be that they p primarily would be checking through things that were already done by someone else ? forced alignment would be one thing . what about just actually doing recognition ? i was just asking , just out of curiosity , if with , the sri recognizer getting one percent word error , would we do better ? if you do a forced alignment but the force but the transcription you have is wrong because they actually made mistakes , or false starts , it 's much less c it 's much less common than one percent ? i 'm not saying it should be one way or the other , but it 's if i would not say it was part of the meetings corpus . it 's th think the idea of two or more people conversing with one another is key . it 's scenario based , it 's human computer interface it 's really pretty different . it 's just that it 's , different directory , it 's called something different , it 's i don i wouldn't call reading digits "" meetings "" . i don't care what directory tree you have it under . but you see , now , between the males and the females , there 's certainly a much bigger difference in the scaling range , than there is , say , just within the males . or you 're doing one too many . he was he 's it looked like the probabil at one point he was looking at the probabilities he was getting out at the likelihoods he was getting out of plp versus mel cepstrum , and they looked pretty different , but , you 're only talking about a percent or two . we 've always viewed it , anyway , as the major difference between the two , is actually in the smoothing , that the that the , plp , and the reason plp has been advantageous in , slightly noisy situations is because , plp does the smoothing at the end by an auto regressive model , is there something quick about absinthe that you ? guess the other thing that we were gonna talk about is , demo . and , these are the demos for the july , meeting and , darpa mee but we 'll just put that off for now , given that but we should have a sub meeting , probably , adam and , chuck and me should talk about should get together and talk about that sometime soon . ","Progress has been made in naming conventions, with file reorganisation to be done at a later date, however this was not discussed fully due to Chuck's absence. In particular, the group discuss their preparation of materials for the transcriptions of digits by IBM, and also the human transcribers who are working towards preparing the set of 20 for the DARPA meeting. The inclusion of different data types in the corpus, and the storage and back-up of the group's data. A number of issues regarding the management of data are addressed by the group: Other discussion focuses on the re-evaluation of recognition without cheating on segmentation, and also how SRI recognition can be improved, especially for the female group. Finally, Absinthe is now up and running with improved performance. Discussion of demos for the July DARPA meeting were left to the individuals concerned. "
Bmr024.E,and we have done that on the automatic segmentations . the references for those segments ? ,
Bmr024.F,"chuck was the one who added out the agenda item . i don't really have anything to say other than that we still haven't done it . naming conventions and things like that , that i 've been trying to keep actually up to date . and i 've been sharing them with u d uw folks also . we , we did another version of the beeps , where we separated each beeps with a spoken digit . chuck came up here and recorded some di himself speaking some digits , they 'll have a b easier time keeping track of where they are in the file . we just sent it to ibm . i was just wondering what people thought about how automated can we make the process of finding where the people read the digits , doing a forced alignment , and doing the timing . guess if we segmented it , we could get one percent on digits . hire some people , or use the transcribers to do it . we could let ibm transcribe it . or we could try some automated methods . and my tendency right now is , if ibm comes back with this meeting and the transcript is good , just let them do it . and the , porzel and the , smartkom group are collecting some dialogues . they have one person sitting in here , looking at a picture , and a wizard sitting in another room somewhere . and , they 're doing a travel task . but it starts where the wizard is pretending to be a computer and it goes through a , speech generation system . should this be part of the corpus or not ? and my attitude was yes , because there might be people who are using this corpus for acoustics , as opposed to just for language . but they don't know that it 's the same person both times . and i said , "" that 's silly , if we 're gonna try to do it for a corpus , there might be people who are interested in acoustics . "" it 's it it the begs the question of what is the meeting corpus . this has two or more people conversing with each other . we give everyone who 's involved as their own user id , give it session i ds , let all the tools that handle meeting recorder handle it , or do we wanna special case it ? and just simply in the file you mark somewhere that this is this type of interaction , rather than another type of interaction . but , i put it under the same directory tree . we 're about half halfway through our disk right now . once everything gets converted over to the disks we 're supposed to be using we 'll be probably , seventy five percent . i 'm much more concerned about the backed up . when i say two or three years what i 'm saying is that i have had disks which are gone in a year . but , you don't want to per p have your only copy on a media that fails . icsi already has a perfectly good tape system and it 's more reliable . for archiving , we 'll just use tape . but even without that , the back up system is becoming saturated . you can try each one on a cross validation set , and got a speedup roughly proportional to the number of processors times the clock cycle . but the what it means is that it 's likely that for net training and forward passes , we 'll absinthe will be a good machine . especially if we get a few more processors and upgrade the processors . ","Progress has been made in naming conventions, with file reorganisation to be done at a later date, however this was not discussed fully due to Chuck's absence. In particular, the group discuss their preparation of materials for the transcriptions of digits by IBM, and also the human transcribers who are working towards preparing the set of 20 for the DARPA meeting. A number of issues regarding the management of data are addressed by the group: The inclusion of different data types in the corpus, and the storage and back-up of the group's data. Finally, Absinthe is now up and running with improved performance. "
Bmr024.G,,
Bmr024.H,,
Bmr024.I,"and and , one of the obvious things that occur to us was that we 're since we now have thilo 's segmenter and it works , amazingly we should actually re evaluate the recognition , results using without cheating on the segmentations . no , actually , nist has , a fairly sophisticated scoring program that you can give a , time , you just give two time marked sequences of words , and it computes the the , the th it we just and we use that actually in hub five to do the scoring . what we 've been using far was simplified version of the scoring . it does time constrained word alignment . that thilo wanted to use the recognizer alignments to train up his , speech detector . that we could use , there wouldn't be much hand labelling needed to , to generate training data for the speech detector . we simulate a computer breakdown halfway through the session , and then after that , the person 's told that they 're now talking to a , to a human . it makes sense to handle it with the same infrastructure , since we don't want to duplicate things unnecessarily . but as far as distributing it , we shouldn't label it as part of this meeting corpus . what about putting the on cd rom or dvd how about putting them on that plus , like on a on dat or some other medium that isn't risky ? but this back up system is smart enough to figure out that something hasn't changed and doesn't need to be backed up again . you have to sa you have to tell people that you 're doing you 're trying the tandem features . a and i 'm still tinkering with the plp features . that was before i tried it on the females . we had reached the point where , on the male portion of the development set , the , or one of the development sets , i should say the , the male error rate with , icsi plp features was identical with , sri features . and the test data is callhome and switchboard . and plus the vocal tract length normalization didn't actually made things worse . something 's really wrong . d the one thing that i then tried was to put in the low pass filter , which we have in the most hub five systems actually band limit the at about , thirty seven hundred , hertz . although , normally , the channel goes to four thousand . and it didn't hurt on the males either . and suddenly , also the v the vocal tract length normalization only in the test se on the test data . between one and two percent , for the females . we 're looking at the discrepancy between the sri system and the sri system when trained with icsi features . no , but with baum welch , there shouldn't be an over fitting issue , really . for the plp features we use the triangular filter shapes . and for the in the sri front end we use the trapezoidal one . ",
Bmr025.A,"hired several more transcribers , they 're making great progress . 've been finishing up the double checking . i know that that thilo you were , bringing the channeltrans interface onto the windows machine ? and i have my feeling on it is that in principle it 's a really idea , and you have the time tags which makes it better tha than just taking ra raw notes . on the other hand , i the down side for me was that the pen is really noisy . ","Transcription is progressing well, with new people hired, and double checking almost complete. Finally, other progress made includes getting the ChannelTrans interface working, ordering more wireless microphones, and analysing recognition runs. "
Bmr025.B,"darpa demos , at the very least we 're gonna want something illustrative with that and if there 's something that shows it graphically it 's much better than me just having a bullet point s when we here were having this demo meeting , what we 're coming up with is that we wanna have all these pieces together , to first order , by the end of the month that 'll give us that 'll give us a week or to to port things over to my laptop and make that works , if it 's stationary it 's not going to go through the increment it 's not gonna burden things in the incremental backups . guess the idea is that we would be reserving the non backed up space for things that took less than twenty four hours to recreate like that , good . crosspads ? that you can you have a record of whatever it is you 've written . and one of the reasons that it was brought up originally was because we were interested in higher level things , not just the , microphone but also summarization and forth we get somebody to buy into the idea of doing this as part of the task . part of the reason part of the reason that adam was interested in the speechcorder idea from the beginning is he said from the beginning he hated taking notes but by i would suggest you return one . because we we haven't used it there 's all this going on between andreas and dave and chuck and others with various kinds of runs recognition runs , trying to figure things out about the features but it 's all in process , ","The most pressing issue concerns the demos which the group are preparing for the DARPA meeting next month. Additionally, the group have progressed further with data storage issues, with backing-up their data now regarded as a priority, and more disk space required. The collection of CrossPad note-taking data will be pursued in future meetings. Finally, other progress made includes getting the ChannelTrans interface working, ordering more wireless microphones, and analysing recognition runs. "
Bmr025.C,we can probably find some examples of different type of prosodic events going on . ,
Bmr025.D,"i 've just processed the first five edu meetings and they are chunked up they would they probably can be sent to ibm whenever they want them . it 's already at ibm , it 's it it 's done , ","Finally, other progress made includes getting the ChannelTrans interface working, ordering more wireless microphones, and analysing recognition runs. "
Bmr025.E,if you wanted to do that the right architecture for it is to get a pda with a wireless card . and that way you can synchronize very easily with the meeting ,The collection of CrossPad note-taking data will be pursued in future meetings. 
Bmr025.F,"the second one of those that 's the one that we 're waiting to hear from them on . and as soon as we hear from brian that this one is and we get the transcript back and we find out that hopefully there are no problems matching up the transcript with what we gave them , then we 'll be ready to go and we 'll just send them the next four as a big batch , we 're doing things in parallel , i never knew we were supposed to put it in the key file . ",Work is also going on in parallel with IBM. 
Bmr025.G,"another idea i w t had just now actually for the demo was whether it might be of interest to sh to show some of the prosody work that don 's been doing . and then show task like finding sentence boundaries or finding turn boundaries . you can show that graphically , what the features are doing . it , it doesn't work great but it 's definitely giving us something . i don't know if that would be of interest or not . next month . quick question on that . is do we have the seat information ? but hadn't ever been putting it in the key files . what if you 're sitting there and you just wanna make an x and you don't wanna take notes one would probably be fine . we could do like a student project , someone who wants to do this as their main like s project for something would be we might wanna do it simultaneous . ","Here they discuss the querying and indexing tool which is progressing well albeit with a few front-end issues, and also the transcriber tool. The collection of CrossPad note-taking data will be pursued in future meetings. "
Bmr025.H,"as a somewhat segue into the next topic , could i get a hold of the data even if it 's not really corrected yet just can get the data formats and make the information retrieval is working ? especially for the information retrieval 've been working on using the thisl tools to do information retrieval on meeting data and the thisl tools are there 're two sets , there 's a back end and a front end , the front end is the user interface and the back end is the indexing tool and the querying tool . and 've written some tools to convert everything into the right for file formats . and the command line version of the indexing and the querying is now working . at least on the one meeting that i had the transcript for conveniently you can now do information retrieval on it , do type in a string and get back a list of start end times for the meeting , but my intention is to do a prettier user interface based either but it does mean you need to be running a web server . and it 's pretty big and complex . and it would be difficult to port to windows the other option is dan did the tcl tk thisl gui front end for broadcast news 've been doing a bunch of xml tools because there are a lot of tools that let you do extraction and reformatting of xml tools . yet again we should probably meet to talk about transcription formats in xml i 'm converting the key files to xml that you can extract m various inf sorted information on individual meetings the seat information is on the key files for the ones which this is why i wanna use a g a tool to do it rather than the plain text because with the plain text it 's very easy to skip those things . and then the other thing also that thilo noticed is , on the microphone , on channel zero it says hand held mike or crown mike , you actually have to say which one . and then also in a couple of places instead of filling the participants under "" participants "" they were filled in under "" description "" . the one that shows up here , that will flash yellow if the mike isn't connected . spoke with dave johnson about putting all the meeting recorder on non backed up disk to save the overhead of backup but he thought it was a bad idea . what he said is doing the manual one , doing nw archive to copy it is a good idea and we should do that and have it backed up . he w he 's a firm believer in lots of different modalities of backup . this data cannot be recovered . and if then a mistake is made and we lose the archive we should have the backup . just the monthly full . and we 're far enough away from saturation on full backups that it 's w probably and the only issue here is the timing between getting more disks and recording meetings . things that are recreatable easily and also things that are recreatable . who said "" if you 're not using them , could you return them ? "" we used them a couple times , if you take notes it 's a great little device . that it 's synchronized with the time on that and then you have to download to an application , and then you have to figure out what the data formats are and convert it over if you wanna do anything with this information . for what you 've been describing buttons would be even more convenient than anything else , if we had them out and sitting on the table people might use them a little more w we ordered more wireless , and then at the same time i 'll probably rewire the room as per jane 's suggestion that the first n channels are wireless , are the m the close talking and the next n are far field . you hav sorta have to . ","Work is also going on in parallel with IBM. Here they discuss the querying and indexing tool which is progressing well albeit with a few front-end issues, and also the transcriber tool. Tools for accessing key file information have been developed which should ensure all meeting information is present. Additionally, the group have progressed further with data storage issues, with backing-up their data now regarded as a priority, and more disk space required. The collection of CrossPad note-taking data will be pursued in future meetings. Finally, other progress made includes getting the ChannelTrans interface working, ordering more wireless microphones, and analysing recognition runs. "
Bmr026.A,"this is gonna be a pretty short meeting because i have four agenda items , three of them were requested by jane who is not gonna be at the meeting today . they ' r they 're doing the full transcription process . jane also wanted to talk about participant approval , but i don't really think there 's much to talk about . i 'm gonna send out to the participants , with links to web pages which contain the transcripts and allow them to suggest edits . nope , they 'll have access to the audio also . because the transcripts might not be right . it 's probably going to have to be the uncompressed versions because , it takes too long to do random access decompression . darpa demo status , not much to say . the back end is working out fine . i 've added some that indes indexes by the meeting type mr , edu , et cetera and also by the user id . that the front end can then do filtering based on that as the back end is going more slowly as i s said before just cuz i 'm not much of a tcl tk programmer . transcriber is tcl tk , very generic with snack , just record the audio clip and show an image and that 's and the last i item on the agenda is disk issues yet again . we 're only about thirty percent on the second disk . we have a little bit of time before that becomes critical , but we are like ninety five percent , ninety eight percent on the scratch disks for the expanded meetings . and , my original intention was like we would just delete them as we needed more space , but unfortunately we 're in the position where we have to deal with all the meeting data all at once , in a lot of different ways . you we need about a gig per meeting . the sysadmins would prefer to have one external drive per machine . they don't want to stack up external drives . and think what he 's been concentrating on is the back up system , rather than on new disk . just a week from tomorrow ? they were comparable . ","This is a relatively short meeting of the Meeting Recorder group, with only a few agenda items. Transcription was discussed briefly because Jane was not present, however this appears to be progressing well in parallel with IBM. Web pages have been set up to show transcription status and to allow participants to approve transcripts. DARPA demos are progressing well with the back-end indexed to allow front-end filtering, and a potential demo ideas investigated which would use X Waves. Transcriber is now working for Windows, however live pitch contours may not work in the time available. Backed-up disk space is now fine, however temporary space is running out fast. The group note that the annual report needs to be worked on for next week, and it is also suggested to hold recognition meetings separately, however these issues will be discussed in more detail at the next meeting. "
Bmr026.B,"and , the more live , the better , but given the crunch of time , we may have to retreat from it to some extent . think for a lot of reasons , it would be very to have this transcriber interface be able to show some other interesting signal along with it it should get another rack . dave johnson is gone for ten days , it 's just a question of figuring out where they should be and hanging them , this is a question that 's pretty hard to solve without talking to dave , but at any rate that there 's a longer term thing and there 's immediate need something that we wanna do next meeting is to put together a reasonable list for ourselves of what is it , that we 've done . just bulletize e do dream up text but this is gonna lead to the annual report . how many people here would not be interested in in a meeting about recognition ? and then if we find , we 're just not getting enough done , there 's all these topics not coming up , then we can expand into another meeting . let 's chat about it with liz and jane when we get a chance , see what they think and mean that 's actually not that different from the amount of training that there was . because it was apparent if you put in a bunch more data it would be better , although we it 's not that many fewer and we take a klt anyway we could and the b and the base u starting off with the base of the alignments that you got from i from a pretty decent system . it 's it 'd be context independent and on . how much training data ? ","Transcriber is now working for Windows, however live pitch contours may not work in the time available. Interim measures are discussed while sysadmin are away. The group note that the annual report needs to be worked on for next week, and it is also suggested to hold recognition meetings separately, however these issues will be discussed in more detail at the next meeting. "
Bmr026.C,"but it 's just transcripts , not the audio ? i 've been putting together transcriber things for windows but the problem is the version transcriber works with , the snack version , is one point six whatever ","Web pages have been set up to show transcription status and to allow participants to approve transcripts. Transcriber is now working for Windows, however live pitch contours may not work in the time available. "
Bmr026.D,"because i need to ask jane whether it 's it would be for her s some of her people to transcribe some of the initial data we got from the smartkom data collection , which is these short like five or seven minute sessions . but to get going we would like some of the data transcribed right away we can get started . and wanted to ask jane if one of their transcribers could do since these are very short , that should really be but it would be if the transcriber interface had like another window for the above the waveform where it would show some arbitrary valued function that is time synchron ti time synchronous with the wavform . one thing we in past meetings we had also a various variously talked about the work that w was happening on the recognition side it 's that it 's just gonna be ver very boring for people who are not really interested in the details of the recognition system . liz and jane probably . yot i tested the the final version of the plp configuration on development test data for this year 's hub five test set . and the recognition performance was exactly , and exactly up to the the first decimal , same as with the mel cepstra front end . they were the males were slightly better and the females were slightly worse but nothing really . and then the really thing was that if we combine the two systems we get a one and a half percent improvement . which u actually uses the whole n best list from both systems to combine that . there 's one thing mean you don't want to overdo it because y every front end if you multiply your effort by n , where n is a number of different systems and then we had some results on digits , with and the reason is there 's a whole bunch of read speech data in the hub five training set . the rest is read timit data and atis data and wall street journal and like that . a fifth would be two hours something . but it definitely helps to have the other read data in there the error rate is half of what you do if you train only on ti timit not timit ti digits , more read speech data definitely helps . and you can leave out all the conversational data with no performance penalty . that 's plenty of read speech data . wall street journal , take one example . that might be useful for the people who train the digit recognizers to use something other than ti digits . and then i th guess chuck and i had some discussions about how to proceed with the tandem system the mlp would be trained on only forty six or forty eight which is smaller than the than the phone set that we 've been using far . we want to try things like deltas on the tandem features . and you h have to multiply everything by two or three . and fewer dimensions in the phone set would be actually helpful just from a logistics point of view . and then we wanted to s just limit it to something on the same order of dimensions as we use in a standard front end . that would mean just doing the top i don't know ten or twelve of the klt dimensions . once we have the new m l p trained up , one thing i wanted to try just for the fun of it was to actually run like a standard hybrid system that is based on those features and retrain mlp and also the the dictionary that we use for the hub five system . it 's sanity check of the m l p outputs before we go ahead and train up the use them as a basis for the tandem system . you started training with outputs from a with alignments that were generated by the cambridge system ? that was probably because your initial system your system was ba worse than cambridge 's . ","Transcriber is now working for Windows, however live pitch contours may not work in the time available. The group note that the annual report needs to be worked on for next week, and it is also suggested to hold recognition meetings separately, however these issues will be discussed in more detail at the next meeting. Improvement has been made in the final version of the PLP, which shows better female performance, and combined with Mel Ceptra offers 1.5% improvement. Digit performance also improved thanks to training using scripted speech data. Progress has also been made in SRI alignment for tandem system. "
Bmr026.E,"we 're doing some in parallel . and , also i was just showing andreas , i got an x waves display , and i don't know how much more we can do with it with like the prosodic where we have like stylized pitches and signals and the transcripts on the bottom and see the pitch contours also . i have have an eighteen gig drive hanging off of my computer . ","Transcription was discussed briefly because Jane was not present, however this appears to be progressing well in parallel with IBM. DARPA demos are progressing well with the back-end indexed to allow front-end filtering, and a potential demo ideas investigated which would use X Waves. Transcriber is now working for Windows, however live pitch contours may not work in the time available. "
Bmr026.F,"first of all with ibm i got a note from brian yesterday saying that they finally made the tape for the thing that we sent them a week or week and a half ago and hopefully next week we 'll have the transcription back from that . guess she 's hired some new transcribers 've been trying to keep a web page up to date f showing what the current status is of the trans of all the things we 've collected that 's the thing that i sent out just to foo people saying can you update these pages i 'll send it out to the list telling people to look at it . the audio that they 're gonna have access to , will that be the uncompressed version ? or will you have scripts that like uncompress the various pieces and i was just wondering because we 're running out of the un backed up disk space on sent that message out to , you and dave asking for if we could get some disk . part of the reason why dave can't get the new machines up is because he doesn't have room in the machine room right now . why don't we alternate this meeting every other week ? but i do i don't lot of times lately it seems like we don't really have enough for a full meeting on meeting recorder . andreas brought over the alignments that the sri system uses . and 'm in the process of converting those alignments into label files that we can use to train new net with . and then i 'll train the net . i was wondering what size net i should anybody have any intuitions or suggestions ? ","Transcription was discussed briefly because Jane was not present, however this appears to be progressing well in parallel with IBM. Web pages have been set up to show transcription status and to allow participants to approve transcripts. The group note that the annual report needs to be worked on for next week, and it is also suggested to hold recognition meetings separately, however these issues will be discussed in more detail at the next meeting. "
Bmr027.A,"meant that was the release date that you had on the data . but it might be good to remind people two weeks prior to that they can change it to . one of them is "" censor "" , and the other one is "" incorrect "" . the german ones will be ready for next week . and then it 'd be good to set an explicit deadline , something like a week before that , j july fifteenth date , or two weeks before . this is in the summer period and presumably people may be out of town . but we can make the assumption , can't we ? that , they will be receiving email , most of the month . al altogether we 've got twenty people . these people are people who read their email almost all the time . i really don't see that it 's a problem . that it 's a common courtesy to ask them to expect for them to , be able to have @ @ us try to contact them , u just in case they hadn't gotten their email . they 'd appreciate it . i don't think , they 're recent , these visitors . i w i 'll be able to if you have any trouble finding them , i really could find them . and it i this discussion has made me think it might be to have a follow up email within the next couple of days saying "" we wanna hear back from you by x date at a certain point , that copy that has the deletions will become the master copy . do you suppose that was because they weren't caught by the pre segmenter ? we 've been , contacted by university of washington now , to , we sent them the transcripts that correspond to those six meetings and they 're downloading the audio files . @ this is from one of the nsa meetings no , no . these are our local transcriptions of the nsa meetings . sometimes some speakers will insert foreign language terms . and i 'm hoping that when the checked versions are run through the recognizer that you 'll see s substantial improvements in performance no , actually no . this is cuz , you don't realize in daily life how much you have top down influences in what you 're hearing . and it 's jar it 's jargon coupled with a foreign accent . and then i also the final thing i have for transcription is that i made a purchase of some other headphones because of the problem of low gain in the originals . and they very much appro they mu much prefer the new ones , ","The group suggest sending reminder e-mails, although since many participants are local they can be contacted by other means if necessary. Checking of the NSA meetings has revealed that this non-native English meeting data contains transcription inaccuracies due to the use of foreign language terms and technical vocabulary. With regard to obtaining consent, the group discuss the extent to which they need to attempt to contact people, which methods are most appropriate, and how much responsibility rests on participants being available and checking their e-mail regularly. Additional topics covered more briefly in this meeting are disk space, the DARPA annual report, progress with the demo, conference submissions and attendance, and requests from the University of Washington for data. Transcriptions are back from IBM, and the group discuss the checking of these, particularly since the pre-segmenter has interfered with back-channel data. "
Bmr027.B,"just to repeat the thing bef that we said last week , it was there 's this suggestion of alternating weeks on more , automatic speech recognition related or not ? at some point y you go around and get people to sign something ? july fifteenth . cuz i because if we wanna be able to give it to people july fifteenth , if somebody 's gonna come back and say "" i don't want this and this used "" , clearly we need some time to respond to that . nsa . it you 're right . sometimes somebody will be away that 's it 's , just a certain risk to take . and if there 's half the people , say , who don't respond by , some period of time , we can just make a list of these people and i 'll hand it to administrative staff or whatever , and they 'll just call them up and say , "" have you is this and would you mail mail adam that it is , if i if it , is or not . "" i email is enough . i 'm not a lawyer , but i 've been through these things a f things f like this a few times with lawyers now 'm pretty comfortable with that . the way icsi goes , people , who , were here ten years ago still have acc have forwards to other accounts and on . if we get to a boundary case like that then will call the attorney about it . are you del are you bleeping it by adding ? we heard anything from ibm ? no . just buy them . we 've just ordered a hundred gigabytes . and there 's also an annual report . now , they darpa just said do an annual report . anyway , i 'll be putting together i 'll do it , as much as without bothering people , just by looking at papers and status reports . the status reports you do are very helpful . can grab there . and if , if i have some questions i 'll but , i 'm before it 's all done , i 'll end up bugging people for more clarification about y you going to , eurospeech ? ","With regard to obtaining consent, the group discuss the extent to which they need to attempt to contact people, which methods are most appropriate, and how much responsibility rests on participants being available and checking their e-mail regularly. The group suggest sending reminder e-mails, although since many participants are local they can be contacted by other means if necessary. Checking of the NSA meetings has revealed that this non-native English meeting data contains transcription inaccuracies due to the use of foreign language terms and technical vocabulary. Additional topics covered more briefly in this meeting are disk space, the DARPA annual report, progress with the demo, conference submissions and attendance, and requests from the University of Washington for data. Transcriptions are back from IBM, and the group discuss the checking of these, particularly since the pre-segmenter has interfered with back-channel data. "
Bmr027.C,we got the transcript back from that one meeting . everything seemed fine . adam had a script that will put everything back together and there was there was one small problem but it was a simple thing to fix . it 's gonna have to go through our regular process . ,"Transcriptions are back from IBM, and the group discuss the checking of these, particularly since the pre-segmenter has interfered with back-channel data. "
Bmr027.D,"i still need to get a pair , too . all i need is to hang it off the person who 's coming in , sonali 's , computer . can find something if i 'm desperate ",
Bmr027.E,"because although they signed this , they don't know by which date to expect your email . and someone whose machine is down or whatever the other thing that there 's a psychological effect that at least for most people , that if they 've responded to your email saying "" yes , i will do it "" or "" yes , i got your email "" , they 're more likely to actually do it later than to just ignore it . an official from somebody is better than no answer , even if they responded that they got your email . guess if you 're in both these types of meetings , you 'd have a lot . it also depends on how many like , if we release this time it 's a fairly small number of meetings , and then , the new data that don will start to process before we were working with these segments that were all synchronous and that 's stage two of trying the same kinds of alignments with the tighter boundaries with them is really the next step . we got our abstract accepted for this conference , isca workshop , in , new jersey . and we sent in a very poor abstract , but we 're hoping to have a paper for that as which should be an interesting but , the good news is that will have the european experts in prosody different crowd , and we 're the only people working on prosody in meetings far , it 's isca workshop on prosody in speech recognition and understanding , like that ","The group suggest sending reminder e-mails, although since many participants are local they can be contacted by other means if necessary. Additional topics covered more briefly in this meeting are disk space, the DARPA annual report, progress with the demo, conference submissions and attendance, and requests from the University of Washington for data. "
Bmr027.F,"didn't send out agenda items because until five minutes ago we only had one agenda item and now we have two . we haven't really started , but i figure also if they 're short agenda items , we could also do a little bit of each . as most of you should know , i did send out the consent form thingies far no one has made any ach ! any comments on them . no . and we had decided that they have they only needed to sign once . and if you disagree with it , why don't you read it and give me comments on it ? they 've already signed a form . and the s and the form was approved by human subjects , because people don't read their email , or they 'll read and say "" i don't care about that , i 'm not gonna delete anything "" and they don just won't reply to it . what are we gonna do when we run into someone that we can't get in touch with ? that if they give us contact information and that contact information isn't accurate that we fulfilled our burden . disk space , that for every meeting any meeting which has any bleeps in it we need yet another copy of . don't want i really would rather make a copy of it , rather than bleep it out and then overlapping . it 's exactly a censor bleep . what i really think is "" bleep "" what my s expectation is , is that we 'll send out one of these emails every time a meeting has been checked and is ready . now we haven't actually had anyone go through that meeting , to see whether the transcript is correct the one thing i noticed is it did miss a lot of backchannels . they 're not in the segmented . it 's not that the ibm people didn't do it . but i bet they 're acoustically challenging parts anyway , though . it 's just jargon . the only one was don wanted to , talk about disk space yet again . if you 're desperate , i have some space on my drive . when we remember to fill them out . next week we 'll do automatic transcription status , plus anything that 's real timely . i don't have a paper but i 'd kinda like to go , ","Although the Meeting Recorder group only list two agenda items, this meeting explores transcription, and in particular, consent forms in depth, and at times results in heated debate. With regard to obtaining consent, the group discuss the extent to which they need to attempt to contact people, which methods are most appropriate, and how much responsibility rests on participants being available and checking their e-mail regularly. The group suggest sending reminder e-mails, although since many participants are local they can be contacted by other means if necessary. Additional topics covered more briefly in this meeting are disk space, the DARPA annual report, progress with the demo, conference submissions and attendance, and requests from the University of Washington for data. Transcriptions are back from IBM, and the group discuss the checking of these, particularly since the pre-segmenter has interfered with back-channel data. Checking of the NSA meetings has revealed that this non-native English meeting data contains transcription inaccuracies due to the use of foreign language terms and technical vocabulary. "
Bmr027.G,,
Bmr027.H,"w when the whole thing starts , when they sign the agreement that specify exactly what , how they will be contacted a and , then , say very clearly that if they don't if we don't hear from them , as morgan suggested , by a certain time or after a certain period after we contact them that is implicitly giving their agreement . because if you , do this and you then there 's a dispute later and , some someone who understands these matters concludes that they didn't have , enough opportunity to actually exercise their right do we have mailing addresses for these people ? but we don't our language model right now doesn't know about these words anyhow . you have spare headsets ? because i , i could use one on my workstation , the disk space was short . the next thing on our agenda is to go back and look at the , the automatic alignments i learned from thilo what data we can use as a benchmark to see how we 're doing on automatic alignments of the background speech or , of the foreground speech with background speech . ","The group suggest sending reminder e-mails, although since many participants are local they can be contacted by other means if necessary. Checking of the NSA meetings has revealed that this non-native English meeting data contains transcription inaccuracies due to the use of foreign language terms and technical vocabulary. Additional topics covered more briefly in this meeting are disk space, the DARPA annual report, progress with the demo, conference submissions and attendance, and requests from the University of Washington for data. "
Bmr028.A,,
Bmr028.B,,
Bmr028.C,,
Bmr028.D,,
Bmr028.E,,
Bmr028.F,,
Bmr028.H,,
Bmr028.I,,
Bmr029.A,,
Bmr029.B,,
Bmr029.C,,
Bmr029.E,,
Bmr029.F,,
Bmr030.A,,
Bmr030.B,,
Bmr030.C,,
Bmr030.D,,
Bmr030.E,,
Bmr031.A,,
Bmr031.B,,
Bmr031.C,,
Bmr031.D,,
Bmr031.E,,
Bmr031.F,,
Bmr031.G,,
Bmr031.H,,
Bmr031.I,,
Bns001.A,,
Bns001.B,,
Bns001.C,,
Bns001.D,,
Bns001.E,,
Bns001.F,,
Bns001.G,,
Bns002.A,,
Bns002.B,,
Bns002.C,,
Bns002.D,,
Bns002.E,,
Bns003.A,,
Bns003.B,,
Bns003.C,,
Bns003.D,,
Bns003.E,,
Bns003.F,,
Bro003.A,"did you happen to find out anything about the ogi multilingual database ? were the digits , hand labeled for phones ? you 're doing this test because you want to determine whether or not , having s general speech performs as as having specific speech . was just wondering if the fact that timit you 're using the hand labeled from timit might be confuse the results that you get . i 'll check on that . dan david , put a new , drive onto abbott , that 's an x disk , i 've been going through and copying data that is , some corpus usually , that we 've got on a cd rom onto that new disk to free up space on other disks . we haven't deleted them off of the slash dc disk that they 're on right now in abbott , but we i would like to go through sit down with you about some of these other ones and see if we can move them onto , this new disk also . they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ? you 're saying that there may not be enough information coming out of the net to help you discriminate the words ? we could do an interesting cheating experiment with that too . was thinking , it made me think about this , that if it 'd be an interesting experiment just to see , if you did get all of those right . ","The main topics discussed were arrangements and objectives of an
upcoming field trip to visit research partners OGI; a number of
members reported their progress to date; if there are any tasks that
one member can help others with; an overall description of the Cube
project, a multi-lingual speech recognition system for use by the
cellular phone industry, along with consideration of some of the
issues therein, specifically disk and resource issues. "
Bro003.B,"isn't there like a limit on the computation load , or d latency , like that for aurora task ? fact , most confusions are within the phone classes , right ? ",
Bro003.C,"let 's see , i spent the last week , looking over stephane 's shoulder . and understanding some of the data . i re installed , htk , the free version , which is the same version that , ogi is using . 've been looking at , timit the that we 've been working on with timit , trying to get a , labels file we can , train up a net on timit and test , the difference between this net trained on timit and a net trained on digits alone . the inputs are one dimension of the cube , which , we 've talked about it being , plp , m f c cs , j jrasta , jrasta lda those were automatically derived by dan using , embedded training and alignment . the cube will have three dimensions . the first dimension is the features that we 're going to use . and the second dimension , is the training corpus . and that 's the training on the discriminant neural net . and then , there 's the testing corpus . for the training corpus , we have , the d digits from the various languages . y you did a you did it on a spert board . we could look at articulatory type ","Essentially the cube consists of three dimension: input features;
training corpus; and test corpus. Most important concerns are which
combinations of features to use, and what combinations of languages
and broad/specific corpora to use for the training "
Bro003.D,,
Bro003.E,"sampa phone ? for english american english , and the language who have more phone are the english . but n in spain , the spanish have several phone that d doesn't appear in the e english and we thought to complete . but for that , it needs we must r h do a lot of work because we need to generate new tran transcription for the database that we have . one day . depends on the corpus . yes . d i begin to work with the italian database to nnn , to with the f front end and with the htk program and the @ @ . and i trained with the spanish two neural network with plp and with lograsta plp . and to recognize the italian digits with the neural netw spanish neural network , but prepa to prepare the database are difficult . was for me , n it was a difficult work last week with the labels because the program with the label obtained that i have , the albayzin , is different w to the label to train the neural network . and that is another work that we must to do , to change . the spanish labels ? that was in different format , that the format for the the program to train the neural network . i necessary to convert . ",
Bro003.F,"we should talk a little bit about the plans for the the field trip next week . and mostly first though about the logistics for it . then later on in the meeting we should talk about what we actually might accomplish . in and go around see what people have been doing talk about that , a r progress report . essentially . and then another topic i had was that dave here had said give me something to do . "" and we can discuss that a little bit . and then talk a little bit about disks and resource issues that 's starting to get worked out . those of you who are not , used to this area , it can be very tricky to get to the airport at six thirty . if everybody can get here at six . what 's been going on ? we also have this broadcast news that we were talking about taking off the disk , which is microphone data for english . cuz you don't know who 's gonna call , how do what language it is ? somebody picks up the phone . thi this is their image . the phone doesn't what your language is . but the particular image that the cellular industry has right now is that it 's distributed speech recognition , where the , probabilistic part , and s semantics and forth are all on the servers , and you compute features of the on the phone . we might or might not agree that 's the way it will be in ten years , but that 's that 's what they 're asking for . think that th it is an important issue whether it works cross language .  your turn . it would be another interesting scientific question to ask , "" is it because it 's a broad source or because it was , carefully ? "" superset , a actually now you 've got me intrigued . can you describe what 's on the cube ? something like seven things in each , each column . that 's , three hundred and forty three , different systems that are going to be developed . there 's not really a limit . what it is that there 's , it 's just penalty , that if you 're using , a megabyte , then they 'll say that 's very but , it will never go on a cheap cell phone . how long does it take for an , htk training ? clearly , there 's no way we can even begin to do an any significant amount here unless we use multiple machines . there 's plenty of machines here and they 're n they 're often not in a great deal of use . it 's let 's say it 's six hours or eight hours , for the training of htk . how long is it for training of , the neural net ? again , we do have a bunch of spert boards . you could set up , ten different jobs , to run on spert different spert boards or we 're not going to get through any significant number of these . with very limited time , we actually have really quite a bit of computational resource available if you , get a look across the institute and how little things are being used . carmen , did you do you have something else to add ? it seems like there 's some peculiarities of the , of each of these dimensions that are getting sorted out . and then , if you work on getting the , assembly lines together , and then the pieces get ready to go into the assembly line and gradually can start , start turning the crank , more or less . what 's great about this is it sets it up in a very systematic way , that , once these all of these , mundane but real problems get sorted out , we can just start turning the crank and , that once you get a better handle on how much you can realistically do , concurrently on different machines , different sperts , and forth , and you see how long it takes on what machine and forth , you can stand back from it and say , "" if we look these combinations we 're talking about , and combinations of combinations , and forth , "" you 'll probably find you can't do it all . then at that point , we should sort out which ones do we throw away . which of the combinations across what are the most likely ones , we 're over the next year or two , we 're gonna be upgrading the networks in this place , but right now they 're still all te all ten megabit lines . and we have reached the this the machines are getting faster and faster . it actually has reached the point where it 's a significant drag on the time for something to move the data from one place to another . it 's gonna take us a couple weeks at least to get the , the amount of disk we 're gonna be getting . we 're actually gonna get , four more , thirty six gigabyte drives stephane , where you 're doing your computations . you 're the disk czar now . an another question occurred to me is what were you folks planning to do about normalization ? it 's i we seem to have enough dimensions as it is . i we 're already there , or almost there , is goals for the for next week 's meeting . i it seems to me that we wanna do is flush out what you put on the board here . we can say what we 're doing , ","The main topics discussed were arrangements and objectives of an
upcoming field trip to visit research partners OGI; a number of
members reported their progress to date; if there are any tasks that
one member can help others with; an overall description of the Cube
project, a multi-lingual speech recognition system for use by the
cellular phone industry, along with consideration of some of the
issues therein, specifically disk and resource issues. Essentially the cube consists of three dimension: input features;
training corpus; and test corpus. Most important concerns are which
combinations of features to use, and what combinations of languages
and broad/specific corpora to use for the training "
Bro003.F,"and , also , if you have sorted out , this information about how long i roughly how long it takes to do on what and , what we can how many of these trainings , and testings and forth that we can realistically do , then one of the big goals of going there next week would be to actually settle on which of them we 're gonna do . and , when we come back we can charge in and do it . and and the other the last topic i had here was , dave 's fine offer to , do something on this . he 's doing he 's working on other things , but to do something on this project . the question is , "" where could we , most use dave 's help ? "" let 's fall back to that . but the first responsibility is to figure out if there 's something that , an additional what an additional clever person could help with when we 're really in a crunch for time . but if we could think of some piece that 's defined , that he could help with , he 's expressing a will willingness to do that . wh that the other suggestion that just came up was , what about having him work on the , multilingual super f superset thing . coming up with that and then , training it training a net on that , say , from , from timit wh what would this task consist of ? has ogi done anything about this issue ? do they have any superset that they already have ? because that 's the other route to go . to really mark articulatory features , you really wanna look at the acoustics and see where everything is , and we 're not gonna do that . the second class way of doing it is to look at the , phones that are labeled and translate them into acoustic articulatory , features . it won't really be right . is that we could , just translate instead of translating to a superset , just translate to articulatory features , some set of articulatory features and train with that . ",
Bro003.G,"preparation of the french test data actually . it is , a digit french database of microphone speech , and i 've added noise to one part , with the actually the aurora two noises . one they call the multi language database , and another one is a twenty two language , something like that . but it 's also telephone speech . actually , for the moment if we w do not want to use these phone databases , we already have english , spanish and french with microphone speech . actually , these three databases are generic databases . f for italian , which is close to spanish , french and , i ti digits we have both digits training data and also more general training data . perhaps there is also timit . and we plan to develop a subset of the phonemes , that includes , all the phonemes of our training languages ,  it 's around six hours , i would say two days . it 's nutmeg and mustard ,  we were thinking about using this systematically for all the experiments . but we think perhaps we can use the best , normalization scheme as ogi is using , with parameters that they use there , and , trying to have a closer look at the perhaps the , speech , noise detection or , voiced sound unvoiced sound detection creating the superset , and , modifying the lab labels for matching the superset . creating the mappings , actually . they they 're going actually the other way , defining phoneme clusters , and e i perhaps u using broad phoneme classes , it 's for classifying the digits , but as soon as you will have more words , words can differ with only a single phoneme , and which could be the same , class .  ","The main topics discussed were arrangements and objectives of an
upcoming field trip to visit research partners OGI; a number of
members reported their progress to date; if there are any tasks that
one member can help others with; an overall description of the Cube
project, a multi-lingual speech recognition system for use by the
cellular phone industry, along with consideration of some of the
issues therein, specifically disk and resource issues. "
Bro004.A,it 's the broad data . ,"There have been four types of test, in which the training data
varies, and a variety of input features have been tried. "
Bro004.B,"where are we on our runs ? that 's including the w the the one that it 's now , what 's the noise condition of the training data the noise condition is the same there 's not a statistical sta a strong st statistically different noise characteristic between the training and test but that given the pressure of time we probably want to draw because of that especially , we wanna draw some conclusions from this , and make some strong decisions for what we 're gonna do testing on before next week . and once you the other thing is that once you represent start representing more and more context it is much more specific to a particular task in language . you may have some kinds of contexts that will never occur in one language and will occur frequently in the other , the issue of getting enough training for a particular context becomes harder . we already actually don't have a huge amount of training data but it would still be even more of a binary decision . that would be even more distinct of a binary decision . we could disagree about it at length but the real thing is if you 're interested in it you 'll probably try it and we 'll see . are there is there are there any h t k trainings testings going on ? but you think if you include that plus the other features , and then just to remind me , all of that goes into that all of that is transformed by k kl or ? there 's a question of whether you would whether you would transform together or just one . might wanna try it both ways . think hynek will be here monday . think , we need to choose the experiments carefully , we can get key questions answered before then they 're doing the vad they mean voice activity detection again , it 's the silence their the results look pretty good . think that it 's to do that in this because it 's gonna give a better word error result and therefore will help within an evaluation . as part of the problem with evaluation right now is that the word models are pretty bad and nobody wants has approached improving them . the question we 're gonna wanna go through next week when hynek shows up is given that we 've been we 're looking at by then combinations of features and multi band and we 've been looking at cross language , cross task issues . but they 've been looking at at these issues . at the on line normalization and the voice activity detection . and when he comes here we 're gonna have to start deciding about what do we choose from what we 've looked at to blend with some group of things in what they 've looked at and once we choose that , how do we split up the effort ? we have the little tiny ibm machine that might someday grow up to be a big ibm machine . it 's got s slots for eight , we only got two far , you can check with dave johnson . and somebody could do check out the multi threading libraries . the prudent thing to do would be for somebody to do the work on getting our code running on that machine with two processors even though there aren't five or eight . there 's there 's gonna be debugging hassles and then we 'd be set for when we did have five or eight , to have it really be useful . but . notice how i said somebody and turned my head your direction . that 's one thing you don't get in these recordings . probably the neural net ","The meeting was dominated by a discussion of the first results coming
in. There have been four types of test, in which the training data
varies, and a variety of input features have been tried. The process
and results were explained to the group, the implications of the
results discussed, and plans for moving forward were made. There was also discussion of some of the work being conducted by
research partners OGI, including how the two groups should best work
together. The group also briefly touched upon resource issues. "
Bro004.C,,
Bro004.D,"we as i was already said , we mainly focused on four features . the plp , the plp with jrasta , the msg , and the mfcc from the baseline aurora . and we focused for the test part on the english and the italian . we 've trained several neural networks on on the ti digits english and on the italian data and also on the broad english french and spanish databases . and actually what we @ @ observed is that if the network is trained on the task data it works pretty but actually we didn't train network on both types of data we only did either task data or broad data . the first testing is with task data the second test is trained on a single language with broad database , but the same language as the t task data . but for italian we choose spanish which we assume is close to italian . the third test is by using , the three language database  and the fourth test is excluding from these three languages the language that is the task language . example when we go from ti digits training to timit training we lose around ten percent , the error rate increase u of ten percent , relative . and then when we jump to the multilingual data it 's it become worse twenty to thirty percent further . this was for the plp , for the plp with jrasta the we this is quite the same tendency , with a slight increase of the error rate , if we go to timit . and then it 's it gets worse with the multilingual . there is a difference actually with b between plp and jrasta is that jrasta seems to perform better with the highly mismatched condition but slightly worse for the matched condition . no these are the s same noises , at least for the first for the matched , for the italian the results are stranger what appears is that perhaps spanish is not very close to italian because when using the network trained only on spanish it 's the error rate is almost twice the baseline error rate . there is another difference , is that the noise the noises are different . for the italian part the the networks are trained with noise from aurora ti digits , and perhaps the noise are quite different from the noises in the speech that italian . but actually there is something important , is that we made a lot of assumption concerning the on line normalization and we just noticed recently that the approach that we were using was not leading to very good results when we used the straight features to htk . what we see that is there is that the way we were doing this was not correct , when we use the networks our number are better that pratibha results . and the first thing is the alpha value . i used point five percent , which was the default value in the programs here . and pratibha used five percent . i assume that this was not important because previous results from dan and show that the both values g give the same results . it was true on ti digits but it 's not true on italian . second thing is the initialization of the actually , what we were doing is to start the recursion from the beginning of the utterance . and using initial values that are the global mean and variances measured across the whole database . and pratibha did something different is that he she initialed the values of the mean and variance by computing this on the twenty five first frames of each utterance . there were other minor differences , i changed the code and now we have a baseline that 's similar to the ogi baseline . the networks are retaining with these new features . act actually we have discussed @ these and we were thinking perhaps that the way we use the tandem is not if we trained the networks on the on a language and a t or a specific task , what we ask is to the network is to put the bound the decision boundaries somewhere in the space . and and ask the network to put one , at one side of the for a particular phoneme at one side of the boundary decision boundary and there is reduction of the information there that 's not correct because if we change task and if the phonemes are not in the same context in the new task , the decision boundaries are not should not be at the same place . but the way the feature gives the way the network gives the features is that it reduce completely the it removes completely the information a lot of information from the features by placing the decision boundaries at optimal places for one data what we were thinking about is perhaps one way to solve this problem is increase the number of outputs of the neural networks . context dependent phonemes . the way we do it now is that we have a neural network and the net network is trained almost to give binary decisions . and we plan to work also on the idea of using both features and net outputs . we have come up with different broad phonetic categories . and we have we have three types of broad phonetic classes . twenty seven broad classes . it 's it 's standard net with fewer classes . because i believe the effect that of too reducing too much the information is what happens one single kl to transform everything ","The meeting was dominated by a discussion of the first results coming
in. The process
and results were explained to the group, the implications of the
results discussed, and plans for moving forward were made. There have been four types of test, in which the training data
varies, and a variety of input features have been tried. "
Bro004.E,"i i 'm trying the htk with plp twelve on line delta and msg filter together . the combination , but i haven't result at this moment . ",
Bro004.F,but including the features . ,
Bro004.G,"could try to get the train the neural network trainings or the htk running under linux , and to start with i 'm wondering which one i should pick first . ",
Bro005.A,"i 'm for the table , but as it grows in size , it . s there is summary of what has been done summary of experiments since , since last week and also since the we 've started to run work on this . since last week we 've started to fill the column with features w with nets trained on plp with on line normalization but with delta also , but we have more results to compare with network using without plp and finally , hhh , ehhh pl delta seems very important . when we use the large training set using french , spanish , and english , you have one hundred and six without delta and eighty nine with the delta .  but , actually , for english training on timit is still better than the other languages . it 's multi english , we have a ninety one number , and training with other languages is a little bit worse . and , and here the gap is still more important between using delta and not using delta . if y if i take the training s the large training set , it 's we have one hundred and seventy two , and one hundred and four when we use delta . even if the contexts used is quite the same , except for the multi english , which is always one of the best . then we started to work on a large dat database containing , sentences from the french , from the spanish , from the timit , from spine , from english digits , and from italian digits . and actually we did this before knowing the result of all the data , we have to redo the the experiment training the net with , plp , but with delta . we have also started feature combination experiments . many experiments using features and net outputs together . and this is the results are on the other document . it 's the similar to the tandem that was proposed for the first . the multi stream tandem for the first proposal . the second is using features and klt transformed mlp outputs . and the third one is to u use a single klt trans transform features as as mlp outputs . we ju just to be clear , the numbers here are recognition accuracy . actually , if w we look at the table , the huge table , we see that for ti digits msg perform as as the plp , but this is not the case for italian what where the error rate is c is almost twice the error rate of plp . i don't exactly . perhaps the fact that the there 's no low pass filter , or no pre emp pre emphasis filter and that there is some dc offset in the italian , but that we need to sort out if want to get improvement by combining plp and msg because for the moment msg do doesn't bring much information . this is this is in the table . the number is fifty two , fift no , it 's the of eighteen of eighteen . it 's error rate , it 's er error rate ratio . we have nine let 's say ninety percent .  no .  and test across everything . discussion with hynek , sunil and pratibha for trying to plug in their our networks with their within their block diagram , where to plug in the network , after the feature , before as as a plugin or as a anoth another path , actually hynek would like to see , perhaps if you remember the block diagram there is , temporal lda followed b by a spectral lda for each critical band . and he would like to replace these by a network which would , make the system look like a trap . i started multi band mlp trainings , take exactly the same configurations , seven bands with nine frames of context , and we just train on timit , and on the large database , with spine and everything . i 'm starting to train also , networks with larger contexts . this would be something between traps and multi band because we still have quite large bands , and but with a lot of context also . we still have to work on finnish , to make a decision on which mlp can be the best across the different languages . for the moment it 's the timit network , and perhaps the network trained on everything . the next part of the document is , a summary of what everything that has been done . we have seventy nine m l ps trained on ten on ten different databases . as we mentioned , timit is the only that 's hand labeled , and perhaps this is what makes the difference . the other are just viterbi aligned . the observation is what we discussed already . the msg problem , the fact that the mlp trained on target task decreased the error rate . but when the m mlp is trained on the is not trained on the target task , it increased the error rate compared to using straight features . the fourth point is , the timit plus noise seems to be the training set that gives better the best network . the reason the reason is that the perhaps the target the task dependency the language dependency , and the noise dependency the e but this is still not clear i don't think we have enough result to talk about the language dependency . the timit network is still the best the fact that it 's hand labeled . but you have two effects , the effect of changing language and the effect of training on something that 's viterbi aligned instead of hand labeled . but but , perhaps it 's not really the alignment that 's bad ","This included reporting the results, and making conclusions to shape future work. The main topic for discussion by the Berkeley Meeting Recorder group was progress on the experiments run as part of the groups main project, a speech recogniser for the cellular industry. Also discussed were the details of the continued collaboration with project partner OGI. "
Bro005.A,"but the just the ph phoneme string that 's used for the alignment there might be errors just in the ph string of phonemes . this is not really the viterbi alignment , we can , we can tell which training set gives the best result , but we don't know exactly why .  the two of the main issues perhaps are still the language dependency and the noise dependency . and perhaps to try to reduce the language dependency , we should focus on finding some other training targets . for moment you use we use phonetic targets but we could also use articulatory targets , soft targets , and perhaps even , use networks that doesn't do classification but just regression and com compute features and noit not , nnn , features without noise . transform the fea noisy features in other features that are not noisy . perhaps , something like multi band trained on a lot of noises with features based targets could help . the future work is , try to connect to the to make to plug in the system to the ogi there are still open questions there , where to put the mlp thi this sh would be more working on the mlp as an additional path instead of an insert to the to their diagram . perhaps the insert idea is strange the way we want to do it perhaps is to just to get the vad labels and the final features . they will send us the provide us with the feature files , ",Also discussed were the details of the continued collaboration with project partner OGI. 
Bro005.B,"yes , and the baseline have i is eighty two . and first in the experiment one i do i use different mlp , and is that the multi english mlp is the better . for the ne rest of experiment i use multi english , and i try to combine different type of feature , but the result is that the msg three feature doesn't work for the italian database because never help to increase the accuracy .  another table . eighty . it 's plus six .  first the feature are without delta and delta , and we can see that in the situation , the msg three , the same help nothing . and then i do the same but with the delta and delta plp delta and delta . ","This included reporting the results, and making conclusions to shape future work. "
Bro005.C,"stephane was saying that they weren't hand labeled , the french and the spanish . they 've been running all the experiments and and i 've been w doing some work on the preparing all the data for them to train and to test on . right now , i 'm focusing mainly on this final project i 'm working on in jordan 's class . and i 'm just gonna see if that better models asynchrony in any way ",
Bro005.D,"you 've got some , xerox things to pass out ? for th the last column we use our imagination . a all of these numbers are with a hundred percent being , the baseline performance , baseline is eighty two . when you said the baseline system was eighty two percent , that was trained on what and tested on what ? that was , italian mismatched d digits , is the testing , and the training is italian digits ? the "" mismatch "" just refers to the noise and , microphone and forth , what that says is that in a matched condition , we end up with a fair amount worse putting in the plp . now w would do we have a number , i suppose for the matched i don't mean matched , but use of italian training in italian digits for plp only ? fifty two percent . no , fifty two percent of eighty two ? this is accuracy ! i 'm i k i keep getting confused because this is accuracy . and then adding the msg does nothing , actually , the answer for experiments with one is that adding msg , if you does not help in that case . then you 're assuming multi english is closer to the thing that you could use since you 're not gonna have matching , data for the for the new for the other languages and forth . it 's still it hurts you seems to hurt you a fair amount to add in this french and spanish . i wonder why what we 're saying is that one o one of the things that my interpretation of your s original suggestion is something like this , as motivation . when we train on data that is in one sense or another , similar to the testing data , then we get a win by having discriminant training . when we train on something that 's quite different , we have a potential to have some problems . guess the other thing is to take if one were to take , a couple of the most successful of these , try all these different tests . on the msg problem that in the in the short time solution that is , trying to figure out what we can proceed forward with to make the greatest progress , it 's in category that it 's , it may be complicated . and it might be if someone 's interested in it , certainly encourage anybody to look into it in the longer term , once we get out of this particular rush for results . but in the short term , unless you have some s strong idea of what 's wrong , but my guess would be that it 's something that is a simple thing that could take a while to find . that 's that what we were concerned about is that if it 's not on the target task if it 's on the target task then it helps to have the mlp transforming it . if it if it 's not on the target task , then , depending on how different it is , you can get a reduction in performance . yes there 's what you would expect in terms of a language dependency and a noise dependency . that is , when the neural net is trained on one of those and tested on something different , we don't do as as in the target thing . do you think the alignments are bad ? have you looked at the alignments what the viterbi alignment 's doing ? might be interesting to look at it . the pronunciation models and forth right , the multi english far is the best . "" multi english "" just means "" timit "" , that seems like a good thing to do , probably , not again a short term thing . but one of the core quote "" open questions "" for that is if we take the the best ones here , not just the best one , but the best few you want the most promising group from these other experiments . we know that there 's a mis there 's a loss in performance when the neural net is trained on conditions that are different than , we 're gonna test on , but if you look over a range of these different tests how do these different ways of combining the straight features with the mlp features , stand up over that range ? look at these different ways of combining it . and just look take that case and then look over all the different things . how does that compare between the all the different test sets , and for the couple different ways that you have of combining them . how do they stand up , over the we wanna get their path running here , if we can add this other as an additional path we first thing we 'd wanna do there is to make that when we get those labels of final features is that we get the same results as them . without putting in a second path . just th w to make that we have we understand properly what things are , our very first thing to do is to double check that we get the exact same results as them on htk . here the problem seems to be is that we don't have a hug a really huge net with a really huge amount of training data . but we have s f for this task , i would think , modest amount . a million frames actually isn't that much . we have a modest amount of training data from a couple different conditions , and then in that and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type ","The main topic for discussion by the Berkeley Meeting Recorder group was progress on the experiments run as part of the groups main project, a speech recogniser for the cellular industry. This included reporting the results, and making conclusions to shape future work. Also discussed were the details of the continued collaboration with project partner OGI. "
Bro005.D,"and channel characteristic , barry , you 've been pretty quiet . but that what were you involved in this primarily ? ",
Bro005.E,,
Bro007.A,,
Bro007.B,"today we 're looking at a number of things we 're trying do you e they mentioned made some when i was on the phone with sunil they mentioned some weighting scheme that was used to evaluate all of these numbers . and we don't have the ti digits part yet ? and have you put all these numbers together into a single number representing that ? that should be pretty easy to do and that would be good then we could compare the two and say what was better . the one place where it looks like we 're messing things up a bit is in the highly mismatched italian . one of the ideas that you had mentioned last time was having a second silence detection . we can't do it . how many words are in one of these test sets ? see the the reason i 'm asking is we have all these small differences and i don't know how to take them , right ? if you had just to give an example , if you had if you had a thousand words then tenth of a percent would just be one word , it wouldn't mean anything . it be 'd like to the sizes of these test sets were actually . also just to know the numbers , anyway if you could just mail out what those numbers are and then that be great . the silence plus the klt output ? it looks to me the same given that we have to take the filt ones out of the running because of this delay problem it looks to me like the ones you said i agree are the ones to look at but would add the second row one if we can how many words are in each and then dave dave promised to get us something tomorrow which will be there as far as they 've gotten friday and then we 'll operate with that do we fix the system tomorrow or do we fix the system on tuesday ? i except that we do have to write it up . what we do is we as soon as we get the data from them we start the training and forth but we start the write up right away because as you say there 's only minor differences between these . and i would i would i 'd like to see it edit it a bit anyway , sounds like there 'll be a lot to do just to work with our partners to fill out the tables over the next next few days yes , mean we have to actually get it done tuesday but my assumption is that we have to be done tuesday . then next thursday we can have a little aftermath and next meeting we can start talking a little bit about where we want to go from here in terms of the research . what things did you think of when you were doing this process that you just didn't really have time to adequately work on that would be pretty low maintenance to try it . the other thing that i will say is that the digits that we 're gonna record momentarily is starting to get are starting to get into a pretty good size collection and in addition to the speechdat we will have those to work with really pretty soon now that 's another source of data . ","They discussed most recent results, finalized plans to continue and discussed the work required and timing needed for completion of this stage of the project. "
Bro007.C,"it 's forty percent for ti digit , sixty for all the speechdat cars , generally what you observe with ti digits is that the result are very close whatever the system . not yet . actually ogi two is the baseline with the ogi features but this is not exactly the result that they have because they 've they 're still made some changes in the features and but actually our results are better than their results . don't know by how much because they did not send us the new results it seems f for the match and mismatched condition it 's it brings something . but actually there are there 's no room left for any silence detector at the server side because of the delay . no . it 's it depends the matched is generally larger than the other sets no they 're there is this silence in addition to the klt outputs it is because we just keep we don't keep all the dimensions after the klt we try to add the silence also in addition to the these twenty eight dimensions . we fixed on tuesday , you we could start soon , write up something . we will we 'll try to focus on these three architectures and perhaps i was thinking also a fourth one with just a single klt ","They discussed most recent results, finalized plans to continue and discussed the work required and timing needed for completion of this stage of the project. "
Bro007.D,it 's only one small experiment to happened . to apply also to in include also the silence of the mlp we have the fifty six form and the silence to pick up the silence and we include those . and we not s we are not if we pick we have the silence . ,"They discussed most recent results, finalized plans to continue and discussed the work required and timing needed for completion of this stage of the project. "
Bro008.A,"y actually , for the danish , there 's still some mystery because , when we use the straight features , we are not able to get these number with the icsi ogi one , that 's probably something wrong with the features that we get from ogi . and sunil is working on trying to check everything . this tuesday , hynek will try to push for trying to combine , different things ? first , to really have a look at the speech from these databases because , we tried several thing , but we did not really look at what 's happening , and where is the noise , and actually , there is one thing that generally we think that most of the errors are within phoneme classes , think it could be interesting to see if it i don't think it 's still true when we add noise , and we have the confusion ma the confusion matrices are very different when we have noise , and when it 's clean speech . and probably , there is much more between classes errors for noisy speech . perhaps we could have a large gain , just by looking at improving the , recognition , not of phonemes , but of phoneme classes , simply . again , it 's the thing that , we were thin thinking that it would work , but it didn't work . and , there is not a bug , but something wrong in what we are doing , perhaps . something wrong , perhaps in the just in the fact that the labels are what worked best is the hand labeled data . i don't know if we can get some hand labeled data from other languages . but that would be something interesting t to see . ",
Bro008.B,,
Bro008.C,"how much better was the best system than ours ? when is the development set the , test set results due ? now , i 'm interested in , looking at the experiments where you use , data from multiple languages to train the neural net . and i don't know how far , or if you guys even had a chance to try that , but that would be some it 'd be interesting to me . ",
Bro008.D,"i got , these results from , stephane . also , that , we might hear later today , about other results . that , there were some other very good results that we 're gonna wanna compare to . but , r our results from other places , most of the time , even even though it 's true that the overall number for danish we didn't improve it we have a little bit of time on that , actually . we have a day or when do you folks leave ? sunday ? until saturday midnight , we have even with these results as they are , it 's it 's really not that bad . and it looks like the overall result as they are now , even without , any bugs being fixed is that , on the other tasks , we had this average of , forty nine percent , or improvement . and here we have somewhat better than that than the danish , and somewhat worse than that on the german , but it sounds like , one way or another , the methods that we 're doing can reduce the error rate from mel ceptrum down by , fourth of them to , a half of them . that , one of the things that hynek was talking about was understanding what was in the other really good proposals and trying to see if what should ultimately be proposed is some , combination of things . cuz there 's things that they are doing there that we certainly are not doing . and there 's things that we 're doing that they 're not doing . we don't know yet . first place , there 's still this thing to work out , and second place second thing is that the only results that we have far from before were really development set results . it 's probably a good time to look at what 's really going on and seeing if there 's a way to combine the best ideas while at the same time not blowing up the amount of , resources used , but i it it looks like they did some , reasonable things , and they 're not things that we did , precisely . we did unreasonable things , which because we like to try strange things , and , and our things worked too . it 's possible that some combination of these different things that were done would be the best thing to do . but the only caveat to that is that everybody 's being real conscious of how much memory and how much cpu they 're using because these , standards are supposed to go on cell phones with m moderate resources in both respects . now , one of the things that 's about what we did is , we do have a , filtering , which leads to a , reduction in the bandwidth in the modulation spectrum , which allows us to downsample . as a result of that we have a reduced , transmission rate for the bits . in reality , if you put this system in into , the field , it would be twenty four hundred bits per second , not forty eight hundred . that 's a feature of what we did . probably the day after they leave , but we 'll have to stop it the day before we leave . tha the meeting is on the thirteenth and the , results are due like the day before the meeting since we have a bit farther to travel than some of the others , we 'll have to get done a little quicker . but , it 's just tracing down these bugs . just exactly this thing of , why these features seem to be behaving differently , in california than in oregon . the question is "" is there some advantage ? "" you could just take the best system and say that 's the standard . but that if different systems are getting at good things , a again within the constraint of the resources , if there 's something simple that you can do everything that we did could probably just be added on to what alcatel did , and i it 'd probably work pretty with them , too . the vad they both had , and , and they both had some on line normalization , it seems like the main different there is the , filtering . shouldn't take a lot of memory to do that and i also wouldn't think the cpu , would be much either for that part . if you can add those in then , you can cut the data rate in half . i 'm interested in hearing your thoughts about where you think we should go from this . we tried a lot of things in a hurry , and , if we can back off from this now and take our time with something , and not have doing things quickly be quite much the constraint , what you think would be the best thing to do . the other thing that strikes me , just looking at these numbers is , just taking the best cases , some of these , even with all of our wonderful processing , still are horrible kinds of numbers . but just take the best case , the matched german case after er matched danish after we the numbers we 're getting are about eight or nine percent error per digit . this is not usable , if you have ten digits for a phone number every now and then you 'll get it right . in a way , that 's , that 's the dominant thing ","This included some discussion of results, comparing various other groups' systems, issues involving the set up, and plans for future work. "
Bro008.D,"is that even , say on the development set that we saw , the , the numbers that , that alcatel was getting when choosing out the best single numbers , it was just it wasn't good enough for a real system . we still have to do . does a any you have any thoughts about what else y you 're thinking that you didn't get to that you would like to do if you had more time ? there were a lot of times when we 've tried something and it didn't work right away , even though we had an intuition that there should be something there . and then we would just stop it . and , one of the things i don't remember the details on , but i remember at some point , when you were working with a second stream , and you tried a low pass filtering to cepstrum , in some case you got but it was an msg like thing , but it wasn't msg , you y in some case you got some little improvement , and it seems to me that , if that is an important idea , which , might be , that one could work at it for a while , as you 're saying . and you had , you had the multi band things also , and , there was issue of that . barry 's going to be , continuing working on multi band things as we were just talking about , some , some work that we 're interested in . inspired by the by larry saul with the , learning articulatory feature in in the case of his paper with sonorance based on , multi band information where you have a combination of gradient learning an and , mean , they 're much better than they were , we 're talking about thirty to sixty percent , error rate reduction . that 's really great to do that in relatively short time . but even after that it 's still , poor that , no one could really use it . also , there was just the whole notion of having multiple nets that were trained on different data . one form of different data was is from different languages , but the other i m in those experiments it wasn't much combining multiple nets , it was a single net that had different first thing is would it be better if they were multiple nets , for some reason ? second thing is , never mind the different languages , just having acoustic conditions rather than training them all up in one , would it be helpful to have different ones ? ",
Bro008.E,"we trying again with the articulatory feature . because we tried we some one experiment that doesn't work . because , tsk do better some step the general , diagram . ",
Bro010.A,"what 's the allowable ? what were they thinking of changing it to ? a person i don't think a person can tell the difference between , a quarter of a second and a hundred milliseconds , i 'm not even if we can tell the difference between a quarter of a second and half a second . it just it feels quick . is the latency from the neural net caused by how far ahead you 're looking ? it seems like this thing could add to the latency . depending on where the window was that you used to calculate the signal to noise ratio . ","The group also touched upon matters that had broader implications for the work, such as the work of other groups on the same project. "
Bro010.B,"do you have news from the conference talk ? we 've a little bit worked on trying to see , what were the bugs and the problem with the latencies . we took first we took the lda filters and , we designed new filters , using recursive filters actually . we took the filters the fir filters and we designed , iir filters that have the same frequency response . similar , but that have shorter delays . and the low frequency band has sixty four milliseconds of delay , and the high frequency band filter has something like eleven milliseconds compared to the two hundred milliseconds of the iir filters . no . three hundred and thirty .  but , when we add up everything it 's it will be alright . sixty five , plus ten , plus for the downsampling , plus eighty five for the on line normalization . plus eighty for the neural net and pca . it would be around two hundred and forty and the best proposal had something like thirty or forty milliseconds of latency . actually , it 's a if you want to have a good estimation on non stationary noise you have to look in the future . they just look in the past . it works because the noise are , pret almost stationary that 's hard to do . ","The Berkeley Meeting Recorder Group discussed the progress of several of their members. The group also touched upon matters that had broader implications for the work, such as the work of other groups on the same project. "
Bro010.C,"no , nobody 's told me anything . no , that would have been a good thing to find out before this meeting , let 's assume for right now that we 're just plugging on ahead , because even if they tell us that , the rules are different , we 're still interested in doing what we 're doing . what are you doing ? you had a discussion with sunil about this though ? you should talk with him . no , because the whole problem that happened before was coordination , you need to discuss with him what we 're doing , cuz they could be doing the same thing and we just have to be in contact more . that the fact that we did that with had that thing with the latencies was indicative of the fact that there wasn't enough communication . that would be , a reduction of a hundred and thirty six milliseconds , what was the total we ended up with through the whole system ? that would be within ? just barely in there . two fifty , unless they changed the rules . which there is there 's some discussion of . the people who had very low latency want it to be low very narrow , latency bound . and the people who have longer latency don't . unfortunately we 're the main ones with long latency , to get down to forty or fifty milliseconds we 'd have to throw out most of what we 're doing . we could cut we else , we could cut down on the neural net time by , playing around a little bit , going more into the past ,  one thing that would be no good to find out about from this conference call is that what they were talking about , what they 're proposing doing , was having a third party , run a good vad , and determine boundaries . and then given those boundaries , then have everybody do the recognition . they argued about that yesterday i don't know the answer but we should find out . there 's a lot of different ways of computing the noise spectrum . not necessarily . cuz if you don't look into the future , but what does what does alcatel do ? and france telecom . y you 're talking about non stationary noise but that spectral subtraction is rarely is not gonna work really for non stationary noise , but it 's hard to we can talk about a couple other things briefly , you 're coming up with your quals proposal , y you want to talk c two or three minutes about what we 've been talking about today and other days ? there 's also this , echo cancellation that we 've been chasing , and when we 're saying these digits now we do have a close microphone signal and then there 's the distant microphone signal . and you could as a baseline say , "" given that we have both of these , we should be able to do , a cancellation . "" that , we , essentially identify the system in between the linear time invariant system between the microphones and re and invert it , or cancel it out to some reasonable approximation ","The Berkeley Meeting Recorder Group discussed the progress of several of their members. The group also touched upon matters that had broader implications for the work, such as the work of other groups on the same project. There were also some progress reports from group members working on other projects. "
Bro010.D,"yesterday morning on video conference . also we were thinking to , apply the spectral subtraction from ericsson and to change the contextual klt for lda . to change and use lda discriminative . ",
Bro010.E,"but i 'm , looking into extending the work done by larry saul and john allen and mazin rahim . they have a system that 's , a multi band , system but their multi band is a little different than the way that we 've been doing multi band in the past , where where we 've been @ @ taking sub band features and i training up these neural nets and on phonetic targets , and then combining them some somehow down the line , they 're taking sub band features and , training up a detector that detects for , these phonetic features and h he shows that using this method to detect sonorance is it 's very robust compared to , to typical , full band gaussian mixtures estimations of sonorance . that 's just one detector . you can imagine building many of these detectors on different features . you get enough of these detectors together , then you have enough information to do , higher level discrimination , discriminating between phones and then you keep working your way up until you build a full recognizer . that 's the direction which i 'm thinking about going in my quals . ",There were also some progress reports from group members working on other projects. 
Bro010.F,"we 're interested in , methods for far mike speech recognition , mainly , methods that deal with the reverberation in the far mike signal . one approach would be , say msg and plp , like was used in aurora one and , there are other approaches which actually attempt to remove the reverberation , instead of being robust to it like msg . and we 're interested in , comparing the performance of a robust approach like msg with these , speech enhancement or de reverber de reverberation approaches . ",There were also some progress reports from group members working on other projects. 
Bro011.A,"you guys had a meeting with with hynek which i unfortunately had to miss . everybody knows what happened except me . was there any conclusion about that ? g the key thing for me is figuring out how to better coordinate between the two sides was talking with hynek about it later and the had the sense that neither group of people wanted to bother the other group too much . but that you were waiting for them to tell you that they had something for you and they were waiting for you and we ended up with this thing where they were filling up all of the possible latency themselves , just talk more . let 's let 's , that as we said before that one of the things that we 're imagining is that there will be in the system we end up with there 'll be something to explicitly do something about noise in terms of think they were using ericsson 's approach suggest actually now we sorta move on and hear what 's what 's happening in other areas like what 's happening with your investigations about echos and on . and don't know if we 've talked lately about the plans you 're developing that we talked about this morning but that 's all that 's is a certainly relevant study mean , there 's these issues of what are the variables that you use and do you combine them using the soft "" and or "" or you do something , more complicated and why don't you tell us again about this database ? when was mark randolph there , or ? he 's he 's at motorola now . the only hesitation i had about it since , haven't see the data is it sounds like it 's continuous variables and a bunch of them . i don't know how complicated it is to go from there what you really want are these binary labels , and just a few of them . and there 's a trivial mapping if you wanna do it and it 's e but it i worry a little bit that this is a research project in itself , whereas if you did something instead that like having some manual annotation by linguistics students , this would there 'd be a limited s set of things that you could do a as per our discussions with john before course then , that 's the other question is do you want binary variables . what 's next ? your plan for you guys ' plan for the next week is just continue on these same things we 've been talking about for aurora and you have a big list of things to do . that 's good . ","This included a recap of a meeting with one of the members of their research partner OGI. The Berkeley Meeting Recorder Group met to discuss their recent progress. There were progress reports from group members working on echo cancellation, acoustic feature detection, and HTK optimization, along with discussion of many issues arising from this topics. "
Bro011.B,"they 're training up nets to try to recognize these acoustic features ? that people have done studies like that w way back can't remember where wisconsin or someplace that used to have a big database of researcher at a t andt a while back that was studying , trying to do speech recognition from these kinds of features . but i know remember it had to do with positional parameters and trying to m do speech recognition based on them . i could say a little bit about w 've been playing with . wanted to do this experiment to see what happens if we try to improve the performance of the back end recognizer for the aurora task and see how that affects things . and then com and then optimize the b htk system and run that again . look at the difference there and then do the same thing for the icsi ogi front end . 'm looking at the italian right now . one of the first things of was the fact that they use the same number of states for all of the models and just looked at , the number of phones in each one of the digits . and when i did that for the italian digits , i got a number of states , ranging on the low end from nine to the high end , eighteen . and their guess of eighteen states seems to be pretty matched to the two longest words of the italian digits , but for the most of the words are sh much shorter . the majority of them wanna have nine states . and theirs are s twice as long . and it turns out that the longest words are actually the ones that do the best . 'm going to try to create more word specific prototype h m ms to start training from . 'll , the next experiment i 'm gonna try is to just create models that seem to be more w matched to my guess about how long they should be . ","There were progress reports from group members working on echo cancellation, acoustic feature detection, and HTK optimization, along with discussion of many issues arising from this topics. "
Bro011.C,"first we discussed about some of the points that i was addressing in the mail i sent last week . about the the downsampling problem . and about the f the length of the filters try it "" . we should have a look first at , perhaps , the modulation spectrum . the i this idea of trying to find filters with shorter delays . we started to work with this . and the third point was the the on line normalization where , the recursion f recursion for the mean estimation is a filter with some delay for this , the conclusion of hynek was , "" we can try it but "" that was that 's all we discussed about . but there was also problem perhaps a problem of communication . now we will try to carmen is just looking at the ericsson code . is it the guy that was using the pattern of pressure on the tongue we can try to have some new baseline for next week perhaps . with all these minor things modified . ",This included a recap of a meeting with one of the members of their research partner OGI. 
Bro011.D,i modified it to take @ @ the first step the spectral subtraction . but we haven't result until this moment . but we are working in this also ,
Bro011.E,continuing to extend ,"There were progress reports from group members working on echo cancellation, acoustic feature detection, and HTK optimization, along with discussion of many issues arising from this topics. "
Bro011.F,"haven't started writing the test yet , i 'm meeting with adam today and he 's going t show me the scripts he has for running recognition on mee meeting recorder digits . i haven't asked hynek for the for his code yet . cuz i looked at avendano 's thesis and i don't really understand what he 's doing yet but it sounded like the channel normalization part of his thesis was done in a bit of i don't the word is , a bit of a rough way he it wasn't really fleshed out and he did something that was interesting for the test situation but i 'm not if it 's what i 'd wanna use i don't really understand what he 's doing yet . ","There were progress reports from group members working on echo cancellation, acoustic feature detection, and HTK optimization, along with discussion of many issues arising from this topics. "
Bro012.A,"had some interesting mail from dan ellis . where this came up was that was showing off these wave forms that we have on the web and just hadn't noticed this , but that the major , major component in the wave in the second wave form in that pair of wave forms is actually the air conditioner . i have to be more careful about using that as a as a good illustration , it 's not , of of the effects of room reverberation . it is isn't a bad illustration of the effects of room noise . on some mikes the other thing that i don't know the answer to , but when people are using feacalc here , whether they 're using it with the high pass filter option or not . when we 're doing all these things using our software there is and it 's pretty it 's not a very severe filter . doesn't affect speech frequencies , we want to go and check that in i for anything that we 're going to use the p d but if we do make use of the cheap mikes , we want to be to do that filtering before we process it . since i was talking about reverberation and showing this thing that was noise , it wasn't a good match , think we 'll change our picture on the web , when we 're @ @ . another , i was thinking of was taking some spectral slices , and look at the spectrum or cepstrum that you get out of there , all the recognizers look at frames . at one point in time or twenty over twenty milliseconds you have a spectrum or a cepstrum . but it 's not wer worse and it 's better latency , what 's what are according to the rules what are we supposed to do about the transition probabilities ? and that says that we could have lots more parameters actually . they do improvement in terms of accuracy ? rather than word error rate ? if you have ten percent error and you get five percent absolute improvement then that 's fifty percent . what you 're saying then is that if it 's something that has a small word error rate , then even a relatively small improvement on it , in absolute terms , will show up as quite large in this . but that 's it 's the notion of relative improvement . that 's why i 've been saying we should be looking at word error rate and not at accuracies . we probably should have standardized on that all the way through . but you 're but when you look at the numbers , your sense of the relative size of things is quite different . we are getting hurt somewhat . do the does the new filter make things better or worse for the other cases ? doesn't hurt , but doesn't get a little better , what i was asking , though , is are what 's the level of communication with the o g i gang now , about this is there any further discussion about this idea of having some source code control ? sounds like a great idea but that he 's saying people are scrambling for a eurospeech deadline . but that 'll be done in a week . after this next one . that you could certainly start looking at the issue but think it 's probably , on s from what stephane is saying , it 's unlikely to get active participation from the two sides until after they 've dave , the other thing , actually , is this business about this wave form . you and talk a little bit at some point about coming up with a better demonstration of the effects of reverberation for our web page , it made a good audio demonstration because when we could play that clip the really obvious difference is that you can hear two voices and in the second one and only hear but you can't when you play it back in a room with a big room , nobody can hear that difference really . but for the visual , just , i 'd like to have the spectrogram again , the other thing that we had in there that i didn't like was that the most obvious characteristic of the difference when you listen to it is that there 's a second voice , and the the cuts that we have there actually don't correspond to the full wave form . but it 's it 's the first six seconds of it and it 's in the seventh or eighth second where @ @ the second voice comes in . we would like to actually see the voice coming in , too , can i ask a , top level question , which is if most of what the ogi folk are working with is trying to integrate this other spectral subtraction , why are we worrying about it ? "" intellectually it 's interesting to work on things th one way or the other but i 'm just wondering if on the list of things that there are to do , if there are things that we won't do because we 've got two groups doing the same thing . just asking . do you remember when the next meeting is supposed to be ? the other thing is that you saw that mail about the vad v a ds performing quite differently ? this there was this experiment of what if we just take the baseline ? "" and you inc incorporate the different v a and it looks like the french vad is actually better significantly better . it was enough better that it would account for a fair amount of the difference between our performance , actually . if they have a better one , we should use it . ","There was also concern over overlap of work with partners OGI, and a lack of a good example of room reverberation for demonstrations. The Meeting Recorder group at Berkeley met to discuss recent progress. "
Bro012.A,"h hynek will be back in town the week after next , back in the country . and start organizing more visits and connections and forth , working towards june . no use of pitch we had a guy here some years ago who did some work on making use of voicing information to help in reducing the noise . what he was doing is you do estimate the pitch . or you estimate fine harmonic structure , i if there is strong harmonic structure , you can throw away that 's non harmonic . and that is another way of getting rid of part of the noise brings in a little more information than just spectral subtraction . and he had some he did that in combination with rasta . and got some decent results doing that . that 's another way . that 's a good idea . if you get if you go into harmonics related thing it 's definitely going to be different than what they 're doing should have some interesting properties in noise . ",
Bro012.B,"looked at looked at the results when stephane did that and it 's really wo really happens . th the only difference is you change the self loop transition probability by a tenth of a percent and it causes ten percent difference in the word error rate . and n not tenth of a percent , one tenth , it 's just very get stuck in some local minimum and this thing throws you out of it you 're not allowed to that 's supposed to be point six , for the self loop . but changing it to point five is which gives you much better results , but that 's not allowed . right . we only tested it on the medium mismatch , somebody , it was morgan , suggested at the last meeting that i actually count to see how many parameters and how many frames . and there are almost one point eight million frames of training data and less than forty thousand parameters in the baseline system . for all of the digit models , they end up at three mixtures per state . and just did a quick experiment , where i changed it it went to four and it didn't have a r any significant effect at the medium mismatch and high mismatch cases 'm r gonna run that again but with many more mixtures per state . but one thing i wanted to check out before i increased the number of mixtures per state was in their default training script they do an initial set of three re estimations and then they do seven iterations then the add mixtures and they do another seven then they add mixtures then they do a final set of seven and they quit . and it also makes the experiments go take a really long time and one of the things i did was i compiled htk for the linux machines cuz we have this one from ibm that 's got like five processors in it ? and now i 'm you can run on that and that really helps a lot because now we 've got extra machines that we can use for compute . and if i 'm do running an experiment right now where i 'm changing the number of iterations ? from seven to three ? just to see how it affects the baseline system . and if it 's not a huge difference from running with seven iterations , we should be able to get a lot more experiments done . and i 'll let what happens with that . but i could try to look into like this cvs over the web . that seems to be a very popular way of people distributing changes and over , multiple sites and things if figure out how do that easily and then pass the information on to everybody that it 's as easy to do as possible and people don't it won't interfere with their regular work , ask you some questions . ",
Bro012.C,"and if you 're interested in using cvs , i 've set it up here , i 'll be away tomorrow and monday but i 'll be back on tuesday or wednesday . ",
Bro012.D,"he actually , that he say with the good vad of from ogi also is stephane was thinking that it was useful to f to think about voiced unvoiced ",
Bro012.E,,
Bro012.F,"since the last meeting we 've tried to put together the clean low pass downsampling , upsampling , the new filter that 's replacing the lda filters , and then we have results that are not very impressive . there is no real improvement . it seems better when we look at the mismatched case but we are like cheated here by the th this problem that in some cases when you modify slight slightly modify the initial condition you end up completely somewhere air somewhere else in the space , the parameters . i don't think it means that the new system is more robust but from this se seventy eight percent recognition rate system , i could change the transition probabilities for the first and it will end up to eighty nine also . by using point five instead of point six , point four as in the htk script . but even if you use point five , i 'm not it will always give you the better results actually the way the final score is computed is quite funny . it 's not a mean of word error rate . it 's not a weighted mean of word error rate , it 's a weighted mean of improvements . which means that actually the weight on the matched is what happened is that if you have a small improvement or a small if on the matched case it will have huge influence on the improvement compared to the reference because the reference system is quite good for the ma matched case also . no , it 's compared to the word er it 's improvement on the word error rate , anyway it hurts a little bit on the match no , point six percent absolute on italian we start from ninety four point sixty four , and we go to ninety four point o four . for finnish , we start to ninety three point eight four and we go to ninety three point seventy four . and for spanish we are we were at ninety five point o five and we go to ninety three s point sixty one . but the filter the filter with the shorter delay hurts on italian matched , we are exchanging mail as soon as we have significant results . for the moment , they are working on integrating the spectral subtraction from ericsson . we are working on our side on other things like also trying a sup spectral subtraction but of our own , another spectral substraction . for the moment they 're there is this eurospeech deadline , i start we started to work on spectral subtraction . and the preliminary results were very bad . the thing that we did is just to add spectral subtraction before this , the wall process , which contains lda on line normalization . and it hurts lot . it 's just it 's another they are trying to u to use the the ericsson and we 're trying to use something else . and what they did at ogi is just they don't use on line normalization , for the moment , on spectral subtraction as soon as they will try on line normalization there will be a problem . we 're working on the same thing but with different system it 's in june . but i don't know which vad they use . it 's pratibha that did this experiment . we should ask which vad she used . my feeling is that actually when we look the proposals , ev everybody is still using some spectral envelope but to look at the fine at the high re high resolution spectrum . cuz i have a feeling that when we look at the just at the envelope there is no way you can tell if it 's voiced and unvoiced , think if we try to develop a second stream there would be one stream that is the envelope and the second , it could be interesting to have that 's something that 's more related to the fine structure of the spectrum . voiced speech detector , that 's working on the fft larry saul could be an idea . we were are thinking about just taking the spectrum and computing the variance of the high resolution spectrum and things like this . because , spectral subtraction is good and we could u we could use the fine structure to have a better estimate of the noise but still there is this issue with spectral subtraction that it seems to increase the variance of it 's this musical noise which is annoying if you d you do some on line normalization after . spectral subtraction and on line normalization don't seem to go together very ",Of greatest interest was the progress on improving the latency and performance of their recogniser. The Meeting Recorder group at Berkeley met to discuss recent progress. 
Bro013.A,"what are we talking about today ? the both the the sri system and the oth and for one thing that shows the difference between having a lot of training data or not , the the best number we have on the english on near microphone only is three or four percent . and it 's significantly better than that , using fairly simple front ends on the with the sri system . but that 's using pretty huge amount of data , mostly not digits for the actual training the h m ms whereas in this case we 're just using digits for training the h m but what 'd be interested to do given that , is that we should take is to take some of these tandem things and feed it into the sri system , to there 's there 's two things being affected . one is that , there 's something simple that 's wrong with the back end . we 've been playing a number of states don't know if he got to the point of playing with the number of gaussians yet but , far he hadn't gotten any big improvement , but that 's all with the same amount of data which is pretty small . you could do that , but i 'm saying even with it not with that part not retrained , just using having the h m ms much better h m what is the problem that you 're trying to explain ? i but i 'm almost certain that it that it has to do with the amount of training data . it 's orders of magnitude off . let 's see , in the in these multi train things did we include noisy data in the training ? that could be hurting us actually , for the clean case . i don't think there 's anything magical here . it 's , we used a simple htk system with a modest amount of data . and this is a , modern system has a lot of points to it . the htk is an older htk , even . but to me it just meant a practical point that if we want to publish results on digits that people pay attention to we probably should cuz we 've had the problem before that you get show some improvement on something that 's it seems like too large a number , and people don't necessarily take it there 's even though it 's close miked there 's still there really is background noise . and suspect when the ti digits were recorded if somebody fumbled or said something wrong that they probably made them take it over . it was not there was no attempt to have it be realistic in any sense wha where did this good vad come from ? this is the one they had originally ? but they had to get rid of it because of the space , but the other thing is to use a different vad entirely . i don't the thinking was amongst the etsi folk but if everybody let 's use this vad and take that out of there but i don't think we need to be stuck on using our or ogi 's vad . we could use somebody else 's if it 's smaller as long as it did the job . you didn't gain anything , right ? now i wonder i know you want to get at something orthogonal from what you get with the smooth spectrum but if you were to really try and get a voiced unvoiced , do you want to ignore that ? clearly a very big cues for voiced unvoiced come from spectral slope and on , certainly if you want to do good voiced unvoiced detection , you need a few features . each feature is by itself not enough . but , people look at slope and first auto correlation coefficient , divided by power . or you could you just do it going through the p fft 's figuring out some probable harmonic structure . what i was talking about was just , starting with the fft you could do a very rough thing to estimate pitch . and given given that , you could come up with some estimate of how much of the low frequency energy was explained by those harmonics . it 's variant on what you 're s what you 're doing . but as you say it 's not that smooth here . and if you just subtracted off your guess of the harmonics then something like this would end up with quite a bit lower energy in the first fifteen hundred hertz or what 's up with you ? people will do what they say . in digit recognition we 've done before , you have two pronunciations for that value , "" o "" and "" zero "" . no , they just write and you just they just want people to read the digits as you ordinarily would ","The Main purpose of the meeting of ICSI's Meeting Recorder Group at Berkeley was to discuss the recent progress of it's members. There was also talk of comparing different recognition systems and training datasets, and a discussion of the pronunciation of the digit zero for the recording at the end of the meeting. This includes reports on the progress of the groups main digit recogniser project, with interest on voice-activity detectors and voiced/unvoiced detection, work on acoustic feature detection, and research into dealing with reverberation. "
Bro013.B,"our t i went to talk with mike jordan this week and shared with him the ideas about extending the larry saul work and asked him some questions about factorial h m like later down the line when we 've come up with these feature detectors , how do we model the time series that happens and we talked a little bit about factorial h m ms and how when you 're doing inference or w when you 're doing recognition , there 's like simple viterbi that you can do for these h m and the the great advantages that lot of times the factorial h m ms don't don't over alert the problem there they have a limited number of parameters and they focus directly on the sub problems at hand he seemed like really interested in in this and said this is something very do able and can learn a lot i 've just been continue reading about certain things . thinking of using modulation spectrum to as features also in the sub bands because it seems like the modulation spectrum tells you a lot about the intelligibility of certain words and ",
Bro013.C,"'ve been looking at avendano 's work 'll try to write up in my next stat status report a description of what he 's doing , but it 's an approach to deal with reverberation or that the aspect of his work that i 'm interested in s 'm my first stab actually in continuing his work is to re implement this thing which changes the time and frequency resolutions cuz he doesn't have code for me . that 'll take some reading about the theory . i don't really know the theory . the way i want to extend his work is make it able to deal with a time varying reverberation response we don't really know how fast the the reverberation response is varying the meeting recorder data y you do you read some of the zeros as o 's and some as zeros . is there a particular way we 're supposed to read them ? ","This includes reports on the progress of the groups main digit recogniser project, with interest on voice-activity detectors and voiced/unvoiced detection, work on acoustic feature detection, and research into dealing with reverberation. There was also talk of comparing different recognition systems and training datasets, and a discussion of the pronunciation of the digit zero for the recording at the end of the meeting. "
Bro013.D,"no , i w i begin to play with matlab and to found some parameter robust for voiced unvoiced decision . and we they we found that is a classical parameter , the sq the variance between the fft of the signal and the small spectrum of time we after the mel filter bank . and , is more or less robust . is good for clean speech . because when did noise clear in these section is clear ","This includes reports on the progress of the groups main digit recogniser project, with interest on voice-activity detectors and voiced/unvoiced detection, work on acoustic feature detection, and research into dealing with reverberation. "
Bro013.E,"first there are perhaps these meeting recorder digits that we tested . because it 's their very d huge , their huge system . but the main point is the data because our back end is fairly simple but until now , the attempts to improve it or have fail we could retrain some of these tandem on huge perhaps it 's not related , the amount of data but the recording conditions . the fact that the result with the tandem and aurora system are much worse . ti digit is it 's very , very clean and it 's like studio recording whereas these meeting recorder digits sometimes you have breath noise perhaps that we 've been working on is , we have put the the good vad in the system and it really makes a huge difference . this is perhaps one of the reason why our system was not the best , because with the new vad , it 's very the results are similar to the france telecom results and perhaps even better sometimes . the problem is that it 's very big and we still have to think how to where to put it either some delay and we if we put it on the server side , it doesn't work , because on the server side features you already have lda applied from the f from the terminal side and you accumulate the delay it 's from ogi . but the abso assumption is that we will be able to make a vad that 's small and that works fine . they just want , they don't want to fix the vad because they think there is some interaction between feature extraction and vad or frame dropping but they still want to just to give some requirement for this vad because it 's it will not be part of they don't want it to be part of the standard . there just will be some requirements that are still not not yet ready designed a new filter because when i designed other filters with shorter delay from the lda filters , there was one filter with fif sixty millisecond delay and the other with ten milliseconds and hynek suggested that both could have sixty five sixty s both should have sixty five because and it 's running . but the filter is closer to the reference filter . and then we 've started to work with this of voiced unvoiced we wa want to look at something like the ex excitation signal and which are the variance of it and this would be perhaps an additional parameter , it 's another problem . if you look at this spectrum , is it the mel filters ? and what we clearly see is that in some cases , and the harmonics are resolved by the f there are still appear after mel filtering , and it happens for high pitched voice because the width of the lower frequency mel filters is sometimes even smaller than the pitch . we were thinking to modify the mel spectrum to have something that 's smoother on low frequencies . perhaps in the sheets there should be another sign for the and people pronounce "" o "" or zero ","The Main purpose of the meeting of ICSI's Meeting Recorder Group at Berkeley was to discuss the recent progress of it's members. This includes reports on the progress of the groups main digit recogniser project, with interest on voice-activity detectors and voiced/unvoiced detection, work on acoustic feature detection, and research into dealing with reverberation. There was also talk of comparing different recognition systems and training datasets, and a discussion of the pronunciation of the digit zero for the recording at the end of the meeting. "
Bro014.A,"there was a conference call this tuesday . i don't know yet the what happened tuesday , but the points that they were supposed to discuss is still , things like the weights , i have no idea . the points were the weights how to weight the different error rates that are obtained from different language and conditions . it 's not clear that they will keep the same weighting . some people are arguing that it would be better to have weights on to combine error rates before computing improvement . and perhaps they will change the weights to but there is this still this problem of weights . when you combine error rate it tends to give more importance to the difficult cases , some people think that it 's more important to look at to have ten percent imp relative improvement on matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch but there is probably a big change that will be made is that the baseline th they want to have a new baseline , perhaps , and some people are pushing to still keep this fifty percent number . they want to have at least fifty percent improvement on the baseline , they didn't decide yet . what happened since , last week is from ogi , these experiments on putting vad on the baseline . and these experiments also are using , some noise compensation , spectral subtraction , and putting on line normalization , just after this . think spectral subtraction , lda filtering , and on line normalization , which is similar to the pro proposal one , but with spectral subtraction in addition , and it seems that on line normalization doesn't help further when you have spectral subtraction . and what 's happened here is that we we have this new , reference system which use a clean downsampling upsampling , which use a new filter that 's much shorter and which also cuts the frequency below sixty four hertz , no . because we 're still testing . it seems to improve on the matched case , but it 's a little bit worse on the mismatch and highly mismatched latency is short we try to , find good features that could be used for voicing detection , we w we are still playing with matlab to look at what happened , we would be looking at , the variance of the spectrum of the excitation , which is should be high for voiced sounds . that 's right . right now we just are trying to find some features . hopefully , what we want to have is to put these features in s some to obtain a statistical model on these features and to or just to use a neural network it seems ,  if you take this frame , from the noisy utterance and the same frame from the clean utterance we 'll perhaps try to convince ogi people to use the new filters not yet but i wi i will call them now they are they have more time eurospeech deadline is over ","They discussed a conference call with project partners, there have been some developments that should help speed up experiments, along with some progress made in the current area they are looking, voiced/unvoiced detection. The ICSI Meeting Recorder Group at Berkeley met to discuss progress on their main project, Aurora. "
Bro014.B,"and then when you have your final thing , do a full one , it 's you could do something like keep exactly the same procedure and then add a fifth thing onto it do who was since we weren't in on it , do who was in from ogi ? the fact that it 's inconsistent is an obvious mistake . but the question is , do you average the relative improvements or do you average the error rates and take the relative improvement of that ? and it 's not just a pure average because there are these weightings . it 's just when you get all done , that they pro but they started off this process with the notion that you should be significantly better than the previous standard . they said "" how much is significantly better ? and they said "" you should have half the errors , "" "" that you had before "" . but it does seem like i it does seem like it 's more logical to combine them first they don they don't really know , the argument for that being the more important thing , is that you 're gonna try and do that , but you wanna see how badly it deviates from that when the , it 's a little different . the opposite argument is you 're never really gonna have a good sample of all these different things . i gather that in these meetings it 's really tricky to make anything ac make any policy change because everybody has , their own opinion whose vad when you say "" we have that "" , does sunil have it now , too , but it has the , the latencies are much shorter . there was a start of some effort on something related to voicing what yo what you 're calling the excitation , as i recall , is you 're subtracting the , the mel filter , spectrum from the fft spectrum . it 's not really an excitation , but it 's something that hopefully tells you something about the excitation . another way of looking at it is what characterization of the difference between the raw data and this smooth version is something that you 're missing that could help ? looking at different statistical measures of that difference , and seeing if you add them onto the feature vector does that make things better or worse in noise , the way i 'm looking at it is not much you 're trying to f find the best the world 's best voiced unvoiced , classifier , but it 's more that , try some different statistical characterizations of that difference back to the raw data that the data itself is that you 're working with is not perfect . what i 'm saying is that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced unvoiced certainly , and , and you 're not getting the excitation anyway , and this is saying , if you really do have that vocal tract envelope , and you subtract that off , what you get is the excitation . and i call that lies because you don't really have that , you just have some signal processing trickery to get something that 's smooth . you 're not really getting the vocal excitation . that 's why i was going to the why i was referring to it in a more , conservative way , when i was saying "" it 's it 's the excitation "" . but it 's not really the excitation . you don't really get the excitation , but you get something related to it . has anything happened yet on this business of having some standard , source , and he 's been doing all the talking this is this bad thing . we 're trying to get , m more female voices in this record as make sur make carmen talks as anything to add ? ","They discussed a conference call with project partners, there have been some developments that should help speed up experiments, along with some progress made in the current area they are looking, voiced/unvoiced detection. A number of other members of the group also reported the progress they were making on their work. "
Bro014.C,"i have something just fairly brief to report on . what i was started playing with was the th again , this is the htk back end . i was curious because the way that they train up the models , they go through about four rounds of training . that 's part of what takes long to train the back end for this . the first one is three , then seven , and seven . and what these numbers refer to is the number of times that the , re estimation is run . i wanted to first test to see if we actually need to do this many iterations early on . i ran a couple of experiments where i reduced that to l to be three , two , five , and i got almost the exact same results . and but it runs much faster . it only took something like , three or four hours to do the full training , as opposed to wh what , sixteen hours like that ? even we don't do anything else , doing something like this could allow us to turn experiments around a lot faster . and when you have your final thing , we go back to this . it 's like one little text file you edit and change those numbers , we could do a lot more experiments and throw a lot more in there . the other thing that i did was , i compiled the htk for the linux boxes . we have this big thing that we got from ibm , which is a five processor machine . really fast , but it 's running linux . you can now run your experiments on that machine i 've forgotten now what the name of that machine is but send email around about it . you have to make that in your dot cshrc , it detects whether you 're running on the linux or a sparc and points to the right executables . and you may not have had that in your dot cshrc before , if you were always just running the sparc . tell you exactly what you need to do to get all of that to work . after i did that , then what i wanted to do was try increasing the number of mixtures , just to see , see how that affects performance . it sounds like they don't really have a good idea about what the final application is gonna be . what sorts of features are you looking at ? really that 's cartoon picture about what 's voiced and unvoiced . and you said this is pretty doing this thing is pretty robust to noise ? you end up with a similar difference ","They discussed a conference call with project partners, there have been some developments that should help speed up experiments, along with some progress made in the current area they are looking, voiced/unvoiced detection. "
Bro014.D,"n not much is new . when i talked about what i 'm planning to do last time , i said i was , going to use avendano 's method of , using a transformation , to map from long analysis frames which are used for removing reverberation to short analysis frames for feature calculation . but , i decided not to do that after all because i realized to use it i 'd need to have these short analysis frames get plugged directly into the feature computation somehow and right now our feature computation is set to up to , take , audio as input , in general . decided that i 'll do the reverberation removal on the long analysis windows and then just re synthesize audio ",A number of other members of the group also reported the progress they were making on their work. 
Bro014.E,"i 've been continuing reading . i went off on a little tangent this past week , looking at , modulation s spectrum and learning a bit about what , what it is , and i found some , neat papers , historical papers from , kanedera , hermansky , and arai . and they did a lot of experiments where th where , they take speech and , e they modify the , they measure the relative importance of having different , portions of the modulation spectrum intact . and they find that the spectrum between one and sixteen hertz in the modulation is , is im important for speech recognition . i was thinking more like using them as the inputs to the detectors . ",A number of other members of the group also reported the progress they were making on their work. 
Bro014.F,i don't know . that for the recognizer for the meeting recorder that it 's better that i don't speak . ,
Bro015.A,,
Bro015.B,,
Bro015.C,,
Bro015.D,,
Bro015.E,,
Bro015.F,,
Bro016.A,"continuing looking at , ph phonetic events , and , this tuesday gonna be , meeting with john ohala with chuck to talk some more about these , ph phonetic events . came up with , a plan of attack , it 's that 's it . i was thinking getting us a set of acoustic events to to be able to distinguish between , phones and words and we would figure out a set of these events that can be , hand labeled or derived , from h the hand labeled phone targets . we could take these events and , do some cheating experiments , where we feed , these events into an sri system , and evaluate its performance on a switchboard task . he in this paper , it 's talking about phoneme recognition using acoustic events . things like frication or , nasality . this is a paper by hubener and cardson benson bernds berndsen . ","Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. These included making plans for upcoming experiments, clarifying definitions, and approaches which may or may not be against the rules of the Aurora project, alongside alternatives that would not be. "
Bro016.B,"there 's nothing new . we 've been mainly working on the report on the report of the work that was already done . y we 've stopped , experimenting , we 're just writing some technical report .   actually , there were some tables that were also with partial results . we just noticed that , wh while gathering the result that for some conditions we didn't have everything . we have , extracted actually the noises from the speechdat car . we can train neural network with speech and these noises . it 's difficult to say what it will give , this is something we have to try anyway . adding the noises from the speechdat car .  ogi does did that . at some point they did that for the voice activity detector . and spanish , different cars . there is also the spectral subtraction , we should , try to integrate it in our system . that would involve to use a big a al already a big bunch of the system of ericsson . because he has spectral subtraction , then it 's followed by , other processing that 's are dependent on the if it 's speech or noi or silence . and s it 's important , to reduce this musical noise and this increase of variance during silence portions . this was in this would involve to take almost everything from the this proposal and then just add some on line normalization in the neural network . ","Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. "
Bro016.C,"i don't have results yet . we work in the report , too , because we have a lot of result , they are very dispersed , and was necessary to look in all the directory to give some more structure . for icsi . just summary of the experiment and the conclusion first of july . ","Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. "
Bro016.D,i took a lot of time just getting my taxes out of the way i 'm starting to write code now for my work but i don't have any results yet . can you give an example of an event ? ,"Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. These included making plans for upcoming experiments, clarifying definitions, and approaches which may or may not be against the rules of the Aurora project, alongside alternatives that would not be. "
Bro016.E,"i was saying hynek 'll be here next week , wednesday through friday through saturday , i won't be here thursday and friday . but my suggestion is that , at least for this meeting , people should go ahead , cuz hynek will be here , do you think that would be the case for next week also ? what 's your projection on ? cuz the one thing that seems to me we really should try , if you hadn't tried it before , because it hadn't occurred to me is , adjusting the , sca the scaling and , insertion penalty sorta but you were looking at mel cepstrum . i it 's not the direction that you were working with that we were saying what 's the what 's the best you can do with mel cepstrum . the next question to ask , which is the one that andreas was dre addressing himself to in the lunch meeting , is , we 're not supposed to adjust the back end , but anybody using the system would . if you were just adjusting the back end , how much better would you do , in noise ? because the language scaling and insertion penalties and forth are probably set to be about right for mel cepstrum . but , they 're probably not set right for these things , by "" our front end "" take , the aurora two s take some version that stephane has that is , our current best version of something . how much , does it improve if you actually adjust that ? when you adjusted those numbers for mel cepstrum , did it ? but it would be it 'd be good to know that . part of what 's going on , is the , the range of values . if you have something that has a much smaller range or a much larger range , and taking the appropriate root . if something is like the equivalent of a bunch of probabilities multiplied together , you can take a root of some sort . because it changes the scale of the numbers of the differences between different candidates from the acoustic model as opposed to what 's coming from the language model . it 's more directly like the language scaling or the , the model scaling or acoustic scaling , if you 're operating in the wrong range that 's why just in general , if you change what these penalties and scaling factors are , you reach some point that 's a minimum . we do have to do over a range of different conditions , some of which are noisier than others . but , we may get a better handle on that if we see it 's if we actually could pick a more stable value for the range of these features , it , could even though it 's it 's true that in a real situation you can adjust the these scaling factors in the back end , and it 's ar artificial here that we 're not adjusting those , you certainly don't wanna be adjusting those all the time . and if you have a front end that 's in roughly the right range i remember after we got our more or less together in the previous systems we built , that we tended to set those scaling factors at standard level , and we would rarely adjust them again , as for these other things , it may turn out that , it 's reasonable . andreas gave a very reasonable response , and he 's probably not gonna be the only one who 's gonna say this in the future people within this tight knit community who are doing this evaluation are accepting , more or less , that these are the rules . but , people outside of it who look in at the broader picture are certainly gonna say "" minute . you 're doing all this standing on your head , on the front end , when all you could do is just adjust this in the back end with one s one knob . "" we have to at least , determine that 's not true , and as you say as you point out finding ways to then compensate for that in the front end also then becomes a priority for this particular test , what 's new with you ? what 's old with you that has developed over the last week or two ? what wha wh wha what 's going on ? i if summarize , what 's going on is that you 're going over a lot of material that you have generated in furious fashion , f generating many results and doing many experiments and trying to pull it together into some coherent form to be able to see wha see what happens . my suggestion , though , is that you not necessarily finish that . but that you put it all together that it 's you 've got a clearer structure to it . what things are , you have things documented , you 've looked things up that you needed to look up . that , that such a thing can be written . when do you leave again ? and that you figure on actually finishing it in june . because , you 're gonna have another bunch of results to fit in there anyway . and right now it 's important that we actually go forward with experiments . it 's good to pause , and to gather everything together and make it 's in good shape , that other people can get access to it and that it can go into a report in june . but to really work on fine tuning the report n at this point is probably bad timing , that 's permitted ? ","Plans were also made with regard to a visitor from research partner OGI These included making plans for upcoming experiments, clarifying definitions, and approaches which may or may not be against the rules of the Aurora project, alongside alternatives that would not be. Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. There was also debate about the necessary continuation of a group report. "
Bro016.E,"the rules as i understand it , is that in principle the italian and the spanish and the english italian and the finnish and the english ? were development data on which you could adjust things . and the german and danish were the evaluation data . and then when they finally actually evaluated things they used everything . it doesn't appear that there 's strong evidence that even though things were somewhat tuned on those three or four languages , that going to a different language really hurt you . and the noises were not exactly the same . they were different drives . it 's tuned more than , a a you 'd really like to have something that needed no particular noise but that 's not really what this contest is . that 's something i 'd like to understand before we actually use something from it , except that , that 's what we used in aurora one , and then they designed the things for aurora two knowing that we were doing that . no . but , that it it probably would be the case that if , say , we trained on italian , data and then , we tested on danish data and it did terribly , that it would look bad . and someone would notice that 's topic especially if you talk with him when i 'm not here , that 's a topic you should discuss with hynek to , double check it 's this 'll be , something for discussion with hynek next week . how are , how are things going with what you 're doing ? do you wanna say something about your here ? ","These included making plans for upcoming experiments, clarifying definitions, and approaches which may or may not be against the rules of the Aurora project, alongside alternatives that would not be. Plans were also made with regard to a visitor from research partner OGI Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. "
Bro016.F,"i don't really have , anything new . been working on meeting recorder i did play with that , actually , a little bit . what happens is , when you get to the noisy you start getting lots of insertions . 've tried playing around a little bit with , the insertion penalties and things like that . it didn't make a whole lot of difference . like for the matched case , it seemed like it was pretty good . i could do more playing with that , though . you 're talking about for th for our features . the experiment is to , run our front end like normal , with the default , insertion penalties and forth , and then tweak that a little bit and see how much of a difference it makes i don't remember off the top of my head . i didn't even write them down . looking at the i wrote down what the deletions , substitutions , and insertions were , for different numbers of states per phone . but , that 's all i wrote down . i would need to do that . do that for next week . have that for next week when hynek 's here . i wonder if there 's anything that we could do to the front end that would affect the insertion in effect , that 's changing the value of your insertion penalty . if we the insertion penalty is , then we can get an idea about what range our number should be in , any anything new on the thing that , you were working on with the , are you discovering anything , that makes you scratch your head as you write this report , like why did we do that , or why didn't we do this , it 's probably something that , the the , experiment designers didn't really think about , because most people aren't doing trained systems , or , systems that are like ours , where you actually use the data to build models . they just doing signal processing . and they didn't forbid us to build models on the data ? just to expand a little bit on the idea of acoustic event . there 's , in my mind , anyways , there 's a difference between , acoustic features and acoustic events . and of acoustic features as being , things that linguists talk about , that 's not based on data , necessarily . that 's not based on , acoustic data . they talk about features for phones , which may or may not be all that easy to measure in the acoustic signal . versus an acoustic event , which is just some something in the acoustic signal that is fairly easy to measure . it 's kinda like the difference between top down and bottom up . of the acoustic phonetic features as being top down . you look at the phone and you say this phone is supposed to be have this feature , and this feature . whether tha those features show up in the acoustic signal is irrelevant . whereas , an acoustic event goes the other way . here 's the signal . here 's some event . and then that that may map to this phone sometimes , and it 's different way of looking . ","Although the members of ICSI's Meeting Recorder Group at Berkeley had little progress to report, there were still a number of issues relating to their work to discuss. These included making plans for upcoming experiments, clarifying definitions, and approaches which may or may not be against the rules of the Aurora project, alongside alternatives that would not be. Plans were also made with regard to a visitor from research partner OGI "
Bro017.A,,
Bro017.B,"and that carmen and stephane reported on amsterdam meeting , which was interesting because it was for the first time we realized we are not friends really , but we are competitors . there is a plenty of there 're plenty of issues . and what happened was that they realized that if two leading proposals , which was french telecom alcatel , and us both had voice activity detector . and i said "" big surprise , we could have told you that n four months ago , except we didn't because nobody else was bringing it up "" . french telecom didn't volunteer this information either , cuz we were working on mainly on voice activity detector for past several months and everybody said "" but this is not fair . we didn't know that . "" and the it 's not working on features really . and then ev everybody else says "" we should we need to do a new eval evaluation without voice activity detector , or we have to do something about it "" . and in principle i we but in that case , we would like to change the the algorithm because if we are working on different data , we probably will use a different set of tricks . but unfortunately nobody ever officially can somehow acknowledge that this can be done , because french telecom was saying "" no , no , now everybody has access to our code , everybody is going to copy what we did . "" our argument was everybody ha has access to our code , and everybody always had access to our code . we never denied that . we thought that people are honest , that if you copy something and if it is protected by patent then you negotiate , but and french telecom was saying "" no , no , there is a lot of little tricks which cannot be protected and you guys will take them , "" which probably is also true . and they have to be honest in the long run , because winning proposal again what will be available th is will be a code . the the people can go to code and say "" listen this is what you stole from me "" the biggest problem is that f that alcatel french telecom cl claims "" we fulfilled the conditions . we are the best . and e and other people don't feel that , because they they now decided that is the whole thing will be done on endpointed data , still not clear if we are going to run the if we are allowed to run new algorithms , because we would fight for that , really . at least our experience is that only endpointing a mel cepstrum gets gets you twenty one percent improvement overall and twenty seven improvement on speechdat car then obvious the database mean the the baseline will go up . and nobody can then achieve fifty percent improvement . they that there will be a twenty five percent improvement required on u m bad mis badly mismatched and now they want to say "" we will require fifty percent improvement only for matched condition , and only twenty five percent for the serial cases . "" and and they almost on that except that it wasn't a hundred percent and last time during the meeting , brought up the issue , for two years we are fighting for fifty percent improvement and suddenly you are saying "" no we will do something less "" , and everybody said "" we discussed that and you were not a mee there "" and i said "" lot of other people were not there because not everybody participates at these teleconferencing c things . "" then they said "" no because everybody is invited . "" however , there is only ten or fifteen lines , people can't even con participate . they and they said "" we will discuss that . "" now officially , nokia is complaining and said they are looking for support , think qualcomm is saying , too "" we shouldn't abandon the fifty percent yet . we should at least try once again , one more round . "" what we are doing at ogi now is working on our parts which we little bit neglected , like noise separation . we are looking in ways is in which with which we can provide better initial estimate of the mel spectrum which would be a l f more robust to noise , and far not much success . and then most of the effort is now also aimed at this e trap recognition . this this is this recognition from temporal patterns . but , what it is , is that normally what you do is that you recognize speech based on a shortened spectrum . essentially l p lpc , mel cepstrum , everything starts with a spectral slice . if you s given the spectrogram you essentially are sliding the spectrogram along the frequency axis and you keep shifting this thing , and you have a spectrogram . you can say "" you can also take the time trajectory of the energy at a given frequency "" , and what you get is then , that you get a p vector . namely you can say i it i will say that this vector will will describe the phoneme which is in the center of the vector . it 's a very different vector , very different properties , we were just discussing , since you mentioned that , in it w driving in the car with morgan this morning , we were discussing a good experiment for b for beginning graduate student who wants to run a lot of who wants to get a lot of numbers on something ","He reported on a recent project meeting from his group's perspective. There was much politics involved, and disagreement between groups. He also brought the ICSI members up to date with his group's latest work. Having discussed this with the ICSI project leader, the OGI member told of some future investigation they had devised, which would look at the adjusting the importance of some features. "
Bro017.B,"which is imagine that you will start putting every co any coefficient , which you are using in your vector , in some general power . like you take a s power of two , or take a square root , because your gaussian mixture model , you 're compressing the range of this coefficient , it 's becoming less efficient . morgan was @ @ and he was saying this might be the alternative way how to play with a fudge factor ,  and i said "" in that case why don't we just start compressing individual elements , like when because we observed that higher parameters were more important than lower for recognition . and the c ze c one contributes mainly slope , when we talked about aurora still i wanted to m make a plea encourage for more communication between different parts of the distributed center . even when there is nothing to s to say but the weather is good in ore in berkeley . i 'm that it 's being appreciated in oregon and it will generate similar responses down here , no . and then we also can send the dis to the same address and it goes to everybody ","Having discussed this with the ICSI project leader, the OGI member told of some future investigation they had devised, which would look at the adjusting the importance of some features. There were also further calls for greater communication between the groups. "
Bro017.C,"finally we 've not finished with this . we have a document that explain a big part of the experiments , we 've fff done some strange things like removing c zero or c one from the vector of parameters , and we noticed that c one is almost not useful you can remove it from the vector , it doesn't hurt . no . we don't have . ",
Bro017.D,"more or less it 's finished . ma nec to need a little more time to improve the english , and to fill in something some small detail , something like that , ",
Bro017.E,"it seemed like there were still some issues , that they were trying to decide ? like the voice activity detector , how 's your documentation or whatever it w what was it you guys were working on last week ? have you been running some new experiments ? if we mail to "" aurora inhouse "" , does that go up to you guys also ? do we have a mailing list that includes the ogi people ? we should set that up . that would make it much easier . ","There was much politics involved, and disagreement between groups. The ICSI group reported their most recent progress and detailed their recent findings. "
Bro019.A,"the other thing what i tried was , took the baseline and then ran it with the endpoint inf th information , just the aurora baseline , to see that how much the baseline itself improves by just supplying the information of the the w speech and nonspeech . i found that the baseline itself improves by twenty two percent by just giving the wuh . because the second the new phase is going to be with the endpointed speech . and just to get a feel of how much the baseline itself is going to change by adding this endpoint information , use  with what other new p new parameter ? as using just the cepstrum , ","He began the meeting by reporting his recent activities, which included looking at the new baseline system. They also explained some of their projects to their guest. "
Bro019.B,"i don't have good result , with the inc including the new parameters , i don't have good result . i tried to include another new parameter to the traditional parameter , that the auto correlation , the r zero and r one over r zero and another estimation of the var the variance of the difference for of the spec si spectrum of the signal and the spectrum of time after filt mel filter bank . the idea is to found another feature for discriminate between voice sound and unvoice sound . and we try to use this new feature . i do i did two type of experiment to include this feature directly with the other feature n with the neural network i have more or less the same result . sometime it 's worse , sometime it 's a little bit better ,  will try to do that . ",The other members of the group also reported their recent progress in areas such as spectral subtraction and voicing detection. They also explained some of their projects to their guest. 
Bro019.C,"sunil 's here for the summer , and then just progress reports individually , and then plans for where we go between now and then , how about an email that points to the faq , sunil since you 're haven't been at one of these yet , why don't yo you tell us what 's up with you ? just briefly , you could remind us about the related experiments . cuz you did some that you talked about last week , both of you were both combining something from the french telecom system with the u anything else going on ? you probably need to back up a bit we 've had these discussions before , and one of the things that struck me was that about this line of thought that was particularly interesting to me was that we whenever you condense things , in an irreversible way , you throw away some information . and the question is , can we figure out if there 's something we 've thrown away that we shouldn't have . when they were looking at the difference between the filter bank and the fft that was going into the filter bank , i was thinking "" they 're picking on something they 're looking on it to figure out noise , or voice voiced property whatever . "" but for me the interesting thing was , "" but is there just something in that difference which is useful ? "" another way of doing it , would be just to take the fft power spectrum , and feed it into a neural network , and , if it 's used in combination , it will get at something that we 're missing . it 's just a thought . anything on your end you want to talk about ? is that something worth talking about , ","The ICSI Meeting Recorder Group at Berkley have a temporary new member on loan from research partner OGI. The other members of the group also reported their recent progress in areas such as spectral subtraction and voicing detection. The group shall soon be taking delivery of more machines for a computation farm, and they discussed some software tools for running large processes. He began the meeting by reporting his recent activities, which included looking at the new baseline system. They also explained some of their projects to their guest. "
Bro019.D,"the main thing that we did is just to take the spectral subtraction from the france telecom , we just tried directly to just , keep the system as it was when we plug the spectral subtraction it improves signif significantly . you can remove on line normalization , or put it , it doesn't change anything . as long as you have the spectral subtraction . and , right now if we look at the results , it 's , always better than it seems always better than france telecom for mismatch and high mismatch . and it 's still slightly worse for matched . but this is not significant . we are playing we are also playing , trying to put other spectral subtraction in the code . it would be a very simple spectral subtraction , on the mel energies we we find some people that agree to work with us , and they have implementation of vts techniques it 's vector taylor series that are used to to model the transformation between clean cepstra and noisy cepstra . it can be used for getting rid of the noise and the channel effect . ",The other members of the group also reported their recent progress in areas such as spectral subtraction and voicing detection. 
Bro019.E,"it i don't think it directly relates . i was helping a speech researcher named pierre divenyi and he 's int he wanted to look at how people respond to formant changes , he created a lot of synthetic audio files of vowel to vowel transitions , and then he wanted a psycho acoustic spectrum . and he wanted to look at how the energy is moving over time in that spectrum and compare that to the listener tests . ",
Bro019.F,"i could say a few words about some of the compute that 's happening around here , that people in the group know . we just put in an order for about twelve new machines , to use as compute farm . and andreas has gotten that all fixed up and up to speed . and he 's got a number of little utilities that make it very easy to run things using p make and customs . and send an email around or , should do an faq on the web site about it and , if you say that and then some job that you want to execute , it will find the fastest currently available machine , and export your job to that machine , and , soon , when we get all the new machines up , e then we 'll have lots more compute to use . there 's a lot of features to it and it kinda helps to balance the load of the machines people won't even have to worry about , doing speech nonspeech then . ","The group shall soon be taking delivery of more machines for a computation farm, and they discussed some software tools for running large processes. He began the meeting by reporting his recent activities, which included looking at the new baseline system. "
Bro019.G,"sunil hasn't heard about what i 've been doing . that 's just , trying to propose your next your following years of your phd work , trying to find a project to define and to work on . i 've been , looking into , doing something about r speech recognition using acoustic events . building robust primary detectors for these acoustic events , and using the outputs of these robust detectors to do speech recognition . ",They also explained some of their projects to their guest. 
Bro021.A,"still working on my quals preparation 'm thinking about , starting some , cheating experiments to , determine the , the relative effectiveness of , some intermediate categories that i want to classify . if i know where voicing occurs and everything , i would do a phone phone recognition experiment , and this would be a useful thing , to know in terms of which , which of these categories are good for , speech recognition . i hope to get those , those experiments done by the time quals come around in july . ",Other progress was also reported. 
Bro021.B,"do you make errors in different places ? different kinds of errors ? i really would like to suggest looking , a little bit at the kinds of errors . i know you can get lost in that and go forever and not see too much , but sometimes , just seeing that each of these things didn't make things better may not be enough . it may be that they 're making them better in some ways and worse in others , or increasing insertions and decreasing deletions , helping with noisy case but hurting in quiet case . and if you saw that then you it would something would occur to you of how to deal with that . also , we had we 've had these , little di discussions that was something i could say would be that we 've talked a little bit about you just doing it all with complex arithmetic and not , doing the polar representation with magnitude and phase . that @ @ given that you 're using the vad also , the effect of the vts is not far ",
Bro021.C,"the first thing is that the p eurospeech paper is , accepted . it 's the paper that describe the , system that were proposed for the aurora . and the , fff comments seems from the reviewer are good . i 've been working on t mainly on line normalization this week . i 've been trying different slightly different approaches . this actually don't s doesn't seem to help , although it doesn't hurt . but both on line normalization approach seems equivalent . i didn't look , more closely . i 've been playing a little bit with some thresholding , as a first experiment , no , the maximum energy of s each utterance then put a threshold that 's fifteen db below actually it was not a threshold , it was just adding noise . when we look at the , mfcc that result from this , they are a lot more smoother . and the result that we have in term of speech recognition , actually it 's not worse , it 's not better neither , a third thing is that , i play a little bit with the , finding what was different between , he had the france telecom blind equalization in the system . the number o of mfcc that was were used was different . you used thirteen and we used fifteen . ","The majority of the group are working on tasks related to the Aurora Project, including on-line normalization and Wiener filtering. "
Bro021.D,"the right now , the system that is there in the what we have in the repositories , with uses fifteen . we haven't w we have been always using , fifteen coefficients , not thirteen ? i 'll t s run some experiments to see whether once i have this noise compensation to see whether thirteen and fifteen really matters or not . never tested it with the compensation , but without , compensation it was like fifteen was s slightly better than thirteen , i 've been , implementing this , wiener filtering for this aurora task . i actually thought it was doing fine when i tested it once . i it 's using a small section of the code . and i got worse results than not using it . i 've been trying to find where the problem came from . and then it looks like i have some problem in the way there is some very silly bug somewhere . i it actually i it actually made the whole thing worse . and it 's , like w it 's very horrible . i was like i 'm trying to find where the m problem came , hynek showed up one suddenly on one day was actually that day i was thinking about d doing something about the wiener filtering , and then carlos matter of and then he gave me a whole bunch of filters what carlos used for his , thesis instead of designing our own new wiener filters , i may just use one of those carlos filters in this implementation that that is the next thing . once this i once i sort this pro problem out 'll just go into that also . the other thing was about the subspace approach . ","The majority of the group are working on tasks related to the Aurora Project, including on-line normalization and Wiener filtering. "
Bro021.E,"i 'm working with vts . only with vts and nothing more . not vad , no lda , nothing more . to remove the noise too . and when we put the m the , vad , the result is better . and we put everything , the result is better , but it 's not better than the result that we have without vts . ","The majority of the group are working on tasks related to the Aurora Project, including on-line normalization and Wiener filtering. "
Bro021.F,"let 's see , we should just get a list of items there 's the usual updates , everybody going around and saying , what they 're working on , the things that happened the last week . do you want to start , stephane ? how about you , barry ? 'll just pass it on to dave . how about you , sunil ? how about you , carmen ? ","The ICSI Meeting Recorder Group at Berkeley met once more to discuss group members' progress. Other progress was also reported. The majority of the group are working on tasks related to the Aurora Project, including on-line normalization and Wiener filtering. "
Bro021.G,"in my lunch talk last week i said i 'd tried phase normalization and gotten garbage results using that l long term mean subtraction approach . it turned out there was a bug in my matlab code . and , the results were better . but they still weren't as good as just subtracting the magnitude the log magnitude means . and also i 've been talking to , andreas and thilo about the , smartkom language model and about coming up with a good model for , far mike use of the smartkom system . i 'm gonna be working on , implementing this mean subtraction approach in the far mike system for the smartkom system , and thought at first , that , what i should do is unwrap the phase but i actually got worse results doing that unwrapping using the simple phase unwrapper that 's in matlab than i did not unwrapping ",Other progress was also reported. 
Bro022.A,"should we just do the same deal where we go around and do , status report things ? why don't you go ahead , barry ? you want to go next , dave ? do you want to go , stephane ? it didn't seem to help in the htk system . there 's also the normalization . i 'm not how they would do it when they 're working with the digits , but in the switchboard data , there 's , conversation side normalization for the non c zero how about you , carmen ? ","A typical progress report meeting for the ICSI Meeting Recorder Group at Berkeley. Each of the group reported their most recent progress, and any results they have achieved. This then prompted discussion about the reasons behind such findings, which were for the most part not as expected. "
Bro022.B,"i don't think just multiplying the signal by two would have any effect . if you really have louder signals , what is that you have better signal to noise ratio . the microphone isn't gonna pass any dc . actually , there are instrumentation mikes that do pass go down to dc . no , it 's the electronics . then there 's amplification afterwards . you can have dc offset in the data . they have channel adaptation . they have speaker adaptation . they have i thin think they use these , genone things . there 's these pooled models and they can go out to all sorts of dependencies . they have tied states to clarify something for me . they were supp supposedly , in the next evaluation , they 're going to be supplying us with boundaries . does any of this matter ? do we ? is there some reason that we think that 's the case ? but we 'll get some insight on that when , the gang gets back from crete . you 're you 're the first one here to work with vts , we could call someone else up who has , i don't have a good feeling for it . ","This then prompted discussion about the reasons behind such findings, which were for the most part not as expected. "
Bro022.C,"i don't see why your signal is louder after processing , didn't look , but one thing that makes a difference is this dc offset compensation . do y did you have a look at the meet meeting digits , if they have a dc component , i 'm more interested in trying to figure out what 's still the difference between the sri system and the aurora system . will train gender dependent models , because this is also one big difference between the two systems . the other differences were the fact that the acoustic models of the sri are more sri system are more complex . but , chuck , you did some experiments with this this is another difference . their normalization works like on the utterance levels . the next thing is this vad problem that , i 'm just talking about the curves that i sent you whi that shows that when the snr decrease , the current vad approach doesn't drop much frames for some particular noises , which might be then noises that are closer to speech , acoustically . first of all , the boundaries might be , like we would have t two hundred milliseconds or before and after speech . removing more than that might still make a difference in the results . no . actually if i don't don't want to work too much of on it right now . wanted to see if it 's what i observed was the re was caused by this vad problem . the second thing is the this spectral subtraction . which i 've just started yesterday to launch a bunch of , twenty five experiments , with different , values for the parameters that are used . the next thing , what i will try to do is just to try to smooth the , to smooth the d the result of the subtraction , right . ","Topics the group touched upon included spectral subtraction, phase normalization, Voice activity detection, along with comparisons between systems. Each of the group reported their most recent progress, and any results they have achieved. This then prompted discussion about the reasons behind such findings, which were for the most part not as expected. "
Bro022.D,"i am still working with , vts . and , one of the things that last week , say here is that the problem was with the diff because the signal have different level of energy . and , talking with stephane and with sunil , we decide that it was interesting to apply on line normalization before applying vts . but then we decided that 's it doesn't work because we modified also the noise . thinking about that , we then we decide that is a good idea . we don't know . i didn't do the experiment yet to apply vts in cepstral domain . we s decide to m to obtain the new expression if we work in the cepstral domain . but i 'm not if that will be usefu useful . it 's quite a lot it 's a lot of work . and i want to know if we have some feeling that the result i don't have any feeling if this will work better than apply vts aft in cepstral domain will work better than apply in m mel in filter bank domain . i don't know nothing . ","Each of the group reported their most recent progress, and any results they have achieved. "
Bro022.E,"last week i finally got results from the sri system about this mean subtraction approach . and , we got an improvement , in word error rate , training on the ti digits data set and testing on meeting recorder digits of , six percent to four point five percent , on the n on the far mike data using pzm f , but , the near mike performance worsened , from one point two percent to two point four percent . wh why would that be , considering that we actually got an improvement in near mike performance using htk ? with some input from , andreas , i have a theory in two parts . first of all htk sr the sri system is doing channel adaptation , and htk wasn't . and also , the andreas pointed out the sri system is using more parameters . those finer grained acoustic models could be more sensitive to the artifacts in the re synthesized audio . o one thing , i noticed is that , the mean subtraction seems to make the pzm signals louder after they 've been re synthesized . was wondering , is it possible that one reason it helped with the aurora baseline system is just as a gain control ? cuz some of the pzm signals sound pretty quiet if you don't amplify them . and i also , did some experiments about normalizing the phase . the interesting thing that i tried was , adam and morgan had this idea , since my original attempts to , take the mean of the phase spectra over time and normalize using that , by subtracting that off , didn't work . that we thought that might be due to , problems with , the arithmetic of phases . what i did instead is i took the mean of the fft spectrum without taking the log or anything , and then i took the phase of that , and i subtracted that phase off but that , didn't work either . ","Each of the group reported their most recent progress, and any results they have achieved. This then prompted discussion about the reasons behind such findings, which were for the most part not as expected. "
Bro022.F,"this past week i 've just been , getting down and dirty into writing my proposal . finished a section on , on talking about these intermediate categories that i want to classify , as a middle step . i hope to get this , full rough draft done by , monday can give it to morgan . ","Each of the group reported their most recent progress, and any results they have achieved. "
Bro022.G,"no . the dc component could be negligible . any all of the mikes have the dc removal some capacitor sitting right in that bias it . and the acoustic models are like k triphone models or is it the whole word ? can smooth the snr estimate , also . your filter is a function of snr . th i 've been playing with this wiener filter , like . and there are there were some bugs in the program , was p initially trying to clear them up . and , once it was cleared , i ran a few experiments with different ways of smoothing the estimated clean speech and how t estimated the noise and , smoothing the snr also . and the trend seems to be like , smoothing the current estimate of the clean speech for deriving the snr , which is like deriving the wiener filter , seems to be helping . then updating it quite fast but still it 's like it 's still comparable to the baseline . but that 's not using any wiener filter . and the other thing is putting a floor on the , snr , some in some cases the clean speech is , like when it 's estimated , it goes to very low values , the snr is very low . that actually creates a lot of variance in the low energy region of the speech . far i 've been testing only with the baseline , which is which doesn't have any lda filtering and on line normalization . want to separate the contributions out . it 's just vad , plus the wiener filter , plus the baseline system , which is , just the spectral the mel sp mel , frequency coefficients . ","Each of the group reported their most recent progress, and any results they have achieved. "
Bro028.A,"'ve , downloaded , a couple of things from mississippi state . they wrote some scripts that make it easy to run the system on the wall street journal , data . haven't run the scripts yet . i 'm waiting there was one problem with part of it and i wrote a note to joe asking him about it . 'm waiting to hear from him . they 're i 'm still waiting for them to release the , multi cpu version of their scripts , cuz right now their script only handles processing on a single cpu , which will take a really long time to run . as soon as they get that , then i 'll grab those too i 'll go ahead and try to run it though with just the single cpu one , and i they , released like a smaller data set that you can use that only takes like sixteen hours to train and can run it on that just to make that the thing works and everything . ","On the larger vocabulary task, there are still a few issues to resolve before work can really get started. "
Bro028.B,"we have to reduce it by ten milliseconds somehow . w it 's p d primary primarily determined by the vad at this point , s we can make the vad a little shorter . we probably should do that pretty soon that we don't get used to it being a certain way . but th the two th two thirty includes the tandem network ? and i is the tandem network , small enough that it will fit on the terminal size ho how much memory d ? h how many ? i 'd like to see that , cuz could think a little bit about it , cuz we we could make it a little smaller i 'd like to see how far off we are . but it 's still within their rules to have it on the , t server side . this is a particular human . this is this i this is stephane . there 's two problems there . mean , the first is that by doing lpc twelve with synthesized speech w like you 're saying , it 's i you 're adding other degradation . it 's not just the noise but you 're adding some degradation because it 's only an approximation . and the second thing is which is m more interesting is that , if you do it with whispered speech , you get this number . what if you had done analysis re synthesis and taken the pitch as now you put the pitch in . what would the percentage be then ? see , that 's the question . that would say at least for people , having the pitch is really , really important , th lpc is not a really great representation of speech . but i don't know . i do don't wanna take you away from other things . it 's probably not worth your time . it 's a side thing and there 's a lot to do . while we 're still on aurora you can talk a little about the status with the , wall street journal things for it . cuz we have to get started , it could be chuck and i had actually talked about this a couple times , and over some lunches , that , one thing that we might wanna do the there 's this question about , what do you wanna scale ? suppose y you can't adjust these word insertion penalties and forth , you have to do everything at the level of the features . and , one thing i had suggested at an earlier time was some scaling , some root of the , features . it occurred to me later , because what you really want to do is scale the , @ @ the range of the likelihoods rather than but , we still haven't had a ruling back on this . and we may end up being in a situation where we just really can't change the word insertion penalty . but the other thing we could do is also we could this may not help us , in the evaluation but it might help us in our understanding at least . we might , just run it with different insper insertion penalties , and show that , "" not changing it , playing the rules the way you wanted , we did this . but if we did that , it made a big difference . "" th it 's it 's kinda neat the thing that i wanted to add to it also was to have us use this in a multi stream way . that , when you come up with these different things , and these different functions , you don't necessarily just put them all into one huge vector , but perhaps you have some of them in one stream and some of them in another stream , and forth . ","The latency limit has been set, and the group's system is performing very well, but is a little over. On the larger vocabulary task, there are still a few issues to resolve before work can really get started. The group heard of the plan of one of it's member's work into intermediate classifiers, and also of how a visiting research student's work into auditory models can be applied to their work. "
Bro028.C,"michael kleinschmidt , who 's a phd student from germany , showed up this week . he 'll be here for about six months . and he 's done some work using an auditory model of , human hearing , and using that f to generate speech recognition features . and he did work back in germany with , a toy recognition system using , isolated digit recognition as the task . he w he 's coming here to u use it on a a real speech recognition system . th this is because it 's , there are these different parameters for the shape of these basis functions , there are a lot of different possible basis functions . and he actually does an optimization procedure to choose an optimal set of basis functions out of all the possible ones . is , he starts with he has a set of m of them . he t he tries , using just m minus one of them . there are m possible subsets of this length m vector . he tries classifying , using each of the m possible sub vectors . whichever sub vector , works the best , he says the fe feature that didn't use was the most useless feature , we 'll throw it out and we 're gonna randomly select another feature from the set of possible basis functions . ","The group heard of the plan of one of it's member's work into intermediate classifiers, and also of how a visiting research student's work into auditory models can be applied to their work. "
Bro028.D,"there was this conference call this morning , and the only topic on the agenda was just to discuss a and to come at to get a decision about this latency problem . there were like two hours of discussions , and then suddenly , people were tired , and they decided on a number , two hundred and twenty , included e including everything . currently d we have system that has two hundred and thirty . that 's not a problem , the second thing is the system that we have currently . yes . we have system that gives sixty two percent improvement , but if you want to stick to the this latency it has a latency of two thirty , but if you want also to stick to the number of features that limit it to sixty , then we go a little bit down but it 's still sixty one percent . and if we drop the tandem network , then we have fifty seven percent . no , i don't think it 's still in terms of computation , if we use their way of computing the maps the mips , it fits , but it 's , m mainly a problem of memory . and i don't know how much this can be discussed or not , because it 's it could be in rom , it 's not that expensive . i d i don't kn remember exactly , there were small tricks to make this tandem network work . and one of the trick was to , use some hierarchical structure where the silence probability is not computed by the final tandem network but by the vad network . it looks better when , we use the silence probability from the vad network there was a p problem that we observed , that there was there were many insertions in the system . actually plugging in the tandem network was increasing , the number of insertions . and then just using the other silence probability helps . the last thing is that we are getting close to human performance . that 's something i would like to investigate further , i did did , listen to the m most noisy utterances of the speechdat car italian and tried to transcribe them . that 's the flaw of the experiment . but still , what happens is that , the digit error rate on this is around one percent , while our system is currently at seven percent . but what happens also is that if i listen to the , re synthesized version of the speech and i re synthesized this using a white noise that 's filtered by a lpc , filter you can argue , that , that this is not speech , the ear is not trained to recognize this . but s actually it sound like whispering , ","On the Aurora Project, there were reports on a project conference call, the status of the tandem neural networks, and progress with the Mississippi State recognizer. The latency limit has been set, and the group's system is performing very well, but is a little over. "
Bro028.E,"that segues into what i 'm doing . the big picture is k come up with a set of , intermediate categories , then build intermediate category classifiers , then do recognition , right now i 'm in the phase where i 'm looking at , deciding on a initial set of intermediate categories . and i 'm looking for data driven methods that can help me find , a set of intermediate categories of speech that , will help me to discriminate later down the line . and one of the ideas , that was to take a neural net train an ordinary neural net to to learn the posterior probabilities of phones . the other one was , to , come up with a model a graphical model , that treats the intermediate categories as hidden variables , latent variables , that we don't know anything about , but that through , s statistical training and the algorithm , at the end of the day , we have , we have learned something about these latent , latent variables which happen to correspond to intermediate categories . ","The group heard of the plan of one of it's member's work into intermediate classifiers, and also of how a visiting research student's work into auditory models can be applied to their work. "
Bsr001.A,,
Bsr001.B,,
Bsr001.C,,
Bsr001.D,,
Bsr001.E,,
Bsr001.F,,
Bsr001.G,,
Bsr001.H,,
Btr001.A,,
Btr001.B,,
Btr001.C,,
Btr001.D,,
Btr001.E,,
Btr001.F,,
Btr002.A,,
Btr002.B,,
Btr002.C,,
Btr002.D,,
Btr002.E,,
Btr002.F,,
Buw001.A,"what can you add to it to get , some information that would be helpful for the user interface design ? the there will be jargon that we he there 'll be transcription errors . actually there 's three issues . there 's the crosspad issue . should we do it and , if what 'll we have them do ? do we have s people write summaries ? everybody or one person ? and then , do we ask people for how they would query things ? that normally at most meetings somebody is delegated to be a note taker . and why don't we just use the notes that somebody takes ? the second thing you could possibly do with it is just run a little pilot experiment with somebody saying "" here 's a summary of a meeting , what questions might you want to ask about it to go back ? "" but in most meetings that i attend , there 's somebody t explicitly taking notes , frequently on a laptop and then they post edit it into , a summary and they email it out for minutes . crosspads , we 're just gonna try it and see what happens . i j we should just say this is not we don't want to put any extra burden on people , but if they happen to generate minutes , could they send it to us ? because know when i see the clock coming near the end of the meeting , i 'm like inching towards the door . you 're probably not gonna get a lot of people wanting to do this . that doing it orally at the end of the meeting is the best time . crosspads we were going to try , if landay can get the , get them to you guys , and see if they 're interesting . getting electronic summary from a note taking person if they happen to do it anyway . getting just , digital pictures a couple digital pictures of the table and boards to set the context of the meeting . and then going around the room at the end to just say qu ask people to mention something interesting that they learned . and then the last thing c would be for those people who are willing to stay afterwards and give an oral summary . play with and generate real queries from . what i have down for action items is we 're supposed to find out about our human subject , requirements . people are supposed to send me u r for their for web pages , to c and i 'll put together an overall cover . and you also need to look at your web page and clean it up by mid july . you need to put together a mailing list . i need to email adam or jane , about getting the data . 'm just interested in getting transcriptions . w my u feeling right now on format is you guys have been doing all the work and whatever you want , we 're happy to live with . i 'm shooting to try to get it done get it put together by the beginning of august . that 's what i wanna work with , is things that s the wrong material but the right da the right source . i could use , could use your web pages . you 've got all the web pages on the meeting recor use what web pages there are out there on meeting recorders . there 's icsi , xerox , i don't think we should have rules of participation , but we should try to get a variety of meetings . that 's something that if we get the meeting going at uw , that i probably can do more than you guys , 'm just writing here , we 're not gonna try to specify rules of interaction but we 're gonna try to get more variety by i using different groups of people and different sizes . the reason why i didn't want to is be why i personally didn't want to is because i wanted it to be as , unintrusive as possi ","Meeting notes taken by participants as standard minutes or summaries, or on devices like CrossPads can provide useful information. Another option would be the recording by participants of short oral summaries of the meeting. Taking some photos of the whiteboard and the positioning of participants is easy enough to do. Finally, project web pages and mailing list are being set up and UW are going to investigate the suitability of their recording equipment. There is also work being done on the annotation of prosody. "
Buw001.B,"if you could sense just when people are writing , and you tell them not to doodle , or try not to be using that for other purposes , and each person has a note pad . they just get it when they come in the room . then you c you can just have a fff plot of wh who 's writing when . and , you can also have notes of the meeting . but i bet that 's that will allow you to go into the the hot places where people are writing things down . but i bet it 's a good superset of it . y you should also have a record of what 's on the board . but the outline is up here but i 'm just saying first of all there 's a whole bunch of fusion issues that darpa 's interested in . fusing gesture and face recognition , even lip movement and things like that , also cmu has been doing this and they have said , i talked to the student who had done this , that with two fairly inexpensive cameras they just recorded all the time think we would be we might lose the chance to use this data for somebody later who wants to do some processing on it if we don't collect it it 'd be useful to have a small amount of it just as a proof of concept . the level of the query could be , very low level or very high level . if this is something that requires a one word answer or it 's one place in the recording versus was there general agreement on this issue of all the people who ha you can gen you can ask queries that are meaningful for people . but they won't exist anywhere in the a but mark and others have said that there 's quite a lot of found data from the discourse community that has this characteristic there 's lots of recordings that they 're not close talk mike , and there 's lots of television , on , political debates and things like that , congre congressional hearings . and then the cmu folks and i were on the other side in cuz they had collected a lot of meetings that were like this and , we just left it as @ @ that if there 's found data that can be transformed for use in speech recognition easily , then we would do it , but newly collected data would be natural meetings . but as far as i know they didn't offer that data to the community at this meeting . it 's things occur without any the pe these people weren't wearing close talking mikes , but they were recorded anyway , like the congressional hearings and , for legal purposes or whatever . ","There is also interest in the speech community for fusion of speech with visual data. Candidate types are keyword searches, action items, elaboration on points of interest, and agreement between participants. The corpus could be enriched with found data (public or collected by other projects), if those prove appropriate for use in the project. "
Buw001.C,"now , something that i mentioned earlier to mari and liz is that it 's probably important to get as many non technical and non speech people as possible if you could ask other people to call and use our system , that 'd be good . ",
Buw001.D,"what we had was that we were gonna talk about data collection , and other tasks during data collection , and , once we get out beyond our little group , the people 's motivation factor , reduces enormously . but if we were asking the question , which we were , of of , "" how do we figure out what 's the nature of the queries that people are gonna want to ask of such a system ? "" , knowing what 's important doesn't tell you what people are going to be asking . i we have five hours of pilot data of the other but we have zero hours of queries . now i 'm thinking that the summary a summary , is actually a reasonable , bootstrap into this into what we 'd like to get at . now we have five hours of meetings and , you ask me , "" morgan , what d what questions do you want to ask ? "" i wouldn't have any idea what questions i want to ask . if i looked at summary of it , i 'd go "" i was in that meeting , and th that might then help me to think of things even things that aren't listed in the summary , but just as a as a refresh of what the general thing was going on in the meeting . minimally , what dan is referring to at least having some representation of the p the spatial position of the people , at the moment we should be determining this on the basis of our own , interests and needs rather than hypothetical ones from a community thing . like for a meeting like this , at least , take a polaroid of the of the boards , the question that we started with was whether there was anything else we should do during th during the collection . and the crosspads was certainly one idea , the point was to try again , to try to collect more information that could be useful later for the ui lot of the we 're doing now really is pilot it 's like the note taking thing , that y that you can't certainly can't require it or people aren't gonna want to do this . but if there 's some cases where they will , then it would be helpful . u i actually think that , again , just as a bootstrap , if we do have something like summaries , then having the people who are involved in the meetings themselves , who are cooperative and willing to do yet more , come up with queries , could at least give landay an idea of the things that people might want to know . and it might be something minor of minor importance to the meeting . and then i 'm trying to fi that 's when i look in general when i look things up most , is when it 's something that didn't really stick in my head the first time around and but for some new reason i 'm i 'm interested in the old what i keep coming back to in my own mind is that , the soonest we can do it , we need to get up some system that people who 've been involved in the meeting can go back later , even if it 's a poor system in some ways , and , and ask the questions that they actually want to know . if if as soon as we can get that going at any level , then we 'll have a much better handle on what questions people want to ask than in any anything we do before that . if we can figure out a way to jimmy a very rough system , say in a year , then that in the second and third years we actually have something to in our phone call , before , we , it turns out the way we 're gonna send the data is by , and then what they 're gonna do is take the cd rom and transfer it to analog tape and give it to a transcription service , that will  and then there will be some things many things that don't work out right . and i was gonna m email them the which i haven't yet , a pointer to the web pages that we currently have , cuz in particular they want to see the one with the way the recording room is set up we also had the , that we were s that you were gonna get us the eight hundred number we 're gonna call up your communicator thing and we 're gonna be good slash bad , depending on how you define it , users . we talked about that we 're getting the recording equipment running at uw . and he also said something about outside there that came up about the outside text sources , that he may have there 's some , carnegie mellon right ? it also depends on the style of the group of people . actually , th @ @ the cmu folk have collected a lot of data . we still haven't sent out the first note saying "" hey , this list exists "" . we 're just compiling the list . ","The discussion concerned mainly ideas about data collection and the nature and generation of queries on meetings. Summaries could be used to bootstrap for queries, the exact nature of which remained nebulous. Candidate types are keyword searches, action items, elaboration on points of interest, and agreement between participants. Taking some photos of the whiteboard and the positioning of participants is easy enough to do. Meeting notes taken by participants as standard minutes or summaries, or on devices like CrossPads can provide useful information. An initial prototype system to test any hypotheses can be pipelined. The recorded data will be stored on CD-ROM's and sent to IBM for transcription. Finally, project web pages and mailing list are being set up and UW are going to investigate the suitability of their recording equipment. "
Buw001.E,"right . we because you 'd have several people with these pads , you could collect different things . but th there is this , there is this class of queries , which are the things that you didn't realize were important at the time but some in retrospect you think "" hang on , didn't we talk about that ? "" and it 's something that didn't appear in the summary but you for this data capture , it would be to have a digital camera just to take pictures of who 's there , where the microphones are , and then we could also put in what 's on the board . like three or four snaps for every that was that was what he was saying was this he this thing that , jason had been working on finds web pages that are thematically related to what you 're talking about . that would be a source of text which is supposedly got the right vocabulary . we should go around and say something interesting that happened at the meeting ? ","Meeting notes taken by participants as standard minutes or summaries, or on devices like CrossPads can provide useful information. Taking some photos of the whiteboard and the positioning of participants is easy enough to do. "
Buw001.F,"the down side to that is that he indicated that the , quality of the handwriting recognition was quite poor . in that where or when do you actually ask them about that ? the question i had about queries was , what we 're planning to do is have people look at the summaries and then generate queries ? the question i had is have we given any thought to how we would generate queries automatically given a summary ? that 's a whole research topic un unto itself , ","Summaries could be used to bootstrap for queries, the exact nature of which remained nebulous. "
Buw001.G,"but if we had the crosspads , we could ask people , if something comes up write it down and mark it somehow , don't know how else to generate the queries other than getting an expert to actually listen to the meeting and say "" that 's important , if you had a meeting participant , they could use the summary to refresh themselves about the meeting and then make up queries . but adding photographs adds a whole nother level of problems . but that it 's gonna be a lot of effort on our part to create it , and store it , and get all the standards , and to do anything with it . that if you have that , then people who are interested in vision can use this database . the problem with it is you 'll have more people who don't want to be filmed than who don't want to be recorded . but , that 's a lot of infrastructure and work . though , the importance marking is a good idea , though . what i was gonna say is that i don't want to ask people to do something they wouldn't normally do in a meeting . and then jane 's idea of summarization afterward is not a bad one . we 'll we 'll be telling them that the reason we 're trying to do this is to d generate queries in the future , think we 're gonna have to start with keywords is this ibm ? do they how are they gonna do the multi channel ? not an immediate action item but something we do have to worry about is data formats for higher level information . different level , prosody and all that we 're gonna have to figure out how we 're gonna annotate that . but that 's not that 's display . the "" georgia tech classroom two thousand "" is a good one . ","Summaries could be used to bootstrap for queries, the exact nature of which remained nebulous. Candidate types are keyword searches, action items, elaboration on points of interest, and agreement between participants. The recorded data will be stored on CD-ROM's and sent to IBM for transcription. There is also work being done on the annotation of prosody. "
Buw001.H,"which is when you say "" jimlet "" and the person listening won't that is , sh shou how do we get is that important information ? and i also , was thinking , with reference to the n note taking , the advantage there is that you get structure without the person having to do something artificial later . and the fir third thing i wanted to say is the summaries afterwards , they should be recorded instead of written i e my thought was to have multiple people summarize it , on recording rather than writing whereas if you just say it immediately after the meeting , a two minute summary of what the meeting was about , you would get , my proposal would be that it may be worth considering both of those types , the note taking and a spontaneous oral summary afterwards , but she 's we 're just proposing a minimal preservation of things on boards , i 'm also wondering if we could ask the people a question which would be "" what was the most interesting thing you got out of this meeting ? "" it might be , that the summary would not in even include what the person thought was the most interesting fact . you could ge pick up some of the micro items that wouldn't even occur as major headings it wouldn't be too , cost intensive either . they 'd be s they 'd be separate . they wouldn't be hearing each other 's summaries . and that would also be that the data would be included in the database . i was wondering if there might be one s more source of queries which is indicator phrases like "" action item "" , which could be obtained from the text from the transcript . i m emailed them the transcriber url , the on line , data that adam set up , they can click on an utterance and hear it . and i emailed them the str streamlined conventions we discussed , musi musical score notation i s i know that from with the near field mikes some of the problems that come with overlapping speech , are lessened . if you 've got someone giving a report you 're not gonna have as much overlap . but , in a meeting which would otherwise be highly overlapping , is the near field mike enough or should we have some rules of participation for some of our samples to lessen the overlap ? wanted to be that we will not be having a lot of data which can't be processed . found data "" , they mean like established corpora of linguistics and other fields , right ? but it includes like standard corpora that have been used for years in linguistics and other fields . ","Another option would be the recording by participants of short oral summaries of the meeting. Candidate types are keyword searches, action items, elaboration on points of interest, and agreement between participants. The corpus could be enriched with found data (public or collected by other projects), if those prove appropriate for use in the project. "
