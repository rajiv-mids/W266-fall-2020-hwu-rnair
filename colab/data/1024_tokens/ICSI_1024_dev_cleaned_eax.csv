meeting,extractive,abstractive
Bed002.A,,
Bed002.B,"the street network of our geographic information system . it would always use the closest point to the object , what we found interesting is , first of all , intentions differ . you want to enter a building . you want to see it , take a picture of it . or you actually want to come as close as possible to the building . if you don't have the intention of entering your building , but that something is really close to it , and you just want to approach it , or get to that building . and the places where you will lead them for these intentions are sometimes ex in incredibly different . and we get tons of these "" how do i get to "" , "" i want to go to "" , we can look at some factors that may make a difference . this is crucial factor , "" what type of object is it ? "" then the actual phrases may give us some idea of what the person wants . sometimes i found in the looking at the data , in a superficial way , i found some s modifiers that m may also give us a hint , and this leads us straight to the context which also should be considered . what we do know , is that the parser we use in the smartkom system will never differentiate between any of these . it 's it 's way too crude to d capture those differences in intentions . for a deep understanding task , that 's a playground or first little thing . "" "" we need , we gonna get those m three l structures . and i will try to come up with a list of factors that we need to get out of there , and we want to get a g switch for the context . if we feed it through a belief net along those lines . we 'd get an inferred intention , we produce a structure that differentiates between the vista , the enter , and the , tango mode . a lot of parsers , that 's way beyond their scope , is of interpreting that . because i we can not differentiate , at the moment , between , the intention of wanting to go there or the intention of just know wanting to know where it is . not from that data . but , since we are designing a an , compared to this , even bigger data collection effort , we will definitely take care to put it in there , and i 'm even i the most , deliberate data collection experiment will never give you data that say , "" if it 's phrased like that , the intention is this . "" but it was never th the goal of that data collection to serve for sat for such a purpose . that 's why the tasks were not differentiated by intentionality , i 'm we can produce some if we need it , that will help us along those lines . u 'm , at the moment , curious and i 'm s w want to approach it from the end where we can s start with this toy system that we can play around with , that we get a clearer notion of what input we need for that , and then we can start worrying about where to get this input , and you see , either th the three d model , or a quicktime animation of standing u in a square in heidelberg . just off a textbook , tourist guide , to familiarize , yourself with that odd sounding german street names , like fischergasse and forth . part two is , you 're told that this huge new , wonderful computer system exists , that can y tell you everything you want to know , and you get a certain amount of tasks that you have to solve . first you have to know find out how to get to that place , and then the g system breaks down . and then , a human operator comes on , and exp apologizes that the system has crashed , and you have the same tasks again , just with different objects , this th the dialogue history is producing xml documents . and the ontology that the student is constructing for me back in eml is in oil and that 's also in xml . and that 's where a lot of knowledge about bakeries , about hotels , about castles and is gonna come from . if we , get really w wild on this , we may actually want to use some corpora that other people made and , if they are in mate , then we get x m l documents with discourse annotations , t that 's what i will do next monday is show the state and show the system and show that . but but the this point is really very , very valid that ultimately we hope that both will merge into a harmonious and , wonderful , state where we can not only do the bare necessities , ie , changing the table it does exactly in english what it does in german , but also that we can have the system where we can say , "" this is what it usually does , and now we add this little thing to it "" , johno 's and bhaskara 's great belief net , and then for these certain tasks , and we know that navigational tasks are gonna be a core domain of the new system , it all of a sudden it does much better . not only can you show that something sensible but ultimately , if you produce a system like this , it takes the person where it wants to go . ","In the navigational paradigm used for the task, these intentions are to ""see"" to ""enter"" or to ""get to the closest point of"" a building. However, the starting point is, through the use of existing data, to determine possible linguistic, discourse or situation features that define intentionality. These may include the type of building, time of day, particular phrases used or whether the user is a tourist or a native. Consequently, they will be fed into a belief-net -implemented on a software package like JavaBayes- and the conditional probability of each intention calculated. There will be purpose-designed experiments carried out. A prototype system will be put together to test hypotheses regarding both the exact nature of the features and how intentions are derived from them. "
Bed002.B,"rather than taking him always to the geometric center of a building , ",
Bed002.C,"this was about inferring intentions from features in context , and the words , like "" s go to see "" , or "" visit "" , or some we think it 's a formed , starter task for this , deeper understanding in the tourist domain . there 's gonna be contextual things , there 're gonna be linguistic things , there 're gonna be discourse things , and they gotta be combined . and , my idea on how to combine them is with a belief net , which is going to have as output , the conditional pr probability of one of three things , there are two questions is , one , where do you get this i information from , and two , what 's the structure of the belief net ? what are the conditional probabilities of this , that , and the other , given these things ? and you probably need intermediate nodes . i we don't they are yet . another thing you want is some information abou about the time of day . one thing you could do is build a little system that , said , "" whenever you got a question like that i 've got one of three answers . you want the ability to a you want the ability to ask , but what you don't wanna do is onl build a system that always asks every time , that 's not getting at the scientific problem , s the way that might come up , if you wanna suppose you wanted to do that , you might say , "" as an intermediate step in your belief net , is there a source path goal schema involved ? "" and if is there a focus on the goal ? what he was saying is , the m three l does not have any of that . right . th they 're gonna give us some cr or we can assume that y you get this crude information . about intention , and they don't give you the object , think we ought to d a as we have all along , d we 've been distu distinguishing between situational context , which is what you have as context , and discourse context , but the idea is to take as a first goal , see if we could actually build a belief net that would make this three way distinction in a plausible way , these we have all these transcripts and we 're able to , by hand , extract the features to put in the belief net . here 're the things which , if you get them out of the language and discourse , and put them into the belief net , it would tell you which of these three intentions is most likely . "" and if to actually do that , build it , run it y run it on the data where you hand transcribe the parameters . if that goes then we can start worrying about how we would extract them . and , expand it to other things like this . but if we can't do that , then we 're in trouble . that , if we can get the information , a belief net is a perfectly good way of doing the inferential combination of it . the real issue is , do what are the factors involved in determining this ? you all know this , but we are going to actually use this little room and start recording subjects probably within a month we have to have this discussion of th the experiment , and the data collection , and all that sorta we that 's part of what we 'll have to figure out . the problem that i was tr gonna try to focus on today was , let 's suppose by magic you could collect dialogues in which , one way or the other , you were able to , figure out both the intention , and set the context , and language was used . the issue is , can we find a way to , featurize it that we get some discrete number of features that , when we know the values to all those features , or as many as possible , we can w come up with the best estimate of which of the , in this case three little intentions , are most likely . is there a construction , or the object , or w anything else that 's in the si it 's either in the s the discourse itself or in the context . if it turns out that , whatever it is , you want to know whether the person 's a tourist or not , that becomes a feature . now , how you determine that is another issue . though sin f in the short run , you 'd set them , and then in the longer run , you would figure out how you could derive them . from previous discourse or w any anything else you knew . do e either of you guys , you got a favorite belief net that you 've , played with ? javabayes one of th one of the things we wanna do is actually , pick a package , we don't need the one that 'll solve massive , belief nets quickly . but we do want one in which it 's easy to interact with and , modify . and probably one in which it 's easy to have , what amounts to transcript files . you want it stable , you want it and , as soon as we have one , we can start trying to , make a first cut at what 's going on . we have a we the outcomes are gonna be , and we have some data that 's loose , we can use our own intuition , ","The initial task of the EDU group is to work on inferring intentions through context. These may include the type of building, time of day, particular phrases used or whether the user is a tourist or a native. However, the starting point is, through the use of existing data, to determine possible linguistic, discourse or situation features that define intentionality. Consequently, they will be fed into a belief-net -implemented on a software package like JavaBayes- and the conditional probability of each intention calculated. A prototype system will be put together to test hypotheses regarding both the exact nature of the features and how intentions are derived from them. Initially, these features will be hand-coded, but the goal is to find ways of extracting them automatically from the XML data. "
Bed002.C,"it if it turns out that just , thinking about the problem , you come up with things you really need to this is the thing that is , an intermediate little piece in your belief net . that 'd be really interesting . an and there 're plenty of people around , students in the department who , live and breathe bayes nets . we 're committed to xml as the interchange . it 'd be but i do i don't wanna count on it . in terms of the , the what the smartkom gives us for m three l packages , it could be that they 're fine , or it could be eeh . we 're not abs we 're not required to use their packages . we are required at the end to give them in their format , bu w i 'd like that this y this week , to ha to n to have y guys , pick the y belief net package and , then as soon as we have it , we should start trying to populate it for this problem . i 'd like to also , though , ha have a first cut at what the belief net looks like . even if it 's really crude . unt until we know more . what one hopes is that when we understand how the analyzer works , we can both worry about converting it to english and worry about how it could ex extract the parameters we need for the belief net . n none of this is i n neither of these projects has got a real tight time line , in the sense that over the next month there 's a deliverable . ",A prototype system will be put together to test hypotheses regarding both the exact nature of the features and how intentions are derived from them. 
Bed002.D,"and it 's clear from the data , sorta the correct answer in each case . it 's just like , "" let 's figure out what they would say under the circumstances "" . the reason i was asking about the the de the details of this thing is that , it 's one thing to collect data for , i don't know , speech recognition or various other tasks that have pretty c clear correct answers , but with intention as you point out , there 's a lot of di other factors and i 'm not really how e the question of how to make it a t appropriate toy version of that it 's ju it 's just hard . ",
Bed002.E,the m three l is the old smartkom output ? how exactly does the data collection work ? what 's the time frame for this ? ,
Bed002.F,"we 'll be like , hand , doing all the probabilities . ","Initially, these features will be hand-coded, but the goal is to find ways of extracting them automatically from the XML data. "
Bed003.A,"the middle layer is also binary ? and at this stage we will we do want to get modifiers in there because they may also tell us whether the person is in a hurry or not however it the purpose was not really , at this stage , to come up with meaningful probabilities but to get thinking about that hidden middle layer . how long would it take to add another node on the observatory and , play around with it ? and then there 's also the question whether it may be entered . guess your question is far i have no really arg no real argument why to differentiate between statues as statues and houses of celebrities , from that point of view . can we add , just can see how it 's done , a "" has door "" property command line . but if th if there is an xml file that or format that it can also read is that we ob we could observe a couple of discourse phenomena such as the admission fee , and something else and something else , that happened in the discourse before . this could be separate region of the net , which has two has it 's own middle layer . which is something that is a more general version of the actual phenomenon that you can observe . they ra may have there own hidden layer that points to some of the real hidden layer , or the general hidden layer . and the same we will be able to do for syntactic information , something somebody can have discussed the admission fee and u the answer is s if we still , based on that result is never going to enter that building . and this won't differentiate between all modes , but at least it 'll tell us "" here we have something that somebody that wants to go someplace , what happens is the parser parses it and then it 's handed on to the discourse history which is , one of the most elaborate modules . it helps an anaphora resolution and it fills in all the structures that are omitted , the discourse model actually stores what was presented at what location on the s on the screen we can query it whether admission fees were discussed in the last turn there are some of them are extremely elaborate , what happened what might happen is that we do get this task based middle layer , suggest w to for to proceed with this in the sense that throughout this week the three of us will talk some more about segmenting off different regions , identify four regions , make up some features for each region middle layer for those . and then these should then connect somehow to the more plan based deep space also we can start looking at the smartkom tables i have to report data collection . we interviewed fey , she 's willing to do it , meaning be the wizard for the data collection , also transcribe a little bit , if she has to , but also recruiting subjects , organizing them , and forth . jerry however suggested that we should have a trial run with her , and it looks as if friday afternoon will be the time when we have a first trial run for the data . yes w we would like to test the wizard , but if we take a subject that is completely unfamiliar with the task , or any of the set up , we get a more realistic but in terms of specifying the scenario , we 've gotten a little further but we wanted to until we know who is the wizard , and have the wizard partake in the ultimate definition probe . if on friday it turns out that she really likes it and we really like her , then nothing should stop us from sitting down next week and getting all the details completely figured out . bu e i 'm even this tango , enter , vista is itself , an ad hoc scenario . the basic u idea behind the data collection was the following . the data we get from munich is very command line , not a rich language . we wanted just to collect data , to get that elicits more , that elicits richer language . and we actually did not want to constrain it too much , this is a parallel track , it may get us some more information on the human machine pragmatics , that no one knows anything about , as of yesterday . now we should have at least one navigational task with explicit not ex it 's implicit that the person wants to enter , and some task where it 's more or less explicit that we can label it . ","The structure of the belief-net comprises, firstly, a feature layer, which includes linguistic, discourse and world knowledge information that can be gleaned from the data. It is possible for these variables to form thematic clusters( eg ""entrance"", ""type of object"", ""verb""), each one with a separate middle layer. These feed, in turn, into the main middle layer, that defines more general hidden variables, such as the tourist/business status of the user. The feature layer can end up being cue-based, while the middle layers task-based. However, there has been progress in the design and organisation of experiments, that will eventually provide data more useful and appropriate for this task. "
Bed003.B,"then the features we decided or we decided we were talked about , the prosody , the discourse , verb choice . we had a list of things like "" to go "" and "" to visit "" and what not . the "" landmark iness "" of "" walls "" which we can look up the context , which in this case we 've limited to "" business person "" , "" tourist "" , or "" unknown "" , the time of day , our initial idea was not very satisfying , because our initial idea was all the features pointing to the output node . reasons being , it 'd be a pain to set up all the probabilities for that . if we moved onto the next step and did learning of some sort , according bhaskara we 'd be handicapped . then our next idea was to add a middle layer , but the middle thing , we were thinking along the lines of trying to figure out the concept of whether they 're a tourist or whether they 're running an errand like that yes , we could things we couldn't extract the from the data , the hidden variables . then the hidden variables hair variables we came up with were whether someone was on a tour , running an errand , or whether they were in a hurry , right now it 's still in a toy version of it , because we didn't know the probabilities of or the mode has three different outputs . the probability whether the probability of a vista , tango , or enter . the "" context "" , we simplified . it 's just the businessman , the tourist , unknown . mainly because it 's just whether the verb is a tango verb , an enter verb , or a vista verb . we kept things from directly affecting the mode beyond the concept , but you could see perhaps discus the "" admission fee "" going directly to the mode pointing at "" enter "" , but we just decided to keep all the things we extracted to point at the middle and then down . the reason i say the demo doesn't work very is yesterday we observed everything in favor of taking a tour , and it came up as "" tango "" , once we look at the data more we 'll get more hidden nodes , whether it 's a public building , and whether it 's actually has a door . explain to me why it 's necessary to distinguish between whether something has a door and is not public . you could affect theoretically you could affect "" doing business "" with "" has door "" . i didn't think it did learning . it did a little bit of learning , but actually it had an interface . a lot of them were like , the ordering isn't very clear on know there is an i was looking on the we web page and he 's updated it for an xml version of bayes nets . th you can either you ca or you can read both . that would all f funnel into one node that would constitute entrance requirements like that . the fact that the there 's a complete separation between the observed features and in the output . the "" discourse admission fee "" node seems like it should point directly to the or increase the probability of "" enter directly "" versus "" going there via tourist "" . the discourse refers to "" admission fee "" but it just turns out that they change their mind in the middle of the discourse . what discourse processing is are the how much is built into smartkom and ","The structure of the belief-net comprises, firstly, a feature layer, which includes linguistic, discourse and world knowledge information that can be gleaned from the data. These feed, in turn, into the main middle layer, that defines more general hidden variables, such as the tourist/business status of the user. The group discussed the first version of the Bayes-net used to work out a user's intentions when asking for directions from a navigation device. The latter determine the final probability of each intention in the output layer. Three intentions were identified: Vista (to view), Enter (to visit) and Tango (to approach). This first model of the belief-net was built in JavaBayes, since it is a free package, has a graphical interface, and it can take XML files as input. "
Bed003.C,"it 's talks about it just refers to the fact that one of main things we had to do was to decide what the intermediate nodes were , if you have n features , then it 's two to the n or exponential in n . that 's that needs a lot of work . but the other ones , the final destination , the whether they 're doing business , whether they 're in a hurry , and whether they 're tourists , that thing is all probabilistically depends on the other things . like we don't have nodes for "" discourse "" and "" parse "" , although like in some sense they are parts of this belief net . but the idea is that we just extract those features from them , we don't actually have a node for the entire parse , we looked at the data and in a lot of data people were saying things like "" can i get to this place ? "" "" what is the admission fee ? "" . that 's like a huge clue that they 're trying to enter the place rather than to tango or vista , normally context will include a huge amount of information , but we are just using the particular part of the context which consists of the switch that they flick to indicate whether they 're a tourist or not , similarly prosody is not all of prosody but simply for our purposes whether or not they appear tense or relaxed .  we can do a demo in the sense that we can just ob observe the fact that this will , do inference . we can , set some of the nodes and then try to find the probability of other nodes . that 's just to do with our probabilities . like , we hand tuned the probabilities , it 's more like "" are you are tourist ? that 's a different thing . if the context were to set one way or another , that like strongly says something about whether or not they 're tourists . the issue is that in belief nets , it 's not common to do what we did of like having , a d bunch of values and then "" unknown "" as an actual value . what 's common is you just like don't observe the variable , and then just marginalizes but we didn't do this because we felt that there 'd we were thinking in terms of a switch that actually you want to have a node for like whether or not it can be entered ? i don't know if javabayes is about that . it might be that if you add a new thing pointing to a variable , you just like it just overwrites everything . what would be if it is if it just like kept the old function for either value in a way this is a lot of good features in java it 's cra has a gui and it 's but it 's free . but its interface is not the greatest . there is actually a text file that you can edit . theoretically you could edit that . but they 're not very friendly . we can write an interface th for entering probability distributions easily , something like a little script . i don't know if it actually manipulate the source , though . that might be a bit complicated . it might be simpler to just have a script that , essentially a lot of those nodes can be expanded into little bayes nets of their own . the probabilities and all are completely ad hoc . another thing to do , is also to , guess to ask around people about other bayes net packages . just figured it has to be someone who 's , familiar enough with the data to problems for the wizard , we can , see if they 're good . ","These feed, in turn, into the main middle layer, that defines more general hidden variables, such as the tourist/business status of the user. At this stage, all the actual probabilities are ad-hoc and hand-coded. This first model of the belief-net was built in JavaBayes, since it is a free package, has a graphical interface, and it can take XML files as input. "
Bed003.D,"there 's landmark for touristic reasons and landmark for navigational reasons but you can imagine wanting the oth both kinds of things there for different goals . there are certain cues that are very strong either lexical or topic based concept cues and then in that second row or whatever that row of time of day through that all of those some of them come from the utterance and some of them are either world knowledge or situational things . this will happen when we think more about the kinds of verbs that are used in each cases but you can imagine that it 's verb plus various other things that are also not in the bottom layer that would help you are "" doing business "" versus "" tourist "" they refer to your current task . that was directly given by the context switch . the "" tourists "" node should be very consistent with the context node . it get into plan recognition kinds of things in the discourse . that 's a whole set of discourse related cues to your middle layer . one thing that might be helpful which is implicit in the use of "" admission fee discussion "" as a cue for entry , is thinking about the plans that various people might have . they 're in non in more traditional ai kinds of plan recognition things you have some idea at each turn of agent doing something , tha that structure that robert drew on the board was like more cue type based , and then some of the things we 're talking about here are more entering or som like they might be more task based . is it a experimental setup for the data collection ready determined ? like we wanna be able to collect as much of the variables that are needed for that , ","The feature layer can end up being cue-based, while the middle layers task-based. The structure of the belief-net comprises, firstly, a feature layer, which includes linguistic, discourse and world knowledge information that can be gleaned from the data. "
Bed005.A,it 's mystery functions . ,
Bed005.B,"what we think is gonna happen is that , in parallel starting about now we 're gonna get fey to , where you 're working with me and robert , draft a note that we 're gonna send out to various cogsci c and other classes saying , "" here 's an opportunity to be a subject . we 're looking for a total of fifty people , not necessarily by any means all students in parallel with that , we 're gonna need to actually do the script . but what i 'd like to do , if it 's o k , is to s to , as i say , start the recruiting in parallel and possibly start running subjects next week . this the permission form . and we 're just gonna use it as it is , what i 'd like to do is also have our subjects sign a waiver saying "" i don't want to see the final transcript "" . there were other actions that s seemed to step state variables somewhere , i definitely think it 's worth the exercise of trying to actually add something that isn't there . these are your friends back at eml . it seemed to me , what we ought to do is get our story together . and think about it some , internally , before asking them to make changes . the problem isn't the short ra range optimization . what are the thl class of things we think we might try to do in a year or two ? and what do we want to request now that 's leave enough space to do all that this and this , was just the action end . at some point we 're going to have to worry about the language end . this is everything that , we might want to do in the next couple years . it 's beyond source path goal , it seems to me we can get all the complexity we want in actions and in language without going outside of tourists in heidelberg . at least unless somebody else wants t to suggest otherwise the general domain we don't have t to broaden . that is , tourists in heidelberg . and , again , this is li in the databa this is also pretty formed because there is an ontology , and the database , and all the entities do have concrete reference . although th the to get at them from a language may not be trivial . let 's say that by the end of spring break , i 'll try to come up with some general story about , construction grammar , and what constructions we 'd use and how all this might fit together . there 's this whole framework problem that i 'm feeling really uncomfortable about . is in this over arching story we worked it out for th as you say , this the storytelling scenario . and it 's really worth thinking through what it looks like . what is the simspec mean , et cetera . once we have fulfilled these requirements , and we can w we can do all sorts of things that don't fit into their framework if we want to turn it into u understan standing stories about heidelberg , we can do that . the there 's two packages there 's a , quote parser , there 's a particular piece of this big system , which , in german , takes these t sentence templates and produces xml structures . and one of our jobs was to make the english equivalent of that . that , these guys did in a day . the other thing is , at the other end , roughly at the same level , there 's something that takes , x m l structures , produces an output xml structure which is instructions for the generator . and then there 's a language generator , and then after that a s a synthesizer that goes from an xml structure to , language generation , to actual specifications for a synthesizer . but again , there 's one module in which there 's one piece that we have to convert to english . this is the heckerman paper you 're working with ? the important point is that there is a general idea of shortcutting the full cpt . th c the full conditional probability table with some function . if ba javabayes won't do it for you , the f the bayes nets in general are quite good at saying , "" if you have no current information about this variable just take the prior for that . "" if you don't have any information about the discourse , you just use your priors of whatever the discourse like the noisy or function , really is one that 's essentially says , take the max . and , i thi that 's the standard way people get around the there are ways of breaking this up into s to subnets and like that . we definitely it 's a great idea tha to pursue that . which is the hierarchy that s comes with the ontology is just what you want for this . you may or then you 'd have this little vector of , approach mode or eva mode . let 's we have the eva vector for various kinds of landmarks . if it for a specific landmark you put it there . if you don't , you just go up the hierarchy to the first place you find one . but in any case i view it logically as being in the ontology . it 's part of what about a an object , is its eva vector . that 's a very pretty relationship between these local vectors and the ontology . we 're gonna need some way to either get a p tag in the ontology , or add fields , ","The data collection running in parallel with the project can start shortly with recruiting subjects. The parser's output modifies the XML used by the system to initiate actions and generate responses. As the project evolves, further enrichment of the ontology (actions, linguistic features) will be necessary. Meanwhile, the german parser now works with english sentences. Noisy-OR's can help avoid this by simplifying the probability tables and applying a deterministic function to produce their complete version. Similarly, object representations will include an EVA vector. This can be incorporated in the database entry for a particular building or inherited from the ontology of the building type. "
Bed005.B,"if it 's that type of thing , and we want its eva vector , pppt ! it 's that . "" and then but , the combination functions , and whether we can put those in java bayes , and all that is , is the bigger deal . that 's the question is "" to what extent does it allow us to put in these g functions ? "" you and i should talk about it . ",This can be incorporated in the database entry for a particular building or inherited from the ontology of the building type. 
Bed005.C,"which is we gonna check out our social infrastructures for possible subjects . except that with munich everything is fine now . they 're gonna transcribe . they 're also gonna translate the , german data from the tv and cinema for andreas . and , and now it 's we have a complete english parser that does everything the german parser does . and , the reason was that the parser i c completely ignores the verb . these are the the ten different sentence types that the the parser was able to do . and it still is , now in english .  and the value of the score is , v i assume , the more of these optional things that are actually in there , the higher the r score it is . let 's hope that the generation will not be more difficult , and the next thing i would like to be able to do , and it seems like this would not be too difficult either , is to say , "" let 's now pretend we actually wanted to not only change the mapping of , words to the m three l but we also wanted to change add a new sentence type and make up some new m three l s "" we 'll find that out . get a complete understanding of the whole thing . i got the , m three l for the routes today . you have a route , and you cut it up in different pieces . and every element of that e r f of that every segment we call a "" route element "" . and from a to b we cut up in three different steps , and every step has a "" from object "" where you start , a "" to object "" where y where you end , and some points of interest along the way . and i suggested that they should n be k kind enough to do s two things for us , is one , also allocating , some tags for our action schema enter vista approach , and the approach mode , anyhow , is the default . that 's all they do it these days . wherever you 'll find a route planner it n does nothing but get to the closest point where the street network is at minimal distance to the geometric center . now , we hav the whole unfortunately , the whole database is , in german . we have just commissioned someone to translate some bits of it , it 's a relational database with persons , events , and , objects . the reason is , given the craw bet the projects that all carry their own taxonomy and , on all history , they 're really trying to build one top level ontology ft that covers all the eml projects , but , nevertheless , it 's going to be there by n by , next monday what i don't think is ever going to be in the ontology , is the likelihood of , people entering r town halls , and looking at town halls , and approaching town halls , especially since we are b dealing with a case based , not an instance based ontology . and , since the d decision was on types , on a d simply type based , we now have to hook it up to instances . and i 'll think s through this , getting eva vectors dynamically out of ontologies one more time ","Meanwhile, the german parser now works with english sentences. The XML for Map requests also comprise a route, route elements and points of interest along the way. It is at this level that Enter/Vista/Approach tags will be added as action modes. "
Bed005.D,"and , you can have i many variations in those sentences , they will still parse fine . one thing i was wondering , was , those functions there , are those things that modify the m three l one thing i was wondering was , those percentage signs , right ? those functions "" action "" , "" goodbye "" , and on , right ? are they present in the code for the parser ? each of those functions act on the current xml structure , and change it in some way , by adding a l a field to it , they 're defined somewhere , presumably . recall that , we want to have this structure in our bayes nets . the typical example is that , these are all a bunch of cues for something , and this is a certain effect that we 'd like to conclude . enter , v view , approach , right ? given n nodes , and furthermore , the fact that there 's three things here , we need to specify "" three times "" , "" two to the n "" probabilities . and , that 's a lot of probabilities to put here , which is pain . noisy ors are a way to , deal with this . if we don't really know if landmark or not , or , i if that just doesn't seem relevant , then that would be th the disting the distinguish state .  the idea is that , each of these ei is represents what this would be if all the other ones were in the distinguish state . right ? if it is a landmark , and no none of the other things really ap applicable , then this would represent the probability distribution . we come up with these l little tables for each of those and the final thing is that , this is a deterministic function of these , the what we want , is javabayes to support deterministic , functions . i don't see why the , combining f functions have to be directly hacked into they 're used to create tables we can just make our own little functions that create tables in xml . ",The parser's output modifies the XML used by the system to initiate actions and generate responses. These elements will constitute only a small part of the inputs of the Bayes-net that determines the action mode. The actual number of the inputs can create a combinatorial explosion when setting the probabilities. Noisy-OR's can help avoid this by simplifying the probability tables and applying a deterministic function to produce their complete version. 
Bed005.E,"it really is key word matching , ",
Bed005.F,"what is the basic thing that you are , obligated to do , by the summer before w c we can move ",
Bed006.A,"the idea is to try to get the actual phrasing that they might use and try to interfere as little as possible with their choice of words . u the one experiment th that i 've read somewhere , it was they u used pictures . you have to be careful with that thing because mean many actions presuppose some almost infinitely many other actions . ",
Bed006.B,"and then those actions can be in multiple categories at the same time if necessary . what this is that there 's an interface between what we are doing and the action planner and right now the way the interface is "" action go "" and then they have the what the person claimed was the source and the person claimed as the goal passed on . and the problem is , is that the current system does not distinguish between goes of type "" going into "" , goes of type "" want to go to a place where take a picture of "" , et cetera . instead of just making it an attribute and which is just one thing we decided to make it 's own entity that we could explode it out later on in case there is some structure that we need to exploit . right , the roles will be filled in with the schema and then what actual a action is chosen is will be in the action schema section . inside of enter there will be roles that can be filled if i want to go from outside to inside then you 'd have the roles that need to filled , where you 'd have a source path goal set of roles . if you wanted to have a new type of action you 'd create a new type of category . ","The model also allows for multiple action schemas to be triggered in parallel. These modes will form categories of complete XML schemas with information filled in from the language understanding in a more elaborate way than the current Object-""Go Action""-Object model. Categories and action schemas can have -in theory- any number of blocks depending on the expansion of the domain. "
Bed006.C,"we 're about to collect data and we have a little description of asking peop subjects to contact fey for recruiting them for our thing however there is always more people in a facul in a department than are just taking his class or anybody else 's class at the moment and then , secondly , we had , you may remember , the problem with the re phrasing , that subject always re phrase the task that we gave them , there the idea is now that next actually we need to hire one more person to actually do that job the idea now is to come up with a high level of abstract tasks "" go shopping "" out of these f s these high level categories the subject can pick a couple , make up their own itinerary a and tasks the person is able to take notes on a map that we will give him and the map will be a tourist 's schematic representation with symbols for the objects . hopefully . and then we 're going to have another we 're gonna have w another trial run talk to a machine and it breaks down and then the human comes on . the question is just how do we get the tasks in their head that they have an intention of doing something and have a need to ask the system for something without giving them clear wording or phrasing of the task . however , not only was the common census were among all participants of friday 's meeting was it 's gonna be very laborious to make these drawings for each different things , all of a sudden we 'll get descriptions of pictures in there . let me make one more general remark , has two side actions , its action items that we 're do dealing with , one is modifying the smartkom parser and the other one is modifying the smartkom natural language generation module . and do have some good news for the natural language generation however . meaning that tilman becker , who does the german one , actually took out some time and already did it in english for us . it 's the last week of april until the fourth of may no it 's just they 're coming for us that we can bug them and ask them more questions and they can give some talks and the basic requirement is fulfilled almost . when andreas stolcke and his gang , when they have changed the language model of the recognizer and the dictionary , then we can actually a put it all together you can speak into it and ask for tv and movie information if something actually happens and some answers come out , then we 're done . then on to the modeling . the idea is , imagine we have a library of schema such as the source path goal and then we have forced motion , and there 's gonna be s a lot of on the goal and blah blah , that a goal can be and forth . we 're talking more on the intention level , up there , and more on the this is the your basic bone schema , down there . in the future though , the content of a hypothesis will not only be an object and an action and a domain object but an action , a domain object , and a rich action description , a and the answer is meaning we can reference . and link it to another one , and this not only within a document but also via documents , personally , i 'm looking even more forward to the day when we 're going to have x forms , which l is a form of notation where it allows you to say that if the spg action up there is enter , then the goal type can never be a statue .  it would be considered valid if we have an spg action "" enter "" and no spg schema , but a forced action schema . it 's crucially necessary , is that we can have multiple schemas and multiple action schemas in parallel . we 're gonna hit a lot of interesting problems should have we should have added an ano an xml example , and this is on a on my list of things until next week . it 's also a question of the recursiveness and a hier hierarchy in there . and i agree that this is something we need to discuss , we will not end this discussion anytime soon . and it 's gonna get more and more complex the l complexer and larger our domains get . this is a schema that defines xml messages that are passed from one module to another , mainly meaning from the natural language understanding , or from the deep language understanding to the action planner . now the reason for not using this approach is because you always will have to go back , each module will try have to go back to look up which entity can have which entity can have which parents , you always need the whole body of y your model to figure out what belongs to what . or you always send it along with it , we i will promise for the next time to have fleshed out n xml examples for a run through and see how this then translates , but the principal distinction between having the pure schema and their instantiations on the one hand , and adding some whatever , more intention oriented specification on parallel to that this approach seems to be workable to me . meeting time rescheduling . and if you can get that binding point also with a example that would be helpful for johno and me . ","Another trial run will take place, while a call to recruit subjects is being emailed to students. Meanwhile, the translation of the TV and cinema information system to english is almost complete. On the other hand, there was a presentation of the model that offers more elaborate action planning for SmartKom, of which Enter/View/Approach (EVA) modes are a part. These categories will, in turn, be linked with action schemas, one of which is Source-Path-Goal (SPG). Categories and action schemas can have -in theory- any number of blocks depending on the expansion of the domain. The notation provides for linking and referencing between different schemas. The model also allows for multiple action schemas to be triggered in parallel. However, the structure of the model is open for discussion, since its use was to elicit discussion and highlight issues. "
Bed006.C,"but it 's it for me it seems to be conceptually important that we find out if we can s if there are things in there that are general nature , we should distill them out and put them where the schemas are . ",
Bed006.D,"it 's a tourist information web site , ",
Bed006.E,"what are some types of action schemas ? one of the types of action schemas is source path goal action . and what are some types of that ? and an enter , a view , an approach . ","These categories will, in turn, be linked with action schemas, one of which is Source-Path-Goal (SPG). "
Bed006.F,"s like are you gonna have similar schemas for fm like forced motion and caused action and like you have for spg ? mean clearly there 's talk about the the parser changes on friday at least , ",
Bed006.G,"you 'll have those say somewhere what their intention was they 'll have a little bit more natural interaction ? but if it is , then the top block is like and it has about that specific to entering or viewing or approaching , and you can also describe them in a general way as source path goal schema all of those have either specific frame specific roles or more general frame specific roles that might have binding . it 's somewhere in there that you need to represent that there is some container and the interior of it corresponds to some part of the source path goal goal goal in this case . you could have a flat structure and just say these are two independent things , but there 's also this like causal , one is really facilitating the other and it 's part of a compound action of some kind , which has structure . there 's like levels of granularity . there 's a bit a redundancy , which is why i would think you would say enter and then just say all the things that are relevant specifically to enter . and then the things that are abstract will be in the abstract things as and that 's why the bindings become useful . it 's just like a frame hierarchy , like unless @ @ are there reasons why one is better than the other that come from other sources ? i n didn't you say something about friday , in general they 'll be bindings across both intentions and the actions . are the sample data that you guys showed sometime ago cuz it 'd be for me to like look if i 'm thinking about examples ","The data collection script has been slightly modified, so that it encourages more natural dialogue between the subjects and the ""wizard"". "
Bed008.A,we 're deriving this the this feature of whether the main action at this place happens inside or outside or what we 're deriving that from what activity is done there ? and one might be f fairly pleased with getting a really good analysis of five ten in a summer know we 're going for rough and ready . ,
Bed008.B,"we were gonna do two things one of which is just lay out the influence structure of what we think influences what but du we should have all of the basic design of what influences what done before we decide exactly how to compute it . you could have all those values for go there or you could have go there be binary and given that you 're going there when . this is actually a gui to a simulator that will if we tell it all the right things we 'll wind up with a functioning belief net at the other end . it 's you took some actions , you spent money and and if it does influence anything then you 're gonna need something that converts from the number here to something that 's relevant to the decision there . that was the of the endpoint ? the way we had been designing this , there were three intermediate nodes which were the endpoint decision as seen from the user model as seen from the ontology and as seen from the discourse . there was a decision with the same three outcomes based on the th those three separate considerations but the they 're undoubtedly gonna be more things to worry about . my advice to do is get this down to what we think is actually likely to be a strong influence . that the belief net itself has properties and the properties are filled in from on ontology items . that there 'd be certain properties that would fit into the decision node and then again as part of the ou outer controlling conditioning of this thing those would be set , that some somehow someone would find this word , look it up in the ontology , pull out these properties , put it into the belief net , and then the decision would flow . but for this purpose one of these places is quite like the other . the idea would be that you might wanna merge those three for this decision it 's just true or false that if any of those things is true then it 's the place that you are more likely to enter . what does this look like , what are intermediate things that are worth computing , what are the features we need in order to make all these decisions and what 's the best way to organize this that it 's clean and consistent and all that and we assume that some of these properties would come indirectly through an ontology , but then we had this third idea of input from the discourse . you could have a node that 's that was a measure of the match between the object 's feature , the match between the object the entity , i 'm and the user . cuz then we get into huge combinatorics and like that we 're gonna have to somehow figure out some way to encapsulate that if there 's some general notion of the relation to the time to do this to the amount of time the guy has like that is the compatibility with his current state , one is the technical one that you don't wind up with such big exponential cbt 's , the other is it can be it presumably can be used for multiple decisions . anyway th in general this is the design , this is really design problem . but that we we had some things that and anyway we 're gonna have to find some way to cl get this sufficiently simple to make it feasible . clearly there 's more work to be done on this but it 's gonna be more instructive to think about other decisions that we need to make in path land . in terms of the planner what 's a good one to do ? the fir see the first thing is , getting back to thing we left out of the other is the actual discourse . because we 're gonna want to know which constructions indicate various of these properties we 're gonna wind up pulling out discourse properties like we have object properties and we don't they are yet . for go there , probably is true and false , let 's say . and they 'll be a y a user go there really important in the belief worl net world not to have loops what 's going to happen is that eventually they 'll be some system which is able to take the discourse in context and have outputs that can feed the rest of belief net . and now some of those will get fancier like mode of transportation and that you 'd have to do see in order to do reference and like that you 've gotta have both the current discourse and the context to say i wanna go back there , s we 're gonna have to use some t care in the knowledge engineering to not have this explode . we just have to figure out what the regularities are and code them . i don't know how easy it is to do this in the interface but you it would be great if you could actually just display at a given time all the things that you pick up , you click on "" endpoint "" , and everything else fades and you just see the links that are relevant to that . s this assumes symmetry and equal weights and all this things , which may or may not be a good assumption , ","Details of how different inputs feed into them were discussed at length. Its structure was discussed during the meeting. There are several endpoints (User, Ontology, Discourse etc) with separate EVA (Enter/View/Approach) values. Ideas mentioned included grouping features of buildings like ""selling"", ""fixing"" and ""exhibiting"", as well as creating a User-compatibility node that would take different values depending on the situation and the user status. Similarly, a Go-there (towards a building) node can be influenced by things like the user's budget and discourse parameters amongst other things. The study of the linguistic constructions that people use in this kind of navigational domain is expected to be prove useful in that respect. The latter are still ill-defined at this stage. "
Bed008.C,"if the person talking is angry or slurs their speech they might be tired or , it seems like everything in a user model a affects just seems like it 'd push the problem back a level . it 's hard for me to imagine how everything wouldn't just contribute to user state again . w i was just , if you wanted to pay attention to more than one you could pass a w a weighting s system though too , the situations that h has , are they built into the net they could either be hand coded or learned or based on training data , in terms of java base it 's what you see is what you get in i would be surprised if it supports anything more than what we have right here . ","There are several approaches ranging from simply averaging the inputs to using a hidden variable in order to weight them differently depending on context. If the latter architecture is used, the net could -to an extent- be trained with the data that is currently being collected. "
Bed008.D,"the other thing is that every time that 's updated beliefs will have to be propagated but then the question is do you do we wanna propagate beliefs every single time it 's updated or only when we need to ? was thinking less likely to view what ex and either those is true f or false ? no but , it 's more than that , situation go there , see i 'm thinking that any node that begins with "" go there "" is either gonna be true or false . it might soon , if this is gonna be used in a serious way like java base then it might soon be necessary to start modifying it for our purposes . recall the basic problem which is that you have a belief net and you have like a lot of different nodes all contributing to one node . as we discussed specifying this thing is a big pain what helps us in our situation is that these all have values in the same set , these are all like saying ev or a , we wanna view each of these as experts ea who are each of them is making a decision based on some factors and we wanna combine their decisions and create sorta weighted combination . the problem is to specify the the conditional property of this given all those , like each node given its parents , what guess , what jerry suggested earlier was that we , view these guys as voting and we just take the we essentially take averages , step two is we don't wanna like give them all equal weight you 'd compute the weighted average , the next possibility is that we 've given just a single weight to each expert , whereas it might be the case that in certain situations one of the experts is more reliable and in certain situations the other expert is more reliable . you have a new thing called "" h "" , this is a hidden variable . and what it does is it decides which of the experts is to be trusted in this particular situation . produces some it produces a number , either one , two , three , or four , in our situation , mean we need data with people intentions , like , are we able to get these nodes from the data ? ","As each node in the tree is the decision point of the combination of its parent nodes, which rules govern this combination is an important issue. There are several endpoints (User, Ontology, Discourse etc) with separate EVA (Enter/View/Approach) values. There are several approaches ranging from simply averaging the inputs to using a hidden variable in order to weight them differently depending on context. "
Bed008.E,"go there in the first place or not is definitely one of the basic ones . everything that has user comes from the user model everything that has situation comes from the situation model a . here is the we had that the user 's budget may influence the outcome of decisions . the finance is here thought of as the financial policy a person carries out in his life , this was eva . if it 's fixing things selling things , or servicing things we have the user interest is a vector of five hundred values , go there or not is a good one . ","Similarly, a Go-there (towards a building) node can be influenced by things like the user's budget and discourse parameters amongst other things. Ideas mentioned included grouping features of buildings like ""selling"", ""fixing"" and ""exhibiting"", as well as creating a User-compatibility node that would take different values depending on the situation and the user status. "
Bed010.A,,
Bed010.B,"how do you go about this process of deciding what these connections are ? cuz i 'm still itching to look at what look at the and see what people are saying . because i went to a linguistics colloquium on the fictive motion seems to me that will fairly be of relevance to to what we 're doing here be of use to someone who 's trying to do this , i 'm actually probably going to be in contact with her pretty soon anyway because of various of us students were going to have a reading group about precisely that thing over the summer , ",
Bed010.C,"we are committed for our funding . n no , to just get the dem get the demos they need . if it turns out we can also give them lots more than that by , tapping into other things we do , that 's great . th the demo requirements for this fall are taken care of as of later this week and then it 's probably fifteen months until there 's another serious demo requirement . the idea is there 's this other subgroup that 's worrying about formalizing the nota getting a notation . but in parallel with that , the hope is tha in particularly you will work on constructions in english ge and german for this domain , but y not worry about parsing them or fitting them into smartkom or any of the other anything lik any other constraints for the time being . 'd like to , for the summer turn into science mode . c sh we could set that up as actually an institute wide thing ? there are a lot of issues , what 's the ontology look like , what do the constructions look like , what 's the execution engine look like , but , more focused on an idealized version than just getting the demo out . and one of the things we need to do is the and this is relatively tight tightly constrained , is to finish up this belief net and we went through this , and , more or less convinced ourselves that at least the vast majority of the nodes that we needed for the demo level we were thinking of , were in there . bhaskara and i went off and looked at some technical questions about were certain operations legitimate belief net computations and was there some known problem with them or had someone already solved how to do this and the answer seems to be "" no , no one has done it , but yes it 's a perfectly reasonable thing to do if that 's what you set out to do "" . and there 're two aspects to it , one of which is , technical , getting the coding right , and making it run , and like that . and the other is the actual semantics . what all what are the considerations and how and what are the ways in which they relate . we do in the long run wanna do better visualization and all that one is you design and the other is you learn . what we 're gonna do initially is do design , and , i if you will , guess . if it 's done right , and if you have data then , there are techniques for learning the numbers given the structure but for the limited amount of we have for this particular exercise we 'll just design it . there something that i didn't know until about a week ago or is there are separate brain areas for things within reach , and things that are out of reach . in addition to e ego and allocentric which appear all over the place , you also have this proximal distal thing which is very deeply embedded . and these issues about reference , and spatial reference , discourse reference , all this deixis which is part of what you were talking about , we gotta do all this . and then there 's also a set of system things that come up . we 're not using their system . that means we need our system . "" and in addition to the business about just getting the linguistics right , and the formalism and we 're actually gonna build something and we 're gonna start on that in parallel with the the grammar but to do that we 're gonna need to make some decisions like ontology , i does either the smartkom project or one of the projects at eml have something that we can just p pull out , for that . cuz we 're not only going the plan is not only to lay out this thing , but to actually build some of it . it looks like we 're now in a position that the construction analyzer that we want for this applied project can be the same as the construction analyzer that nancy needs for the child language modeling . to come full circle on that , this formalization task , is trying to get the formalism into a shape where it can actually and , while we 're at this level , there 's at least one new doctoral student in computer science who will be joining the project , either next week or the first of august , depending on the blandishments of microsoft . and actually i talked today to a undergraduate who wants to do an honors thesis on this . there 's yet another one of the incoming first year graduate students who 's expressed interest , as far as this group goes , it 's certainly worth continuing for the next few weeks to get closure on the belief net and the ideas that are involved in that , and what are th what are the concepts . and anyway we c we can m undoubtedly get ami to give a talk at eml like that . while he 's in a lot of interest . actually , either place , dfki or how about if you two guys between now and next week come up with something that is partially proposal , and partially questions , ","This is the first of two working demos required for the project. In parallel, another team is working on formalisation and notation. Further than that, there are no restrictions on the focus of the research or its possible applications. The majority of the nodes are already there. This leaves the dependencies between them and the rules of computation to be set. The latter is also reflected in neuro-physiological data. The variety of linguistic conventions seem to develop around an ego/allo-centric and a proximal/distal paradigm. For example, issues like spatial descriptions could be investigated. Since the whole system is going to be re-designed, there are major decisions to be taken regarding the parser and the ontology, as well as what can be re-used from past EML projects. Finally, more ideas are expected to come from students and their research. From an engineering perspective, the belief-net for the AVE task should be completed within a few weeks. "
Bed010.D,"why i had i did need to chan generate different trees than the german ones , i did look into that , in terms of , exploding the nodes out and down ag javabayes does not support that . it 'd probably take two weeks or to actually go through and do it , ",
Bed010.E,"the we got to the point where we can now speak into the smartkom system , and it 'll go all the way through and then say something like "" roman numeral one , am smarticus . "" which means it 's just using a german sythesis module for english sentences . and "" concept to speech "" is feeding into this synthesis module giving it what needs to be said , and the whole syntactic structure it can pronounce things better , presumably . then , just with text to speech . and did write the tree adjoining grammar for some sentences . but that that out of the twelve possible utterances that the german system can do , we 've already written the syntax trees for three or four . right now it 's brittle and you need to ch start it up and then make ts twenty changes on seventeen modules before they actually can stomach it , anything . because it 's designed for this seevit thing , where you have the gestural recognition running with this s siemens virtual touch screen , which we don't have here . but it 's working now , we can even make a an internal demo ,  we do wanna have all the bugs out b where you have to pipe in extra xml messages from left and right before you 're it was just amazing to see how instable the whole thing is , and i g i got the feeling that we are the only ones right now who have a running system . e the version that is , the full version that 's on the server d does not work . and part of my responsibility is to use our internal "" group ware "" server at eml , make that open to all of us and them , that whatever we discuss in terms of parsing and generating and constructions w we put it in there and they put what they do in there and we can even get some overlap , get some synergy out of that . because the tree adjoining grammars that tilman is using is as you said nothing but a mathematical formalism . and you can just do anything with it , whether it 's syntactic trees , h p s g like or whether it 's construction . if you ever get to the generation side of constructing things and there might be something of interest there , i also s would suggest not to d spend two weeks in changing the javabayes code . i will send you a pointer to a java applet that does that , but that 's th but we have data in english and german already . transcribed . i will send you that . and there is a huge project on spatial descriptions differences in spatial descriptions . it 's kleist . carroll , ninety three . i there is a study on the differences between english and german on exactly that problem . also give you a pointer to a paper of mine which is the ultimate taxonomy of reference frames . on this scale , you have it either be ego or allocentric . it 's called "" an anatomy of a spatial description "" . ","The system is still buggy and unstable, but it will soon be ready for a demonstration. The variety of linguistic conventions seem to develop around an ego/allo-centric and a proximal/distal paradigm. "
Bed010.F,,
Bed010.G,the one that people seem to use is hugin or whatever ? ,
Bed011.A,,
Bed011.B,"if our goal is to really be able to handle a whole bunch of different then throwing harder situations at people will get them to do more linguistic more interesting linguistic and i looked at one of them which was about ten sentences , found fifteen , twenty different construction types that we would have to look for and on you 're talking about rather than having the user decide this you 're supposed t we 're supposed to figure it out ? those files that you sent me are the user side of some interaction with fey ? and i me it would be completely out of the question to really do more than , say don't know , ten , over the summer , ",
Bed011.C,"what i 've tried to do here is list all the decision nodes that we have identified on this side . commented and what they 're about and the properties we may give them . and here are the tasks to be implemented via our data collection . and these are the data tasks where w we can assume the person would like to enter , view or just approach the thing . there 's a lot of things where we have no analogous tasks , and that may or may not be a problem . we can change the tasks slightly if we feel that we should have data for e for every decision node trying to im implant the intention of going to a place now , going to a place later on the same tour , we were gonna put this in front of people . then they will read task where lots of german words are thrown in between . and and they have to read isolated proper names and and then they gonna have to f choose from one of these tasks , which are listed here . six different things they think they would do if they were in heidelberg or traveling someplace and and they have a map . very sketchy , simplified map . and then they call this computer system that works perfectly , and understands everything . and and then after three tasks the system breaks down . and fey comes on the phone as a human operator . and and fey has some thirty subjects lined up ? and we 're still l looking for a room on the sixth floor because they stole away that conference room . behind our backs . david and jane and lila are working on that as we speak . we get to the belief net just focusing on the g go there node . and what w what happened is that design wise i 'd noticed that we can we still get a lot of errors from a lot of points to one of these sub go there user go there situation nodes . came up with a couple of additional nodes here where can now draw straight lines from these to here , meaning it g goes where the sub s everything that comes from situation , everything that comes from user goes with the sub u , and whatever we specify for the called "" keith node "" , or the discourse , what comes from the parser , construction parser , will contribute to the d and the ontology to the sub o node . and one just s has to watch which also final decision node it doesn't make sense t to figure out whether he wants to enter , view or approach an object if he never wants to go there in the first place . and and for now the question is "" how much of these decisions do we want to build in explicitly into our data collection ? "" suggest we make some fine tuning of these , get run through ten or subjects and see whether we wanna make it more complex or not , depending on what results we 're getting . this means audio , but it 's actually like five minutes dialogue . go there is a yes or no . i 'm also interested in th in this "" property "" line here , timing was have these three . now , later on the same tour , sometimes on the next tour . the reason why do we go there in the first place ie it 's either for sightseeing , for meeting people , for running errands , or doing business . "" mode "" , "" drive there "" , "" walk there "" or "" be driven "" , which means bus , taxi , bart . "" length "" is you wanna get this over with as fast as possible , th the user can always s say it , but it 's just we hand over these parameters if we make if we have a feeling that they are important . and that we can actually infer them to a significant de degree , or we ask . and , timing , length would definitely be part of it , "" costs "" , "" little money , some money , lots of money "" ? object information "" , "" do i wanna know anything about that object ? "" and . if i care about it being open , accessible or not , i don't think there 's any middle ground there . i will tell you the german tourist data . dialogues . smartkom some data i collected in a couple weeks for training recognizers and email way back when . see this ontology node is probably something that i will try to expand . and hopefully you can also try to find out , sooner or later in the course of the summer what we can expect to get from the discourse that might , or the we could sit down and think of the ideal speaker utterances , the ideal sentences where we have complete construction coverage and , they match nicely . the action planner guy has wrote has written a p lengthy proposal on how he wants to do the action planning . and i responded to him , also rather lengthy , how he should do the action planning . and i tacked on a little paragraph about the fact that the whole world calls that module a dis disc dialogue manager , and also rainer m malaka is going to be visiting us shortly , ","It was agreed that making subjects select from categories of tasks, such as ""big place"", ""service"", etc. could provide a better range of data. For the latter, there are already 30 subjects lined up and more are expected to be recruited off campus. Their values will either be inferred from the user-system interaction, or -as a last resort- requested directly from the user. "
Bed011.D,"what we 're gonna do today is two related things . one of them is to work on the semantics of the belief net which is going to be the main inference engine for thi the system making decisions . and decisions are going to turn out to be parameter choices for calls on other modules . and we 're also , in the same process , going to work with fey on what there should be in the dialogues . but s th point is to y to build a system that 's got everything in it that might happen you do one thing . t to build a system that had the most data on a relatively confined set of things , you do something else . and the speech people , are gonna do better if they if things come up repeatedly . if it 's one service , one luxury item , one big ish place , and forth and on , then my guess is that the data is going to be easier to handle . now you have this possible danger that somehow there 're certain constructions that people use when talking about a museum that they wouldn't talk about with a university and let 's plan next monday , to have a review of what we have far . but it would be great if you could , not transcribe it all , but pick out some are you gonna have the audio on the web site ?  i b my guess is it 's gonna be ten . but anyway think it 's a good idea to start with the relatively straight forward res just response system . and then if we want to get them to start doing multiple step planning with a whole bunch of things and then organize them tell them which things are near each other "" which things would you like to do tuesday morning ? "" that 's what i was suggesting for the first round , like at "" attend a theater , symphony or opera "" is a group , and "" tour the university , castle or zoo "" , we we don't know how many we can get next door at the shelter or but there 's th but definitely a back off position to asking . object "" becomes "" entity "" , alright , think the order of things is that robert will clean this up a little bit , although it looks pretty good . robert and eva and bhaskara are gonna actually build a belief net that , has cpt 's and , tries to infer this from various kinds of information . and fey is going to start collecting data , and we 're gonna start thinking a about what constructions we want to elicit . we are expecting johno to build a parser , he 's g he 's hoping to do this for his masters ' thesis s by a year from now . limited . the hope is that the parser itself is , pretty robust . the idea is first of all i misspoke when i said we thought you should do the constructions . because we do wanna get them r u perfectly but we 're gonna have to do a first cut at a lot of them to see how they interact . you can do f f have a complete story ov of s of some piece of dialogue . and that 's gonna be much more useful than having all of the clausal constructions and nothing else , like that . that the trick is going to be t to take this and pick a some lattice of constructions , whatever you need in order to be able to then , by hand , explain , some fraction of the utterances . ","The main focus of the meeting was firstly on the structure of the belief-net, its decision nodes and the parameters that influence them, and secondly, on the design of the data collection tasks. These nodes represent decisions that will function as parameters to action calls in the system. It was agreed that making subjects select from categories of tasks, such as ""big place"", ""service"", etc. could provide a better range of data. The duration of each dialogue will probably be no more than 10 minutes. For the latter, there are already 30 subjects lined up and more are expected to be recruited off campus. Their values will either be inferred from the user-system interaction, or -as a last resort- requested directly from the user. Similarly, the construction parser that is to be built within a year is expected to be relatively basic, yet robust. Finally, as to the semantic and syntactic constructions, work will start with more general and brief descriptions, before moving to exhaustive analysis of at least a subset. "
Bed011.E," can s can probably schedule ten people , whenever . would say two weeks . that w one thing we should do is go through this list and select things that are categories and then o offer only one member of that category ? ","It was agreed that making subjects select from categories of tasks, such as ""big place"", ""service"", etc. could provide a better range of data. "
Bed011.F,"but it seem that there is a difference between going to see something , and things like "" exchange money "" or "" dine out "" ",
Bed012.A,"the different decision nodes , if we trusted the go there node more th much more than we trusted the other ones , then we would conclude , even in this situation , that he wanted to go there . it 's not based on constructions , it 's based on things like , there 's gonna be a node for go there or not , and there 's gonna be a node for enter , view , approach . in general we won't just have those three , we 'll have , many nodes . we have to , like that it 's no longer possible to just look at the nodes themselves and figure out what the person is trying to say . like , if you 're asked a where is question , you may not even look like , ask for the posterior probability of the , eva node , cuz , that 's what in the bayes net you always ask for the posterior probability of a specific node . you may not even bother to compute things you don't need . we 'll meet next tuesday , but how many constructions do could we possibly have nodes for ? ","The input layer deriving information from things like the user and situation models, feeds into a set of decision nodes, such as the Enter/View/Approach (EVA) endpoint. Therefore, they will either have to be pruned a posteriori, or only a subset of the possible decision nodes will be computed in each occasion. "
Bed012.B,"what 's the situation like at the entity that is mentioned ? if we know anything about it ? that 's just specifying the input for the w what 's and , also , this is a what the input is going to be . once you have this as running as a module what you want is you wanna say , "" give me the posterior probabilities of the go there node , when this is happening . ""  as , if i understand it correctly , it always gives you all the posterior probabilities for all the values of all decision nodes . and the question is what to do with it , how to interpret it , the person said , "" where is x ? "" we want to know , is does he want info ? or know the location ? or does he want to go there ? it 's always gonna give us a value of how likely we it is that he wants to go there and doesn't want to go there , or how likely it is that he wants to get information . it n all i 'm saying is , whatever our input is , we 're always gonna get the full output . and some things will always be too not significant enough . what i am thinking , or what we 're about to propose here is we 're always gonna get the whole list of values and their posterior probabilities . and now we need an expert system or belief net that interprets that , because there are interdependencies , but that 's just shifting the problem . then you would have to make a decision , "" if it 's a where is question , which decision nodes do i query ? "" it does make a difference in terms of performance , computational time . what where we also have decided , prior to this meeting is that we would have a rerun of the three of us sitting together sometime this week again and finish up the , values of this . we have , believe it or not , we have all the bottom ones here . we have to come up with values for this , and this , this , and forth . and just fiddle around with it a little bit more . and , we won't meet next monday . then . let 's meet again next tuesday . and , finish up this bayes net . and we present our results , because then , once we have it up and running , then we can start defining the interfaces and then feed into it and get out of it , and then hook it up to some fake construction parser worry about the ontology interface and you can keith can worry about the discourse . they would still c get the closest , best fit . but , you can probably count the ways . ","The input layer deriving information from things like the user and situation models, feeds into a set of decision nodes, such as the Enter/View/Approach (EVA) endpoint. In any particular situation, most of the outputs will not be relevant to the given context. The latter option could could follow a binary search-tree approach and it could also be better in computational terms. The complete prototype of the Bayes-net will be presented in the next meeting. After that, it will be possible to define interfaces and a dummy construction parser, in order to test and link modules together. "
Bed012.C,"y why are you specifying it in xml ? this is just a specification of all the possible inputs ? you won't it 'll be hard to decide . you could say , "" there here 's the where is construction . "" and for the where is construction , we know we need to l look at this node , that merges these three things together they are evenly weighted . know . but how do we weight what we get out ? as , which one i which ones are important ? my i if we were to it with a bayes net , we 'd have to have a node for every question that we knew how to deal with , that would take all of the inputs and weight them appropriately for that question . are we going to make a node for every question ? every construction . if let 's say i had a construction parser , and i plug this in , i would each construction the communicative intent of the construction was and then i would know how to weight the nodes appropriately , in response . then , the bayes net that would merge there , that would make the decision between go there , info on , and location , would have a node to tell you which one of those three you wanted , you 'd have a decision tree query , go there . if k if that 's false , query this one . if that 's true , query that one . and just do a binary search through the ? also , i 'm somewhat boggled by that hugin software . i can't figure out how to get the probabilities into it . it 's that we can that we have one node per construction . cuz even in people they don't you 're talking about if you 're using some strange construction . any form meaning pair , to my understanding , is a construction . as long as your analysis is finite . ","In any case, on what basis the ""winner"" output is chosen is not clear. One suggestion was discussed: the particular constructions used can determine the pertinent decision (output) nodes. The latter option could could follow a binary search-tree approach and it could also be better in computational terms. "
Bed012.D,"eventually , you still have to pick out which ones you look at . ",
Bed013.A,"and how much to get into the cognitive neural part ? like , nlp cognitive neural . the fact that the methods here are all compatible with or designed to be compatible with whatever , neurological neuro biol su like introducing the formalism might be not really possible in detail , but you can use an example of it . what 's the middle thing ? do , or do not take other kinds of constructions into account ? it 's like you infer the speaker intent , and then infer a plan , a larger plan from that , for which you have the additional information , ",
Bed013.B,"the basic idea would be to give allow the system to have intentions , it will d r assign values to all the nodes . yes . let 's see , it would be two to the thirty for every output node ? which is very th very large . but we can do randomized testing . which probabilistically will be good enough . i 'm gonna finish it today , hopefully . we just it wouldn't hurt to write up a paper , is this a computer science conference that 's the only that 's the question mark . i don't see unde how we would be able to distinguish between the two intentions just from the g utterance , though . bef or , before we don't before we cranked it through the bayes net . ","Setting up certain inputs in the Bayes-net would imply certain intentions, which would trigger dialogues. There is potential to make a conference paper out of presenting the current work and the project aspirations within a parsing paradigm. "
Bed013.C,,
Bed013.D,"and the other bit of news is we had i was visited by my german project manager and b , is planning to come here either three weeks in july or three weeks in august , to actually work . and he came up we came up with a pretty strange idea . imagine if you will , that we have a system that does all that understanding that we want it to do based on utterances . it should be possible to make that system produce questions . if you have the knowledge of how to interpret "" where is x ? "" under given conditions , situational , user , discourse and ontological conditions , you should also be able to make that same system ask "" where is x ? "" but one could do some learning . if you get the system to speak to itself , you may find n break downs and errors and you may be able to learn . it 's not the same as the understanding . if we don't have let 's assume we don't have any input from the language . right ? there 's also nothing we could query the ontology , but we have a certain user setting . if you just ask , what is the likelihood of that person wanting to enter some something , it 'll give you an answer . and @ @ whatever that is , it 's the generic default intention . that it would find out . you can observe some user and context and ask , what 's the posterior probabilities of all of our decision nodes . what we actually then need to do is write a little script that changes all the settings , go goes through all the permutations , didn't we calculate that once ? it be it 's an idea that one could n run past , we fixed some more things from the smartkom system , how is the generation xml thing ? it 's cognitive , neural , psycho , linguistic , but all for the sake of doing computer science . it 's cognitive , psycho , neural , plausibly motivated , architectures of natural language processing . might be more interesting to do something like let 's assume we 're right , and take a "" where is x "" sentence , and how we cognitively , neurally , psycho linguistically , construction grammar ally , motivated , envision understanding that "" . that should be able to we should be able to come up with , a parse . i also think that if we write about what we have done in the past six months , we could craft a little paper that if it gets rejected , which could happen , doesn't hurt it 's obvious that we can't do any evaluation , and have no we can't write an acl type paper where we say , "" we 've done this and now we 're whatever percentage better than everybody else "" . even that 's the time to introduce the new formalism that you guys have cooked up . look at the web page and let 's talk about it tomorrow afternoon ? don't make any plans for spring break next year . we 're gonna do an int edu internal workshop in sicily . i 've already got the funding . we could say this is what 's state of the art today . nuh ? and say , this is bad . nuh ? and then we can say , what we do is this . and you can just point to the literature , this will be documenting what we think , and documenting what we have in terms of the bayes net the sudo square is , "" situation "" , "" user "" , "" discourse "" , right ? "" ontology "" . these are our , whatever , belief net decision nodes , and they all contribute to these things down here . in the moment it 's a bayes net . and it has fifty not yet specified interfaces . have taken care that we actually can build little interfaces , to other modules that will tell us whether the user likes these things and , n the or these things , think of back at the eva vector , and johno coming up with the idea that if the person discussed the admission fee , in previously , that might be a good indication that , "" how do i get to the castle ? "" , actually he wants to enter . what would be is that if we encounter concepts that are castle , tower , bank , hotel , we run it through the ontology , and the ontology tells us it has admission , opening times , and then search dynamically through the discourse history for occurrences of these things in a given window of utterances . and furthermore , we can idealize that , people don't change topics , but , even th for that , there is a student of ours who 's doing a dialogue act recognition module . but we what we actually decided last week , is to , and this is , again , for your benefit is to pretend we have observed and parsed an utterance such as "" where is the powder tower "" , and specify what we think the output observe , out i input nodes for our bayes nets for the sub d , for the discourse bit , should be . that and i will then come up with the ontology side bits and pieces , and then we can fiddle with these things to see what it actually produces , in terms of output . we want to come up with what gets input , and how inter in case of a "" where is "" question . what would the outcome of your parser look like ? ","An idea for future work was suggested during the visit of the german project manager: the possibility to use the same system for language generation. Having a system able to ask questions could contribute significantly to training the belief-net. Setting up certain inputs in the Bayes-net would imply certain intentions, which would trigger dialogues. There is potential to make a conference paper out of presenting the current work and the project aspirations within a parsing paradigm. The focus should be the Bayes-net, to which all other modules interface. Situation, User, Discourse and Ontology feed into the net to infer user intentions. Someone asking where the castle is after having asked about the admission fee, indicates that -given that the castle is open to tourists- they want to go there, as opposed to knowing its whereabouts. It was suggested that they start analysing what the Discourse and Ontology would give as inputs to the Bayes-net by working on simple utterances like ""where is X?"". "
Bed013.D,"and , what other discourse information from the discourse history could we hope to get , squeeze out of that utterance ? define the input into the bayes net based on what the utterance , "" where is x "" , gives us . if we have a construction node , "" where is x "" , it 's gonna both get the po posterior probability that it 's info on up , info on is true up , and that go there is true up , as which would be exactly analogous to what i 'm proposing is , this makes makes something here true , and this makes this true up , and this makes this true up as if you can , definitely do , if i if it 's not triggered by our thing , then it 's irrelevant , think this is just a mental exercise . focus on this question , how would you design that ? that we actually end up with nodes for the discourse and ontology that we can put them into our bayes net , and we can run our better javabayes , and have it produce some output . th this might be a opening paragraph for the paper as saying , "" people look at kinds of at ambiguities "" , a , these things are never really ambiguous in discourse , b , but normal statements that seem completely unambiguous , such as "" where is the blah "" , actually are terribly complex , and completely ambiguous . also we 're getting a person who just got fired from her job . person from oakland who is interested in continuing the wizard bit once fey leaves in august . remember this , we can completely change the set up any time we want . look at the results we 've gotten far for the first , whatever , fifty some subjects ? no , we 're approaching twenty now . but , until fey is leaving , we surely will hit the some of the higher numbers . we have found someone here who 's hand st hand transcribing the first twelve . ","It was suggested that they start analysing what the Discourse and Ontology would give as inputs to the Bayes-net by working on simple utterances like ""where is X?"". With this addition, all input layers of the net would be functioning. "
Bed013.E,"e i 'm have the impression that getting it to say the right thing in the right circumstances is much more difficult than getting it to understand something given the circumstances and on , just the fact that we 'll get that getting it to understand one construction doesn't mean that it will n always know exactly when it 's correct to use that construction . right ? it 's g anyway , that given all of these different factors , it 's it 's still going to be impossible to run through all of the possible situations or whatever . what would one possibly put in such a paper ? i kinda like it better without that extra level of indirection too . ",
Bed014.A,"the question of whether the polysemy is like in the construction or pragmatic . the question is whether the construction is semantic or like ambiguous between asking for location and asking for path . or whether the construction semantically , is clearly only asking for location but pragmatically that 's construed as meaning "" tell me how to get there "" . the question is is this conventional or conversational implicature ? that 's the eighteenth . two pm . someday we also have to we should probably talk about the other side of the "" where is x "" construction , which is the issue of , how do you simulate questions ? since we have this idea about the indefinite pronoun thing and all that , i ca can try and , run with that , try and do some of the sentence constructions now . the wh question has this as extra thing which says "" and when you 're done , tell me who fills that slot "" or w the idea of saying that you treat from the simulation point of view or whatever you treat , wh constructions similarly to indefinite pronouns and we 'll figure out exactly how to write that up and on , no , all the focus sometimes hans has been coming in there as like a devil 's advocate type role and he 'll just go off on parts of it which definitely need fixing ","The constructions could be built assuming either conventional or conversational implicature. This module will eventually have to include ways to simulate questions, do emphasis and focus. "
Bed014.B,"you might be y and asking for directions . priming a spreading activation he 's been around . but different perspec when we 're into data and looking at the some specific linguistic phenomenon in english or in german , in particular , whatever , that 's great , but when it 's like , w how do we capture these things , it 's definitely been keith and i who have d who have worried more about the i was just gonna say , though , that , there was out of a meeting with johno came the suggestion that "" could it be that the meaning constraints really aren't used for selection ? "" which has been implicit in the parsing strategy we talked about . which far , in terms of like putting up all the constraints as , pushing them into type constraints , the when i 've , propo then proposed it to linguists who haven't yet given me we haven't yet thought of a reason that wouldn't work . right ? as long as we allow our type constraints to be reasonably complex . ","One suggestion was to use the spreading activation as a paradigm for activating nodes in the belief-net. Finally, using type constraints in the construction analysis should work, as long as they are complex enough not to generate too many parses. "
Bed014.C,"also had a email correspondence with daphne kohler , who said yes she would love to work with us on the , using these structured belief nets and and then we 'll figure out a way for you you to get connected with , their group . and it looks to me like we 're now at a good point to do something start working on something really hard . w which is mental spaces and and or but the other part of it is the way they connect to these , probabilistic relational models . there 's all the problems that the linguists know about , about mental spaces , and the cognitive linguists know about , but then there 's this problem of the belief net people have only done a moderately good job of dealing with temporal belief nets . one of the things i w would like to do over the next , month , it may take more , is to st understand to what extent we can not only figure out the constructions for them for multiple worlds and what the formalism will look like and where the slots and fillers will be , but also what that would translate into in terms of belief net and the inferences . but that 's g that 's , as far as tell , it 's putting together two real hard problems . one is the linguistic part of what are the couplings and when you have a certain , construction , that implies certain couplings and other couplings , and then we have this inference problem of exactly technically how does the belief net work no , i know , i th that is gonna be the key to this wh to th the big project of the summer of getting the constructions right is that people do manage to do this there probably are some , relatively clean rules , anyway , that we were that we 're gonna try to get a first cut at the revised formalism by the end of next week . just trying to write up essentially what you guys have worked out that everybody has something to look at . but but i interrupted before keith got to tell us what happened with "" where is the powder tower ? "" or whatever a i th i agree with you that , it 's a disaster to try to make separate constructions for every pragmatic reading , although there are some that will need to be there . f in the short run it 's more important to know how we would treat technically what we would do if we decided a and what we would do if we decided b , than it is t to decide a or b r right now . w we know for that we have to be able to do both . in th in the bl bayes net you could think about it this way , that if at the time "" admissions fee "" was mentioned you could increase the probability that someone wanted to enter . but my guess is what 'll probably will happen , here 's a proposed design . is that there 're certain constructions which , for our purposes do change the probabilities of eva decisions and various other kinds th that the , standard way that the these contexts work is stack like or whatever , and it could be that when another en tourist entity gets mentioned , you re essentially re initiali re i essentially re initialize the state . and if we had a fancier one with multiple worlds you could have you could keep track of what someone was saying about this and that . now , but ro robert 's right , that to determine that , you may want to go through a th thesaurus if the issue is , if now th this construction has been matched and you say "" does this actually have any implications for our decisions ? "" then there 's another piece of code that presumably does that computation . it th thi think of arguments in either direction on that . you 've recognized the word , which means you have a lexical construction for it , you could just as tag the lexical construction with the fact that it 's a thirty percent increase in probability of entering . it 's very likely that robert 's thesis is going to be along these lines , and the local rules are if it 's your thesis , you get to decide how it 's done . this is this is , speaking of hard problems , this is a very good time to start trying to make explicit where construal comes in and where c where the construction per se ends and where construal comes in , right . right . thing that 's part of why we want the formalism , why don't we plan to meet monday and we 'll see if we want to meet any more than that . we talked about semspec , for "" semantic spec specification "" part of what was missing were markings of all sorts that weren't in there , incl including the questions we didn't we never did figure out how we were gonna do emphasis in the semspec . skolemization . if i part of what the exercise is , t by the end of next week , is to say what are the things that we just don't have answers for yet . has i haven't seen hans boas ? this is consistent with the role i had suggested that he play , which was that o one of the things i would like to see happen is a paper that was tentatively called "" towards a formal cognitive semantics "" which was addressed to these linguists who haven't been following this ","An important research issue to be investigated is how the concept of mental spaces and probabilistic relational models can be integrated into the belief-net. A step towards this goal is the construction formalism being put together. Mental space interdependencies are based on relatively clean rules, since people seem to manage them easily. At this stage both routes need to be examined. The formalism will also serve as a starting point for the definition of construal mechanisms. This module will eventually have to include ways to simulate questions, do emphasis and focus. "
Bed014.C,"it could be that he 's actually , at some level , thinking about how am i going to communicate this story and if you over generate then you 'll have to do more . ",
Bed014.D,"the xml trees for the gene for the synthesizer are written . just need to do the , write a new set of tree combining rules . but those 'll be pretty similar to the old ones . if you 're not used to functional programming , scheme can be completely incomprehensible . what what 's the argument for putting it in the construction ? ",
Bed014.E,"the java the embedded bayes wants to take input a bayes net in some java notation and eva is using the xalan style sheet processor to convert the xml that 's output by the java bayes for the into the , e bayes input . the generation templates are done . natural language generation produces not a just a surface string that is fed into a text to speech but , a surface string with a syntax tree that 's fed into a concept to speech . now and this concept to speech module has certain rules on how if you get the following syntactic structure , how to map this onto prosodic rules . and fey has foolheartedly to rewrite the german concept syntax to prosody rules that 's the lisp type scheme . we ha we have to change the voice . it is we have the choice between the , usual festival voices , which i already told the smartkom people we aren't gonna use because they 're really bad . ogi has , crafted a couple of diphone type voices that are really and it 's probably also uninteresting for all of you to , learn that as of twenty minutes ago , david and i , per accident , managed to get the whole smartkom system running on the icsi linux machines with the icsi nt machines but we would have a person that would like to work on it , and she would like to apply the ontology that is , being crafted at eml . back to the old johno observation that if y if you have a dialogue history and it said the word "" admission fee "" was mentioned it 's more likely that the person actually wants to enter than just take a picture of it and then there is the , middle way that i 'm suggesting and that is you get x , which is whatever , the castle . the ontology will tell us that castles have opening hours , and look up certain linguistic surface structures that are related to these concepts and feed those through the dialogue history and check dynamically for each e entity . we look it up check whether any of these were mentioned and then activate the corresponding nodes on the discourse side . but keith suggested that a much cleaner way would be is , to keep track of the discourse in such a way that you if that something like that ha has been mentioned before , this just a continues to add up , it 's also something that people have not done before , is abuse an ontology for these kinds of , inferences , on whether anything relevant to the current something has been has crept up in the dialogue history already , or not . i have the , if we wanted to have that function in the dialogue hi dialogue module of smartkom , i have the written consent of jan to put it in there . we need some then we need to make some dates . meeting regular meeting time for the summer , let 's say thursday one . the idea is on monday at two we 'll see an intermediate version of the formalism for the constructions , and do an on line merging with my construal ideas . do you wanna run the indefinite pronoun idea past jerry ? and then , "" who fixed the car with a wrench ? "" in the in analogously to the way you would do "" someone fixed the car with a wrench "" . ","Minor technical issues,such as format conversions for XML and JavaBayes and the full translation of the SmartKom generation module in English, are currently being resolved. The voice synthesiser will also be replaced by better technology. "
Bed014.F,"actually , could try emailing the guy and see if he has any something already . ",
Bed015.A,"do you want to do the same for space ? you want to forget stress . canonically speaking you can if you look at a curve over sentence , you can find out where a certain stress is and say , "" hey , that 's my focus exponent . "" it doesn't tell you anything what the focus is . because , as a form cue , not even trained experts can always they can tell you where the focus exponent is sometimes . and the only way of figuring out what that is , by generating all the possible alternatives to each focused element , decide which one in that context makes sense and which one doesn't . the once what the focus is the everything else is background . how about "" topic comment "" just , look read even sem semi formal mats rooth . and what i 'm confused about is what the speaker and the hearer is doing there . but you don't we ultimately want to handle that analogously to the way we handle time and place , that 's up at the reference part . isn't i 'm i was dubious why he even introduces this reality , as your basic mental space d doesn't start with some because it 's obvi it should be obvious , at least it is to me , that whenever i say something i could preface that with "" "" to eva on our web site we can now , if you want to run javabayes , you could see get download these classes . she modified the gui it has now a m a button menu item for saving it into the embedded javabayes format . the construal bit that , has been pointed to but hasn't been , made precise by any means , may w may work as follows . that the following thing would be in incredibly imagine you write a bayes net , completely from scratch every time you do construal . you consult your ontology you won't get you won't really get any c p t 's , therefore we need everything that configures to what the situation is , you get whatever comes from discourse plus the situation and the user model . and that fills in your cpt 's with which you can then query , the net that you just wrote and find out how thing x is construed as an utterance u . you may have some general rules as to how things can be construed as what , that will allow you to craft the initial notes . we can do it th thursday again . one or four . ","Among the issues still being defined, mental spaces and context (eg pronoun references) present similarities that can be echoed in the specification. Work on construal will use Bayes-nets, which will be fed information from other modules and implement general rules to infer how utterances are construed. "
Bed015.B,"is this , was it intentional to leave off things like "" inherits "" ",
Bed015.C,"but they do . the whole the mental space thing is clearly not here . that we 're not expecting much out of the , the purely form cues , and we still have emphasis as or stress , or whatever . then this says that when an analysis is finished , the whole analysis is finished , you 'll have as a result , some s resulting s semspec for that utterance in context , which is made up entirely of these things and , bindings among them . and bindings to ontology items . that the who that this is the tool kit under whi out of which you can make a semantic specification . this is an that anything you have , in the party line , anything you have as the semantic side of constructions comes , from pieces of this ignoring li but it 's got to be pieces of this along with constraints among them . but this is the full range of semantic structures available to you . let 's mark that . in our s terminology , "" want "" can be an action and "" what you want "" is a world . causal we need . mental space we need . the context we need . again , this this is gonna to get us into the mental space and we 're , gonna have to , chain those as had a had an idea that would be very if it works . we might be able to handle context in the same way that we handle mental spaces it c it , think if we did it right we might be able to get at least a lot of the same structure . that pulling something out of a discourse context is similar to other kinds of , mental space phenomena . that when you say "" you 're actually hedging . we don't have that in here either do we ? confidence like that . it 's as far as tell there 's this one major thing we have to do which is the mental the whole s mental space thing . and we 're going to have to s bound the complexity . we had started with the idea that the actual , constraint was related to this tourist domain and the kinds of interactions that might occur in the tourist domain , it was keith 's , job over the summer to come up with this set of constructions . and my suggestion to keith is that you , over the next couple weeks , n don't try to do them in detail or formally but just try to describe which ones you think we ought to have . just define your space . and that 'll also be useful for anybody who 's trying to write a parser . we 're gonna get the w we 're dealing with two domains , the tourist domain and the child language learning . and then my proposal would be to , not cut off more general discussion but to focus really detailed work on the subset of things that we 've we really want to get done . what i would like to do is separate that problem out . and what i 'd like to do is in the short run focus on getting it right . and when we think we have it right then saying , "" aha ! , can we make it more elegant ? "" once you have instantiated the prm with the instances and ther then you can unfold it . but the question is do you want to , send the little group , a draft of your thesis proposal ","It is, essentially, a toolkit with which to create semantic constructions, as well as the bindings between them and with the ontology. Among the issues still being defined, mental spaces and context (eg pronoun references) present similarities that can be echoed in the specification. Work on both of these formalisms will continue with circumscription of the construction space that will be studied in more detail. "
Bed015.D,,
Bed015.E,"know that this is like , we didn't originally have in mind that , that verbs would subcategorize for a particular form . i 'm not that the formalism , the grammatical side of things , is gonna have that much going on in terms of the mental space all of these called space builders that are in the sentence are going to assuming that at any point in discourse there 's the possibility that we could be talking about a bunch of different world scenarios , whatever , and the speaker 's supposed to be keeping track of those . the , the construction that you actually get is just gonna give you a cue as to which one of those that you 've already got going , you 're supposed to add structure to . but it doesn't tell you like exactly what it what the point of doing is . light up with focus , side one . the "" scenario "" box , "" part of what i 'm going to hand you is a whole bunch of s schemas , image , and x schemas . mean , there 's this in the off in the scenario , which just tells you how various what schemas you 're using and they 're how they 're bound together . and that some of the discourse segment that 's where the information structure is which is a profiling on different parts of , of this . there 's languages where , situation like "" bob said he was going to the movies "" , where that lower subject is the same as the person who was saying or thinking , you 're actually required to have "" i "" there . but it 's not perceived as a quotative construction . it 's been analyzed by the formalists as being a logophoric pronoun , which means a pronoun which refers back to the person who is speaking or that thing , right ? that pronoun references , ties in with all this mental space and on , and forth . it 's like what 's happening that , what 's happening , there is that you 're moving the base space like that , right ? we were just talking about this evidentiality and like that , right ? we don't have to care too much about the speaker attitude , right ? but like what are the distinctions among those that we actually care about for our current purposes ? there should be i wanted to s find out someday if there was gonna be some way of dealing with , if this is the right term , multiple inheritance , where one construction is inheriting from , from both parents , ","On the other hand, the semantic specification structures information in terms of ""scenario"", ""referent"" and ""discourse segment"". "
Bed015.F,"the result of m much thinking since the last time we met , but not as much writing , is a sheet that i have a lot of thoughts and justification of comments on and one on one side is a the revised updated semantic specification . and the other side is , revised construction formalism . it 's only slightly more stable than it was before . let 's start b let 's start on number two actually on the notation , the top block is just abstract nota it 's like , listings of the kinds of things that we can have . they could all have a type at the beginning . and then they say the key word construction and then it has a block that is constituents . but in general instead of the word "" construct "" , th here you might have "" meaning "" or "" form "" as or you could just say in the second case that you only know the meaning type . and usually it has formal considerations that will go along with it . and then again semantic constraints here are just bindings . and it was an issue whether constraints there were some linguists who reacted against "" constraints "" , saying , "" if it 's not used for matching , then it shouldn't be called a constraint "" . think the middle block doesn't really give you any more information , ex than the top block . and the bottom block similarly only just illus all it does is illustrate that you can drop the subscripts and that you can drop the , that you can give dual types . one thing i should mention is about "" designates "" . if some other construction says , "" are you of type is this part of type whatever "" , the "" designates "" tells you which part is the meaning part . right now it 's a c contr construction type and meaning type . don't form type is . one option that , keith had mentioned also was like , if you have more abstract constructions such as subject , predicate , things like grammatical relations , those could intersect with these in such a way that subject , predicate , or subject , predicate , subject , verb , ob verb object would require that those things that f fill a subject and object are nom expressions . there 's going to be some extra definitely other notation we 'll need for that which we skip for now . something like "" past "" which i we think is a very simple we 've often just stuck it in as a feature , it 's often thought of as it is also considered a mental space , whereas past is a very conventionalized one instead of just time ? same thing . there are very conventionalized like deictic ones , right ? one other thing want to point out is there 's a lot of confusion about the terms like "" profile , designate , focus "" , et cetera , et cetera . for now i 'm gonna say like "" profile "" 's often used one is in the traditional like semantic highlight of one element with respect to everything else . and the second use , is in framenet it 's they use it to really mean , this in a frame th this is the profiles on the these are the ones that are required . the "" designate "" that we have in terms of meaning is really the "" highlight this thing with respect to everything else "" . but the second one seems to be useful d don't yet know , i have no commitment , as to whether we need it . that , the other terms that are related are like focus and stress . it 's like it 's not it might be that there 's a syntactic , device that you use to indicate focus think that 's to keep "" focus "" being an information structure term . w we 'll want to distinguish stress as a form device . high volume or whatever . like if you 're gonna focus on this thing and you wanna know it evokes all the other possibilities that it wasn't . we need to have a notation for that that 's still in progress . but it did one implication it does f have for the other side , which we 'll get to in a minute is that i couldn't think of a good way to say "" here are the possible things that you could focus on "" , but i can't think of like the limited set of possible meanings that you would focu li the best thing come up with is that information has a list of focused elements . other things we didn't deal with , we 've had a lot of other that keith and i have them working on in terms of like how you deal with like an adjective . a nominal expression . you can like nest things when you need to , but they can also overlap in a flatter way . but , we have the properties of dependency grammars and some properties of constituents constituent based grammar . the fact that you can get it without any stress and you have some whatever is predicated anyway should be the same set of constructions . then you have a separate thing that picks out , stress on something relative to everything else . since i couldn't think of an easy like limited way of doing it , all say is that information structure has a focused slot topic it seems that might be an ongoing thing . again , information structure has a topic slot . one thing that i didn't do consistently , is when we get there , is like indicate what thing fits into every role . ","The discussion concerned the revised semantic specification and the construction formalism. The different levels of the latter focus on what construction types are encountered and what bindings there are between them. Similarly, ways to handle mental spaces will have to be added on top. The encoding of features is still incomplete: frame profiles, focus, adjectives, nominal expressions are phenomena in the process of being integrated. The notation maintains properties of both dependency and constituent-based grammars. "
Bed015.F,"it 'll be another predication or it 'll be , i don't know , some value from some something , some variable and scope like that , or a slot chain based on a variable and scope . you see it 's "" scenario "" , "" referent "" and "" discourse segment "" . and "" scenario "" is essentially what what 's the basic predication , what event happened . and actually it 's just a list of various slots from which you would draw in order to paint your picture , a bunch of frames , bi and bindings , right ? the middle thing used to be "" entities "" and this is intended to be grammatically specifiable information about a referent about some entity that you were going to talk about . it could be a pointer to ontology . because this partly came from , talmy 's schema and i 'm not we 'll need all of these actually . th the big com and finally "" discourse segment "" is about speech act y information structure y , like utterance specific kinds of things . having made that big sca like large scale comment , should go through each of these slots each of these blocks , a little bit ? mostly the top one is image schematic . some of them seem more inherently static , like a container or support ish . and others are a little bit seemingly inherently dynamic like "" source , path , goal "" is often thought of that way or "" force "" , like that . it 's gonna be something in the x schema that tells you "" is this static or is this dynamic "" . one thing i 'm uncertain about is how polarity interacts . polarity , is using for like action did not take place it 's a grab bag of except for "" , that i forgot . that 's , these are all implicitly one within , within one world . or "" there was a whole list of possible speaker attitudes that like say talmy listed . i don't know if those are grammatically marked in the first place . if you stressed , "" john went to the "" , "" the bar "" whatever , you 're focusing that and a in a possible inference is "" in contrast to other things "" . for a particular segment it 's really just a reference to some other entity again in the situation , right ? right , the constructions might will refer , using pronouns or whatever . but when you actually say that "" he walked into "" , whatever , the "" he "" will refer to a particular it depends on if it is a referring exp if it 's identifiable already or it 's a new thing . if it 's a new thing you 'd have to like create a structure or whatever . if it 's an old thing it could be referring to , usually w something in a situation , right ? it 's like the unspecified mental spaces just are occurring in context . and then when you embed them sometimes you have to pop up to the h depending on the construction or the whatever , you you 're scope is m might extend out to the base one . actually one of the child language researchers who works with t tomasello studied a bunch of these constructions and it was like it 's not using any interesting embedded ways just to mark , uncertainty like that . hhh , i there used to be a slot for speaker , and someone had raised like sarcasm as a complication at some point . i didn't want to think too much about that for now . grout out the things that you need . four . ","On the other hand, the semantic specification structures information in terms of ""scenario"", ""referent"" and ""discourse segment"". Each category comprises a number of slots filled in by information derived from the utterance. "
Bed017.A,"cuz some of us do pretty detailed linguistic analyses , and i 'm guessing that you won't be doing that ? the other person of is dan gildea ? we there 's someone in icsi who actually has been working on has worked on that kinda ","Although no detailed linguistic analysis takes place, it was suggested that the use of FrameNet could be a useful approach. There were also further suggestions for meetings with ICSI researchers. "
Bed017.B,"in a smaller group we had talked and decided about continuation of the data collection . fey 's time with us is almost officially over , and she brought us some thirty subjects and , t collected the data , and ten dialogues have been transcribed and we found another cogsci student who 's interested in playing wizard for us . here we 're gonna make it a little bit more complicated for the subjects , this round . she 's actually suggested to look at the psychology department students , because they have to partake in two experiments in order to fulfill some requirements . as for smartkom , i 'm the last smartkom meeting i mentioned that we have some problems with the synthesis , which as of this morning should be resolved . when tomorrow is over , we 're done . something happened , in on eva 's side with the prm that we 're gonna look at today , we have a visitor from bruchsal from the international university . and the , other question i would have is that presumably , from the way the stanford people talk about it , you can put the probabilities also on the relations . good . then , we can move on and see what andreas has got out his sleeve . this they refused the budget again ? is it about citris ? still nothing . ","There is a new wizard for phase two, during which subjects will be given more complex scenarios. Also finished are the modifications on SmartKom: the remaining glitches will take no more than a day to iron out. A big part of the meeting was covered by the presentation of the PRM of the proposed system. Following this, a visiting researcher presented an overview of a parallel project at the International University. "
Bed017.C,"what i started looking at , to begin with is just content management systems i in general . what 's the state of the art there is to you have a bunch of documents or learning units or learning objects , and you store meta data associate to them . there 's some international standards because there 's an implicit assumption behind that is that all the users of this system share the same interpretation of the keyword and the same interpretation of whichever taxonomy is used , and the only thing that m really works out far are library ordering codes , and what i want to build is smart f a q system . now , what you need to do here is you need to provide some context information i want to be able to model information like , in the context of in the context of developing distributed systems , of a at a computer science school , now , u using this you can infer more information , and you could then match this to the meta data of off the documents you 're searching against . now , what i plan to do is i want to do a try to improve the quality of the search results , now , the big problem that i 'm facing right now is it 's fairly easy to hack up a system quickly , that works in the small domain , but the problem is the scalability . what i 'm also looking into is a probabilistic approach into this it 's probably not that easy to simply have a symbolic computational model . and lot of the even though it 's a very different domain , but lot of the , issues are fairly similar . i want to build a smart librarian , that can point you to the right reference . i don't wanna compute the answer , no . cuz it 's very easy to whip up something quickly , and if we want to share and integrate things , they must they must be designed really . ","It attempts to build a smart tutoring system for a computer science course. The assumption is that document searches can give more personalised results, if they take into account contextual parameters (user, situation). Although no detailed linguistic analysis takes place, it was suggested that the use of FrameNet could be a useful approach. "
Bed017.D,"i 've been looking at the prm i sorta constructed a couple of classes . like , a user class , a site class , and a time , a route , and then and a query class . and i tried to simplify it down a little bit , the red lines on the , graph are the relations between the different classes . this is more or less similar to the flat bayes net that i have , with the input nodes and all that . tried to construct the dependency models , a lot of these got from the flat bayes net , and it turns out , the cpt 's are really big , if i do that , and ended up making several classes actually , a class of with different attributes that are the intermediate nodes , and one of them is like , time affordability money affordability , site availability , and the travel compatibility . but , if you look at travel compatibility for each of these factors , you need to look at a pair of , what the preference of the user is versus , what type of an event it is , and that makes the scenario a little different in a prm , anyhow , using those intermediate nodes then , this would be the class that represent the intermediate nodes . with , references to the user and the site and the time . and it 's easier to specify the cpt and all . it only makes two decisions , in this model . and one is how desirable a site is meaning , how good it matches the needs of a user . and the other is the mode of the visit , whether th it 's the eva decision . i remember them learning when , you don't know the structure for but i don't remember reading how you specify ","An alternative representation of the Bayes-net, it depicts context features as classes, and dependencies as relations between them. The current outputs show the desirability of a site, as well as its EVA mode. "
Bed017.E,,
Bed017.F,"robert , why don't you bring us up to date on where we are with edu ? the you didn't look yet to see if there 's anybody has a implementation . we aren't gonna build our own interpreter , one of the things that eva 's gonna do over the next few weeks is see if we can track that down . the people at stanford write papers as if they had one , anyway . that 's a major open issue . i actually think it is cleaner , the notion of instantiating your el elements from the ontology and fits this very nicely and doesn't fit very into the extended belief net . the plan is when daphne gets back , we 'll get in touch and supposedly , we 'll actually get s deep connected to their work somebody 'll if it 's a group meeting once a week probably someone 'll go down and , whatever . on the other hand , framenet could be useful . unfortunately , srini , who is heavily involved in daml and all this is himself out of town . i it turns out i wound up having lunch today with a guy named tom kalil . he 's hired to run a lot of citris , he said , "" , "" if you had a adult literacy program that was as good as an individual tutor , and as compelling as a video game , then that would have a huge social impact "" . i said , "" great ! that 's a good problem to work on . "" ","The fact that this model allows for instantiations of classes fits the research purposes much better than the extended belief-net. Although no detailed linguistic analysis takes place, it was suggested that the use of FrameNet could be a useful approach. "
Bmr003.A,"topic of this meeting is i wanna talk a little bit about transcription . i 've looked a little bit into commercial transcription services and jane has been working on doing transcription . and then get an update on the electronics , eventually we 're probably gonna wanna distribute this thing it seems like it 's not a corpus which is has been done before . and how we do we distribute the transcripts , how do we distribute the audio files , but should we do it in the same format as ldc have a bunch of scripts with x waves , and some perl scripts , and other things that make it really easy to extract out and align where the digits are . and if u d uw 's going to do the same thing it 's worth while for them to do these digits tasks as and what i 've done is pretty ad hoc , we might wanna change it over to something a little more standard . stm files , or xml , we were planning to do like thirty or forty hours worth of meetings . because it would to be able to take that and adapt it to a meeting setting . that would be that would be something to look into . why don't you go ahead and do that then eric ? actually , that 's another thing i was thinking about is that jane should talk to liz , to see if there are any transcription issues related to discourse that she needs to get marked . shall we move on and talk a little bit about transcription then ? what we 're using right now is a tool , from this french group , called "" transcriber "" it has a , useful tcl tk user interface we 're at this point only looking for word level . and the things we that we know that i want are the text , the start and end , and the speaker . but other people are interested in stress marking . and then things like repairs , and false starts , and , filled pauses , and all that other we have to decide how much of that we wanna do . what we wanted to do was have jane do one meeting 's worth , forty minutes to an hour , is and one of the things was to get an estimate of how long it would take , and then also what tools we would use . and the next decision which has to be made actually pretty soon is how are we gonna do it ? and that 's happened in the past . and that 's probably the right way to do it . but can't imagine that we 're gonna get anything that much better from a commercial one . what she 's done far , is more or less breath g not breath groups , phrases , continuous phrases . and that 's because you separate when you do an extract , you get a little silence on either end . we could program that pretty easily , i could generate the segmentation and you could do the words , and time yourself on it . that would at least tell us whether it 's worth spending a week or two trying to get a tool , that will compute the segmentations . i looked at cyber transcriber which is a service that you send an audio file , they do a first pass speech recognition . and then they do a clean up . but it 's gonna be horrible . they 're never gonna be able to do a meeting like this . and what i 'm saying is that if we hire an external service we can expect three hundred dollars an hour . they won't . but at any rate , we have a ballpark on how much it would cost if we send it out . these are linguistics grad students . that 's why i said originally , that i couldn't imagine sending it out 's gonna be cheaper . and also we would have control of we could give them feedback . whereas if we do a service it 's gonna be limited amount . and they 're not gonna provide stress , they 're not gonna re provide repairs , they may or may not provide speaker id . i hope it 's jane . the user interface only allows two . and if you 're using their interface to specify overlapping speakers you can only do two . but my script can handle any . yes , and they said that 's on in the works for the next version . and they have they 've actually asked if we are willing to do any development and i said , if we want if we did something like programmed in a delay , which actually is a great idea , i 'm they would want that incorporated back in . we should probably mark areas that have no speakers as no speaker . if we use the little the conventions that jane has established , i have a script that will convert from that convention to their saved convention . right . the whole saved form the saved format and the internal format , all that handles multiple speakers . it 's just there 's no user interface for specifying multiple any more than two . anyway , are we interested then in writing tools to try to generate any of this automatically ? and we have quite a disparate number of web and other sorts of documents on this project spread around . i have several and dan has a few , let 's move on to electronics . we wanna do that definitely . think the other thing i 'd like to do is , do something about the set up that it 's a little more presentable and organized . ","The Berkeley Meeting Recorder group discussed the aims, methods, timing, and outsourcing issues concerning transcription of the Meeting Recorder corpus. The group received an update on the meeting room recording setup and electronics. The Transcriber software tool was introduced, along with a set of transcription conventions for coding different speech events. The prospect of sending the data to an external transcription service was weighed against that of hiring a graduate student transcriber pool. It was tentatively decided that the latter option would be less costly and allow BMR to maintain greater control over the transcription process. "
Bmr003.A,then the other question is do we wanna try to do a user interface that 's available out here ? do we wanna try to get a monitor ? ,
Bmr003.B,"it 's not much the actu the logistics of distribution are secondary to preparing the data in a suitable form for distribution . and the other thing is that , university of washington may want to start recording meetings as in which case w we 'll have to decide what we 've actually got that we can give them a copy . we don't know , actually . we haven't decided which time we care about , and that 's one of the things that you 're saying , is like you have the option to put in more or less timing data and , be in the absence of more specific instructions , we 're trying to figure out what the most convenient thing to do is .  but it seems like we it doesn't it seems like it 's not really not that hard to have an automatic tool to generate the phrase marks , and the speaker , and speaker identity without putting in the words . i transcribed a minute of this and there was a lot of overlapping . but that 's but there 's only one time boundary for both speakers , alright . let 's look at it anyway . definitely we should have some access to the data . that if t if you have two people sitting next to each other they can actually go into the same box . again , washington wants to equip a system . our system , we spent ten thousand dollars on equipment not including the pc . however , seven and a half thousand of that was the wireless mikes . but once we 've done the intellectual part of these , we can just knock them out , right ? washington could have a system that didn't have any wireless but would had what 's based on these and it would cost pc and two thousand dollars for the a to d p z ms cost a lot . but anyway you 'd save , on the seven or eight thousand for the wireless system . ",
Bmr003.C,"in addition to this issue about the uw there was announced today , via the ldc , a corpus from i believe santa barbara . what i was thinking is it may be useful in transcribing , if it 's far field in doing , some of our first automatic speech recognition models , it may be useful to have that data we get it for free cuz they 're distributing it through the ldc . we should get a copy of it just to see what they did that we can compare . but , it it would also help be helpful for liz , if she wanted to start working on some discourse issues , looking at some of this data and then , we could try the following experiment . take the data that you 've already transcribed and throw out the words , but keep the time markings . and then go through and go through and try and re transcribe it , given that we had perfect boundary detection . and see if it see if it feels easier to you . the question is , is it worth giving you the segmentation ? who knows if they 're gonna be able to m manage multal multiple channel data ? but we can pay a graduate student seven dollars an hour . that means that even if it takes them thirty times real time it 's cheaper to do graduate students . how are you handling backchannels ? or does it does the fact that there 's a backchannel split the it in two .  we should s consider also , starting to build up a web site around all of these things . i 'd like to be able to pore through , the types of tr conventions that you 've come up with and like that . we can add in links and like that to other things . mostly internal . what is the , projector supposed to be hooked up to ? ","The prospect of sending the data to an external transcription service was weighed against that of hiring a graduate student transcriber pool. It was tentatively decided that the latter option would be less costly and allow BMR to maintain greater control over the transcription process. Methods for distributing the data were briefly discussed, along with an initiative for creating a BMR project website. "
Bmr003.D,"but is there anyway to wire a speech recognizer up to it first of all the time marking you 'd get you could get by a tool . are those d delays adjustable ? and the thing to keep in mind too about this tool , guys is that you can do the computation for what we 're gonna do in the future but if uw 's talking about doing two , or three , or five times as much and they can use the same tool , then there 's a real multiplier there . just hypoth hypothetically assuming that we go ahead and ended up using graduate students . did you ask them to change the interface for more speakers ? but you 're saying that by the time you call it back in to from their saved format it opens up a window with five speakers ? your script solves doesn't it solve all our problems , there 's a lip in these tables . that 's the six tables that we 're looking at . these six tables here , with little boxes in the middle here . the box is an inch thick it hangs down a half an inch . and the two head set jacks would be in the front and then the little led to indicate that box is live . the important issue about the led is the fact that we 're talking about eight of these total , which would be sixteen channels . this notion of putting down the p z ms and taking them away would somehow have to be turned into leaving them on the table and to see , thi this is really the way people sit on this table . thi this box is a one off deal . and , it 's function is to s to , essentially a wire converter to go from these little blue wires to these black wires , plus supply power to the microphones cuz the he the , cheap head mounteds all require low voltage . 'm ready to build it . which is to say , the neighborhood of a week to get the circuit board done . can build a cabinet . no , but w certainly one of the issues is the , is security . which is to say just laptop with a wireless . either we figure out how to use a machine somebody already in the group already owns , a and the idea is that if it 's it perk , it 's an advantage not a disadvan or else we literally buy a machine e exactly for that purpose . certainly it solves a lot of the problems with leaving a monitor out here all the time . there 's gonna be actually a plug at the front that 'll connect to people 's laptops you can walk in and plug it in . and it 's gonna be con connected to the machine at the back . we certainly could use that as a constant reminder of what the vu meters are doing . but the idea of having a control panel it 's that 's there in front of you is really ",
Bmr003.E,"they had people come in to a certain degree and they have dat recorders . csae . corpus of spoken american english . as a pilot study . there 's some interesting human factors problems like , what span of time is it useful to segment the thing into in order to transcribe it the most quickly . mean , i 've been playing with , different ways of mar cuz i 'm thinking , if you could get optimal instructions you could cut back on the number of hours it would take . but then , it 's like , you press the tab key to stop the flow and , the return key to p to put in a marking of the boundary . but , there 's a lag between when you hear it and when you can press the return key could you get it that with it would detect volume on a channel and insert a marker ? but there 's an extra problem which is that i didn't really keep accurate it wasn't a pure task the first time , think though it 's a good proposal to be used on a new batch of text that i haven't yet done yet in the same meeting . could use it on the next segment of the text . and the other thing too is with speaker identification , and then he has a script that will convert it into the thing that , would indicate speaker id . the other thing too is that , if they were linguistics they 'd be in terms of like the post editing , i tu content wise they might be easier to handle cuz they might get it more right the first time . no , that i would be interested in that in becoming involved in the project in some aspect like that as far as i 'm concerned those transcription conventions are fixed right now . one of them is the idea of how to indicate speaker change , and but in terms of the con the conventions , then , it 's strictly orthographic which means with some w provisions for , w colloquial forms . if it if there was a word like "" right "" , then i wou i would indicate that it happened within the same tem time frame but wouldn't say exactly when it happened . my focus was to try and maintain conten con content continuity and , to keep it within what he was saying . like i wouldn't say breath groups but prosodic or intonational groups as much as possible . if someone said "" in the middle of a of someone 's , intonational contour , i indicated it as , like what you just did . then i indicated it as a segment which contained @ @ this utterance plus an overlap .  and then , in terms of like words like "" and "" wrote them which allows five . and it can be m edited after the fact , and that works nicely cuz this quick to enter . wouldn't wanna do it through the interface anyway adding which worry who the speaker was . in terms of like the continuity of thought for transcriptions , it 's i it isn't just words coming out , have a convention of putting like a dash arrow just to indicate that this person 's utterance continues . sometimes we had the situation which is which you get in conversations , of someone continuing someone else 's utterance , and in that case i did a tilde arrow versus a arrow tilde , to indicate that it was continuation but just , the arrows showing continuation of a thought . and you 'd be able to look for the continuation . we could do an ht access which would accommodate those things . and th "" that "" being a diagram . which means two at each station . ","The Transcriber software tool was introduced, along with a set of transcription conventions for coding different speech events. "
Bmr006.A,"because can't you , couldn't you like use beam forming to detect speaker overlaps ? ",
Bmr006.B,"that , think that the only thing we should say in the advertisement is that the meeting should be held in english . and you don't really need the close microphone , i had a i spoke with some people up at haas business school who volunteered . should i pursue that ? throughout the meeting . and are you planning to do that or have you done that already ? supervised clustering . if you used the array , rather than the signal from just one . and if there are multiple people talking , you 'll see two peaks . not only can you do microphone arrays , but you can do all sorts of multi band as and then also anonymity , how we want to anonymize the data . because that would give you a mapping between the speaker 's real name and the tag we 're using , and we don't want because if we made the transcript be the tag that we 're using for roger , someone who had the transcript and the audio would then have a mapping between the anonymized name and the real name , and we wanna avoid that . i would sug i don't wanna change the names in the transcript , but that 's because i 'm focused much on the acoustics instead of on the discourse , and think that 's a really good point . you 're right , this is going to require more thought . how will we how would the person who 's doing the transcript even know who they 're talking about ? how is that information gonna get labeled anyway ? the current one they don't do speaker identity . because in naturallyspeaking , or , excuse me , in viavoice , it 's only one person . and in their current conventions there are no multiple speaker conventions . ","And, finally, the problem of speaker anonymization was explored. "
Bmr006.C,"i have , the result of my work during the last days . this information is very useful . because you have the distribution , now . but for me , is interesting because , here 's i is the demonstration of the overlap , problem . it 's a real problem , a frequently problem because you have overlapping zones all the time . by a moment i have , nnn , the , n i did a mark of all the overlapped zones in the meeting recording , heh ? that 's yet b by b by hand because , why . "" but , my idea is , is very interesting to work in the line of , automatic segmenter . no , i plan to do that . now , i need ehm , to detect all the overlapping zones exactly . this information with exactly time marks for the overlapping zones overlapping zone , and a speaker a pure speech speaker zone . zones of speech of one speaker without any noise any acoustic event that w is not speech , real speech . for that , because my idea is to study the nnn the set of parameters what , are more m more discriminant to classify . the overlapping zones in cooperation with the speech zones . the idea is to to use i 'm not to yet , but my idea is to use a cluster algorithm or , nnn , a person strong in neural net algorithm to study what is the , the property of the different feat feature , to classify speech and overlapping speech . and my control set will be the silence without any noise . event which , has , a hard effect of distorti spectral distortion in the speech . i have , two hundred and thirty , more or less , overlapping zones , and is similar to this information , because i want to put , for each frame a label indicating . it 's a sup supervised and , hierarchical clustering process . i put , for each frame a label indicating what is th the type , what is the class , which it belong . a i ha i h i put the mark by hand , because , my idea is , in the first session , i need , to be that the information that , i will cluster , is right . because , if not , i will i will , return to the speech file to analyze what is the problems ,  in the future , the idea is to extend the class , and am going to prepare a test bed , a set of feature structure models . on because i have a pitch extractor yet . and in a first nnn , step in the investi in the research in my idea is try to , to prove , what is the performance of the difference parameter , to classify the different , what is the front end approach to classify the different , frames of each class and what is the , nnn , nnn , what is the , the error of the data and the second is try to to use some ideas similar to the linear discriminant analysis . the classifier is nnn by the moment is similar , nnn , that the classifier used in a quantifier vectorial quantifier is used to some distance to put a vector in a class different . a another possibility it to use netw a neural network . but understand that you your objective is to classify , to know that that zone not is only a new zone in the file , that you have but you have to know that this is overlap zone . because in the future you will try to process that zone with a non regular speech recognizer model , i suppose . you will pretend to process the overlapping z zone with another algorithm because it 's very difficult to obtain the transcription from using regular , normal speech recognizer . a model to detect more acc the mor most accurately possible that is p will be possible the , the mark , the change and another model will @ @ or several models , to try s but several model robust models , sample models to try to classify the difference class . but what is the relation of of the performance when you use the , speech file the pda speech files . ",Ongoing efforts by speaker mn005 to automatically  detect regions of speaker overlap were considered. 
Bmr006.D,"let 's see , we were having a discussion the other day , we should bring that up , about the nature of the data that we are collecting . @ that we should have a fair amount of data that is collected for the same meeting , that we can , for other kinds of research , particularly the acoustic oriented research , i actually feel the opposite need . i 'd like to have many different speakers . would also very much like us to have a fair amount of really random scattered meetings , of somebody coming down from campus , and but if we only get one or two from each group , that still could be useful acoustically just because we 'd have close and distant microphones with different people . was thinking more in terms of talking to professors and senior d and doctoral students who are leading projects and offering to them that they have their hold their meeting down here . it that if we 're aiming at groups of graduate students and professors and forth who are talking about things together , and it 's from the berkeley campus , probably most of it will be the other thing the other thing that i was hoping to do in the first place was to turn it into some portable thing you could wheel it around . i we realized in discussion that the other thing is , what about this business of distant and close microphones ? we really wanna have a substantial amount recorded this way , but what about for th for these issues of summarization , a lot of these higher level things you don't really need the distant microphone . but that the we can't really underestimate the difficulty shouldn't really u underestimate the difficulty of getting a setup like this up . if you 're talking about something simple , where you throw away a lot of these dimensions , then you can do that right away . the first priority should be to pry to get try to get people to come here . the o the other thing is , that we talked about is give to them burn an extra cd rom . and then another topic would be where are we in the whole disk resources question we are getting , another disk rack and four thirty six gigabyte disks . but that 's not gonna happen instantaneously . @ @ then th the last thing i 'd had on my agenda was just to hear an update on what jose has been doing , cuz you 're calling what you 're calling "" event "" is somebody coughing or clicking , or rustling paper , or hitting something , which are impulsive noises . but steady state noises are part of the background . right , it 's it 's "" background "" might be a better word than "" silence "" . but here he 's not he 's not like he has one a bunch of very distinct variables , like pitch and this he 's talking about all these cepstral coefficients , and forth , in which case a any reasonable classifier is gonna be a mess , and it 's gonna be hard to figure out what i would take just a few features . instead of taking all the mfcc 's , or all the plp 's or whatever , i would just take a couple . like c one , c two , something like that , that you can visualize it . and look at these different examples and look at scatter plots . before you do build up any fancy classifiers , just take a look in two dimensions , at how these things are split apart . if you 're just looking at a frame and a time , you don't know anything about , the structure of it over time , and you may wanna build @ @ build a markov model of some sort or else have features that really are based on on some bigger chunk of time . but don't anyway , this is my suggestion , is don't just , throw in twenty features at it , the deltas , and the delta del and all that into some classifier , even if it 's k nearest neighbors , you still won't know look at som some picture that shows you , "" here 's these things are offer some separation . "" and , in lpc , the thing to particularly look at is , is something like , the residual because what we had before for speaker change detection did not include these overlaps . the first thing is for you to build up something that will detect the overlaps . if you look at suppose you look at first and second order cepstral coefficients for some one of these kinds of things and you find that the first order is much more effective than the second , and then you look at the third and there 's not and not too much there , you may just take first and second order cepstral coefficients , and with lpc , lpc per se isn't gonna tell you much more than the other , and on the other hand , the lpc residual , the energy in the lpc residual , will say how the low order lpc model 's fitting it , which should be pretty poorly for two or more people speaking at the same time , and it should be pretty for w for one . and then you can do decision trees or whatever to see how they combine . ","It was agreed that a substantial amount of meeting data is required from different domains, and comprising several speakers, to perform the types of discourse and acoustic analyses desired. Disk space issues were discussed. It was suggested that speaker mn005 focus on a small set of acoustic parameters, e.g. energy and harmonics-related features, to distinguish regions of overlap from those containing the speech of just one speaker. Ongoing efforts by speaker mn005 to automatically  detect regions of speaker overlap were considered. "
Bmr006.D,"and if you can get @ @ again , my prescription would be that you would , with a mixed signal , you would take a collection of possible features look at them , look at how these different classes that you 've marked , separate themselves , and then collect , in pairs , and then collect ten of them and then proceed with a bigger classifier . and then if you can get that to work then you go to the other signal . that that 's another reason why very simple features , things like energy , and things like harmonicity , and residual energy are are better to use than very complex ones because they 'll be more reliable . think it 's it 's a it 's an additional interesting question . that 's a good thing to consider . if there 's a distributed beam pattern , then it looks more like it 's multiple people . it 's spread out . but one of the at least one of the things i was hoping to get at with this is what can we do with what we think would be the normal situation if some people get together and one of them has a pda . but if you can instrument a room , this is really minor league compared with what some people are doing , right ? but , the reason why i haven't focused on that as the fir my first concern is because i 'm interested in what happens for people , random people out in some random place where they 're p having an impromptu discussion . the other thing actually , that gets at this a little bit of something else i 'd like to do , is what happens if you have two p d and they communicate with each other ? and then they 're in random positions , the likelihood that there wouldn't be any l likely to be any nulls , if you even had two . if you had three or four it 's again , th the issue is if you 're tracking discourse things , if someone says , "" frank said this "" and then you wanna connect it to something later , you 've gotta have this part where that 's "" frank colon "" . that we have a need to have a consistent licensing policy of some sort , and and th and the other thing is if liz were here , what she might say is that she wants to look if things that cut across between the audio and the dialogue , ","It was suggested that speaker mn005 focus on a small set of acoustic parameters, e.g. energy and harmonics-related features, to distinguish regions of overlap from those containing the speech of just one speaker. "
Bmr006.E,"if you have people who are using english as a as an interlanguage because they don't they can't speak in their native languages and but their interlanguage isn't really a match to any existing , language model , and i 'm not objecting to accents . i 'm i what is that why not have the corpus , since it 's expensive to put together , useful for the widest range of central corp things that people generally use corpora for and which are , used in computational linguistics . and my point in m in my note to liz was that undergrads are an iff iffy population . we could burn it after it 's been cleared with the transcript stage . after the transcript screening phase . you 're ignoring overlapping events unless they 're speech with speech . the question becomes what symbol are you gonna put in there for everybody 's name , and whether you 're gonna put it in the text where he says "" hey roger "" or are we gonna put that person 's anonymized name in instead ? then , it seems to me that it seems to me that if you change the name , the transcript 's gonna disagree with the audio , and you won't be able to use that . but then there 's this issue of if we 're gonna use this for a discourse type of thing , then and , liz was mentioning in a previous meeting about gaze direction and who 's the addressee and all , then to have "" roger "" be the thing in the utterance and then actually have the speaker identifier who was "" roger "" be "" frank "" , that 's going to be really confusing and make it useless for discourse analysis . how important is it for a person to be identified by first name versus full name ? on the other hand , this is a small pool , and people who say things about topic x e who are researchers and known in the field , they 'll be identifiable and simply from the first name . however , taking one step further back , they 'd be identifiable anyway , even if we changed all the names . now , it would be very possible for me to take those data put them in a study , and just change everybody 's name for the purpose of the publication . and even more i immediate than that just being able to , it just seems like to track from one utterance to the next utterance who 's speaking and who 's speaking to whom , cuz that can be important . and within that , it may be that it 's sufficient to not change the to not incorporate anonymization yet , but always , always in the publications we have to . you see ? it 's complicated . we have to think about w @ @ how . that this can't be decided today . ","And, finally, the problem of speaker anonymization was explored. "
Bmr006.F,"and was thinking of this mostly just that we could do research on this data since we 'll have a new this new student di does wanna work with us , he 's comes from a signal processing background , cuz he 's very interested in higher level things , like language , and disfluencies and all kinds of eb prosody , but i 'd rather try to get more regular meetings of types that we know about , and hear , then mish mosh of a bunch of one time then i was talking to morgan about some new proposed work in this area , separate issue from what the student would be working on where i was thinking of doing some summarization of meetings or trying to find cues in both the utterances and in the utterance patterns , like in numbers of overlaps and amount of speech , raw cues from the interaction that can be measured from the signals and from the diff different microphones that point to hot spots in the meeting , or things where is going on that might be important for someone who didn't attend to listen to . and in that regard , we definitely w will need it 'd b it 'd be for us to have a bunch of data from a few different domains , or a few different kinds of meetings . like the front end meeting and networking group meeting . if that were the case then we 'd have enough . but rather we should have different meetings by the same group but hopefully that have different summaries . but also data where we hold some parameters constant or fairly similar , like a meeting about of people doing a certain work where at least half the participants each time are the same . it has to be a pre existing meeting , like a meeting that would otherwise happen anyway . morgan , you were mentioning that mari may not use the k equipment from ibm if they found something else , cuz there 's a cuz one remote possibility is that if we st if we inherited that equipment , if she weren't using it , could we set up a room in the linguistics department ? and there may be a lot more or in psych , or in comp wherever , in another building where we could record people there . each person who 's interested in we have a cou we have a bunch of different , slants and perspectives on what it 's useful for , they need to be taking charge of making they 're getting enough of the data that they want . and in my case , there w there is enough data for some kinds of projects and not enough for others . and other people need to do that for themselves , h or at least discuss it that we can find some optimal do think that long term you should always try to satisfy the greatest number of interests and have this parallel information , which is really what makes this corpus powerful . but the issue is you definitely wanna make that the group you 're getting is the right group definitely , i 'd love to get people that are not linguists or engineers , cuz these are both weird ","The Berkeley Meeting Recorder group discussed research aims and corresponding concerns for future data collection. It was agreed that a substantial amount of meeting data is required from different domains, and comprising several speakers, to perform the types of discourse and acoustic analyses desired. "
Bmr007.A,"it 'd be interesting to see what the total amount of time is in the overlaps , versus no , when there 's backchannel , just i was just listening , and when there 's two people talking and there 's backchannel it seems like , the backchannel happens when , the pitch drops and the first person ",
Bmr007.B,"the question is , how many more overlaps do you have of , say the two person type , by adding more people . to a meeting , i agree that it 's an issue here but it 's also an issue for switchboard it may be that having three people is very different from having two people or it may not be . from the point of view of studying dialogue , which dan jurafsky and andreas and i had some projects on , you want to know the sequence of turns . what happens is if you 're talking and i have a backchannel in the middle of your turn , and then you keep going what it looks like in a dialogue model is your turn and then my backchannel , even though my backchannel occurred completely inside your turn . for things like language modeling or dialogue modeling it 's we know that 's wrong in real time . but , because of the acoustic segmentations that were done and the fact that some of the acoustic data in switchboard were missing , people couldn't study it , it 's important to distinguish that , this project is getting a lot of overlap but other projects were too , but we just couldn't study them . but we should still be able to somehow say what is the added contra contribution to overlap time of each additional person , like that . there 's a lot of the kind that jose was talking about , where this is called "" precision timing "" in conversation analysis , where they come in overlapping , but at a point where the information is mostly complete . all you 're missing is some last syllables or the last word or some highly predictable words . technically , it 's an overlap . but from information flow point of view it 's not an overlap in the predictable information . that 's exactly , exactly why we wanted to study the precise timing of overlaps ins in switchboard , i was thinking you should be able to do this from the acoustics , on the close talking mikes , my first comment was , only that we should n not attribute overlaps only to meetings , but that in normal conversation with two people there 's an awful lot of the same kinds of overlap , and that it would be interesting to look at whether there are these kinds of constraints that jane mentioned , that what the additional people add to this competition that happens right after a turn , to answer your question i it i don't think it 's crucial to have controls but it 's worth recording all the meetings we can . it 'll be too hard to make barriers , i was thinking because they have to go all the way we need a barrier that doesn't disturb the sound , ",Efforts by speakers fe008 and fe016 are in progress to categorize and subcategorize types of overlapping speech and evaluate the contribution of multiple speakers in an interaction to the amount and types of overlap observed. 
Bmr007.C,"wh what is "" audio pixelization "" ? ",
Bmr007.D,"th the biggest , result here , which is one we 've talked about many times and isn't new to us , but which would be interesting to show someone who isn't familiar with this is just the sheer number of overlaps . here 's a relatively short meeting , it 's a forty plus minute meeting , and not only were there two hundred and fifteen overlaps but , think there 's one minute there where there wasn't any overlap ? we talked over the minute boundary . is this considered as one overlap in each of the minutes , the way you have done this . but what is that , in switchboard , despite the many other problems that we have , one problem that we 're not considering is overlap . and what we 're doing now is , aside from the many other differences in the task , we are considering overlap and one of the reasons that we 're considering it , one of them not all of them , one of them is that w at least , 'm very interested in the scenario in which , both people talking are equally audible , and from a single microphone . and in that case , it does get mixed in , and it 's pretty hard to jus to just ignore it , to just do processing on one and not on the other . what we 've learned about is overlaps in this situation , is that the first order thing i would say is that there 's a lot of them . and it 's not just an overlap bunch of overlaps second order thing is it 's not just a bunch of overlaps in one particular point , but that there 's overlaps , throughout the thing . i have a feeling most of these things are that are not a benevolent kind are , are competitive as opposed to real really hostile . the other thing i was thinking was that , these all these interesting questions are , pretty hard to answer with , a small amount of data . we most of our meetings are meetings currently with say five , six , seven , eight people should we really try to have some two person meetings , or some three person meetings and re record them just to beef up the statistics on that ? we 're not really set up for it to do that . weren't we gonna take a picture at the beginning of each of these meetings ? how are you going to adapt whatever you can very quickly learn about the new data ? if it 's gonna be different from old data that you have ? and that 's a problem with this . but what if you were just looking at very simple measures like energy measures but you don't just compare it to some threshold overall but you compare it to the energy in the other microphones . but what i was s nnn noting just when he when andreas raised that , was that there 's other information to be gained from looking of the microphones and you may not need to look at very sophisticated things , this is the things that we did in the last three months in no particular order one , ten more hours of meeting r meetings recorded , something like that , from , three months ago . pilot data put together and sent to ibm for transcription , next batch of recorded data put together on the cd roms for shipment to ibm , human subjects approval on campus , and release forms worked out the meeting participants have a chance to request audio pixelization of selected parts of the spee their speech . audio pixelization software written and tested . preliminary analysis of overlaps in the pilot data we have transcribed , and exploratory analysis of long distance inferences for topic coherence , that was i was wasn't if those were the right way that was the right way to describe that because of that little exercise that you and lokendra did . ","Speaker fe008 presented raw counts and percentages for one transcribed meeting, revealing a large number of overlaps throughout the 40-plus-minute transcript. The Berkeley Meeting Recorder group focussed its discussion on overlapping speech segments. The group also tentatively discussed the erection of visual barriers during meeting recordings, and speaker me013 presented a list of work performed by BMR over the previous three months to be included in a forthcoming report to IBM. "
Bmr007.E,"but , will do the study of the with the program with the the different , the , nnn , distribution of the duration of the overlaps . the duration is , the variation of the duration is very big on the dat it 's difficult , only to en with energy to consider that in that zone we have overlapping zone if you process only the energy of the , of each frame . ",
Bmr007.F,"as part of the encoding includes a mark that indicates an overlap . it 's not indicated with , tight precision , it 's just indicated that it 's indicated to the people parts of sp which stretches of speech were in the clear , versus being overlapped by others . and , what you can see is the number of overlaps and then to the right , whether they involve two speakers , three speakers , or more than three speakers . and , and , what i was looking for sp specifically was the question of whether they 're distributed evenly throughout or whether they 're bursts of them . this is just this would this is not statistically verified , but it did look to me as though there are bursts throughout , rather than being localized to a particular region . the part down there , where there 's the maximum number of , overlaps is an area where we were discussing whether or not it would be useful to indi to s to code stress , sentence stress as possible indication of , information retrieval . now , another question is there are there individual differences in whether you 're likely to be overlapped with or to overlap with others . i , my i had this script figure out , who was the first speaker , who was the second speaker involved in a two person overlap , i didn't look at the ones involving three or more . and , then if you look down in the summary table , then you see that , th they 're differences in whether a person got overlapped with or overlapped by . raw counts . of the times a person spoke and furthermore was involved in a two person overlap , what percentage of the time were they the overlapper and what percent of the time were they th the overlappee ? that some people tend to be overlapped with more often than they 're overlapped , but , e this is just one meeting , there 's no statistical testing involved , and that would be required for a finding of any scientific reliability . and actually , not about an individual , it 's the point about tendencies toward different styles , different speaker styles . and it would be , there 's also the question of what type of overlap was this , and w what were they , and i and i know that distinguish at least three types and , probably more , then it beco though just superficially to give couple ideas of the types of overlaps involved , i have at the bottom several that i noticed . there are backchannels , like what adam just did now and , anticipating the end of a question and simply answering it earlier , and places also which were interesting , where two or more people gave exactly th the same answer in unison different words that , overlap 's not necessarily a bad thing and that it would be im i useful to subdivide these further and see if there are individual differences in styles with respect to the types involved . le let 's think about the case where a starts speaking and then b overlaps with a , and then the minute boundary happens . and let 's say that after that minute boundary , is still speaking , and a overlaps with b , that would be a new overlap . but otherwise let 's say b comes to the conclusion of that turn without anyone overlapping with him or her , in which case there would be no overlap counted in that second minute . we just haven't done th the precise second to sec second to second coding of when they occur . no , it wouldn't . it would be considered as an overlap in the first one . if a truck goes rolling past , adults will depending , but mostly , adults will hold off to what to finish the end of the sentence till the noise is past . and we generally do monitor things like that , about whether we whether our utterance will be in the clear or not . and partly it 's related to rhythmic structure in conversation , and , just to finish this , that that that there may be an upper bound on how many overlaps you can have , simply from the standpoint of audibility and how loud the other people are who are already in the fray . now if it 's just backchannels , people may be doing that with less intention of being heard , just spontaneously doing backchannels , in which case that those might there may be no upper bound on those . these were benevolent types , as people finishing each other 's sentences , and the because you get then the spatial relationship of the speakers . what i what this has , caused me this discussion caused me to wanna subdivide these further . i 'm gonna take a look at the , backchannels , how much we have anal i hope to have that for next time . ","Speaker fe008 presented raw counts and percentages for one transcribed meeting, revealing a large number of overlaps throughout the 40-plus-minute transcript. Efforts by speakers fe008 and fe016 are in progress to categorize and subcategorize types of overlapping speech and evaluate the contribution of multiple speakers in an interaction to the amount and types of overlap observed. "
Bmr007.G,"other otherwise you 'd get double counts , here and there . actually , that 's in part because the nodding , if you have visual contact , the nodding has the same function , here 's a first interesting labeling task . to distinguish between , say , backchannels precision timing benevolent overlaps , and w and don't know , hostile overlaps , where someone is trying to grab the floor from someone else . if the goal were to just look at overlap you would you could serve yourself save yourself a lot of time but not even transcri transcribe the words . we have in the past and continue will continue to have a fair number of phone conference calls . and , and as a to , as another c comparison condition , we could see what happens in terms of overlap , when you don't have visual contact . or , this is getting a little extravagant , we could put up some blinds to remove , visual contact . you could do that by just noting on the enrollment sheet the seat number . put them like , put them on the table where they but , can imagine building a model of speaker change detection that takes into account both the far field and the actually , not just the close talking mike for that speaker , but actually for all of th for all of the speakers . ","The group also tentatively discussed the erection of visual barriers during meeting recordings, and speaker me013 presented a list of work performed by BMR over the previous three months to be included in a forthcoming report to IBM. "
Bmr007.H,"cuz i find it interesting that there were a large number of overlaps and they were all two speaker . language model prediction of overlap , that would be really interesting . what i had thought we were gonna do is just take pictures of the whiteboards . rather than take pictures of the meeting . seat number , that 's a good idea . i 'll do that on the next set of forms . 'm gonna put little labels on all the chairs with the seat number . i 've been playing with , using the close talking mike to do to try to figure out who 's speaking . my first attempt was just using thresholding and filtering , that we talked about two weeks ago , if you fiddle around with it a little bit and you get good numbers you can actually do a pretty good job of segmenting when someone 's talking and when they 're not . but if you try to use the same paramenters on another speaker , it doesn't work anymore , even if you normalize it based on the absolute loudness . it does work for the one speaker throughout the whole meeting . the algorithm was , take o every frame that 's over the threshold , and then median filter it , and then look for runs . there was a minimum run length , you take a each frame , and you compute the energy and if it 's over the threshold you set it to one , and if it 's under the threshold you set it to zero , now you have a bit stream of zeros and ones . and then i median filtered that using , fairly long filter length . actually depends on what by long , tenth of a second sorts of numbers . and that 's to average out pitch , the pitch contours , and things like that . and then , looked for long runs . and that works o k , if you fil if you tune the filter parameters , if you tune how long your median filter is and how high you 're looking for your thresholds . and then the other thing i did , was i took javier 's speaker change detector acoustic change detector , and i implemented that with the close talking mikes , and unfortunately that 's not working real and it looks like it 's the problem is he does it in two passes , the first pass is to find candidate places to do a break . and he does that using a neural net doing broad phone classification and he has the , one of the phone classes is silence . and then he has a second pass which is a modeling a gaussian mixture model . looking for whether it improves or degrades to split at one of those particular places . and what looks like it 's happening is that the even on the close talking mike the broad phone class classifier 's doing a really bad job . at any rate , my next attempt , which i 'm in the midst of and haven't quite finished yet was actually using the thresholding as the way of generating the candidates . and then feeding that into the acoustic change detector . but all of this is close talking mike , my intention for this is as an aide for ground truth . also what i 'm doing right now is not intended to be an acoustic change detector for far field mikes . what i 'm doing is trying to use the close talking mike and just use can and just generate candidate and just try to get a first pass at something that works . i was thinking about doing that originally to find out who 's the loudest , but i also wanted to find threshold excuse me , mol overlap . it 's just , beeping out parts that you don't want included in the meeting ",Speaker me011 described his attempts to automatically identify speakers via the close-talking microphone channels using thresholding and filtering methods and an existing speaker-change detection algorithm. 
Bmr013.A,"new version of the presegmentation . worked a little bit on the presegmentation to get another version which does channel specific , speech nonspeech detection . and , what i did is i used some normalized features which , look in into the which is normalized energy , energy normalized by the mean over the channels and by the , minimum over the , other . within each channel . and to , to , to normalize also loudness and modified loudness and things and that those special features actually are in my feature vector . and , therefore to be able to , somewhat distinguish between foreground and background speech in the different in each channel . and , i tested it on three or four meetings and it seems to work , fairly i would say . there are some problems with the lapel mike . then i did some things like that , then , the , there are some problems with n with normalization , and , then , there the system doesn't work and , , then the evaluation of the system is a little bit hard , as i don't have any references . that that 's great , but what would be to have some more meetings , not just one meeting to be that , there is a system , it seems to me that it would be good to have , a few minutes from different meetings , i could do a retraining with that , but , there are some as i said some problems with the lapel mike , but , perhaps we can do something with cross correlations to , to get rid of the of those . what i want to do is to look into cross correlations for removing those , false overlaps . that perhaps the transcribers could start then from the those mult multi channel , speech nonspeech detections , if they would like to . ",Pre-segmentation manipulations that allow for the segmentation of channel-specific speech/non-speech portions of the signal and the distinction of foreground versus background speech were discussed. 
Bmr013.B,"and it 's a fine idea partly because , it 's not un unrelated to their present skill set , might have done what you 're requesting , though i did it in the service of a different thing . i have thirty minutes that i 've more tightly transcribed with reference to individual channels . would you be training then , the segmenter that , it could , on the basis of that , segment the rest of the meeting ? it drifted into the afternoon , concerning this issue of , the , there 's the issue of the interplay between the transcript format and the processing that , they need to do for , the sri recognizer . and the , what we discussed this morning , i would summarize as saying that , these units that result , in a particular channel and a particular timeband , at that level , vary in length . and , their recognizer would prefer that the units not be overly long . as a first pass through , a first chance without having to do a lot of hand editing , what we 're gonna do , is , i 'll run it through channelize , give them those data after i 've done the editing process and be it 's clean . and do that , pretty quickly , with just , that minimal editing , without having to hand break things . and then we 'll see if the units that we 're getting , with the at that level , are sufficient . and if they do need to be further broken down then it just be piece wise , it won't be the whole thing . also we discussed some adaptational things , hadn't , incorporated , a convention explicitly to handle acronyms , and then , a similar conv convention for numbers . and also i 'll be , encoding , as i do my post editing , the , things that are in curly brackets , which are clarificational material . it 's gonna be either a gloss or it 's gonna be a vocal sound like a , laugh or a cough , or , forth . or a non vocal sound like a doors door slam , and that can be easily done with a , just a one little additional thing in the , in the general format . that 's very important . the microphones the new microphones , ","Finally, speaker fe008 and fe016 reported on new efforts to adapt transcriptions to the needs of the SRI recognizer, including conventions for encoding acronyms, numbers, ambient noise, and unidentified inbreaths. "
Bmr013.C,"new version of presegmentation . update on transcripts . i 'm impressed by what we could do , is take the standard training set for ti digits , train up with whatever , great features we think we have , and then test on this test set . and presumably it should do reasonably on that , and then , presumably , we should go to the distant mike , and it should do poorly . just by way of a order of magnitude , we 've been working with this aurora , data set . and , the best score , on the , nicest part of the data , that is , where you 've got training and test set that are the same kinds of noise and forth , is about , the best score was something like five percent , error , per digit . the point there , and this is car noise things , but real situation , the there 's one microphone that 's close , that they have as this thing , close versus distant . but in a car , instead of having a projector noise it 's car noise . that we could have done better on the models , but that we got this is the typical number , for all of the , things in this task , all of the , languages . anyway , just an indication once you get into this realm even if you 're looking at connected digits it can be pretty hard . i th no we got under a percent , but it was but it 's but the very best system that i saw in the literature was a point two five percent that somebody had at bell labs , or . but . but , pulling out all the stops . one question i have that we wouldn't know the answer to now but might , do some guessing , but i was talking before about doing some model modeling of arti marking of articulatory , features , with overlap and on . one thought might be to do this on the digits , or some piece of the digits . the reason for doing it is because the argument is that certainly with conversational speech , the that we 've looked at here before , just doing the simple mapping , from , the phone , to the corresponding features that you could look up in a book , isn't right . there 's these overlapping processes where some voicing some up and then some , some nasality is comes in here , and forth . you should . it should be such that if you , if you had o all of the features , determined that you were ch have chosen , that would tell you , in the steady state case , the phone . i i 'm jus at the moment we 're just talking about what , to provide as a tool for people to do research who have different ideas about how to do it . mean i we 'll see wha how much we can , get the people to do , and how much money we 'll have and all this thing , mean that 's probably the right way to go anyway , is to start off with an automatic system with a pretty rich pronunciation dictionary that , tries , to label it all . and then , people go through and fix it . another way to look at this is to , do some on switchboard which has all this other , to it . and then , as we get , further down the road and we can do more things ahead of time , we can , do some of the same things to the meeting data . and having as much variety for speaker certainly would be a big part of that this , blends nicely into the update on transcripts . ","Anticipated results were discussed in reference to results obtained for other digits corpora, i.e. Aurora and TI-digits. The group also considered the prospect of performing fine-grained acoustic-phonetic analyses on a subset of Meeting Recorder digits or Switchboard data. "
Bmr013.D,"we 'll have a corpus that 's the size of ti digits ? do we do on ti digits ? mean if we 're talking about , having the , annotators annotate these kinds of features , it seems like , the question is , do they do that on , meeting data ? but it might be good to do what jane was saying seed it , with , guesses about what we think the features are , based on , the phone or steve 's transcriptions to make it quicker . ",
Bmr013.E,,
Bmr013.F,"the , w as you can see from the numbers on the digits we 're almost done . the digits goes up to about four thousand . and we probably will be done with the ti digits in , another couple weeks . depending on how many we read each time . and once we 're it 's done it would be very to train up a recognizer and actually start working with this data . one particular test set of ti digits . but , in order to do that we need to extract out the actual digits . that the reason it 's not just a transcript is that there 're false starts , and misreads , and miscues and things like that . and have a set of scripts and x waves where you just select the portion , hit r , it tells you what the next one should be , and you just look for that . and the question is , should we have the transcribers do that or should we just do it ? the prosodics are not the same as ti digits , just what we were talking about with grouping . that with these , the grouping , there 's no grouping and it 's just the only discontinuity you have is at the beginning and the end . but that it 's really it 's close talking mikes , no noise , clean signal , just digits , every everything is good . what i 'll do then is i 'll go ahead and enter , this data . and then , hand off to jane , and the transcribers to do the actual extraction of the digits . but what i 'm imagining is a score like notation , where each line is a particular feature . the other difference is that the features , are not synchronous , they overlap each other in weird ways . it 's not a strictly one dimensional signal . not with our current system but you could imagine designing a system , that the states were features , rather than phones . new use ninetieth quartile , rather than , minimum . if we could get a couple meetings done with that level of precision that would be a good idea . since , what i decided to do , on morgan 's suggestion , was just get two , new microphones , and try them out . it 's , it 's by crown , and it 's one of these mount around the ear thingies , ",The Berkeley Meeting Recorder group discussed the collection status for a set of connected digits recordings that are nearly complete and ready to be trained on a recognizer. 
Bmr013.G,"and that includes some the filtering for the , the asi refs , too . you can add the features in , but it 'll be underspecified . and then there 's also , things like door slams that 's really in no one 's channel , and jane had this idea of having , like an extra , couple tiers , and we were thinking , that is useful also when there 's uncertainties . if they hear a breath and they don't know who breath it is it 's better to put it in that channel than to put it in the speaker 's channel the idea is then , don can take , jane 's post processed channelized version , and , with some scripts , convert that to a reference for the recognizer when that 's , ready as soon as that 's ready , and as soon as the recognizer is here we can get , twelve hours of force aligned and recognized data . it 's probably good enough for force alignment . but for free recognition i 'm it 'll probably not be good enough . we 'll probably get lots of errors because of the cross talk , and , noises and things . ","Finally, speaker fe008 and fe016 reported on new efforts to adapt transcriptions to the needs of the SRI recognizer, including conventions for encoding acronyms, numbers, ambient noise, and unidentified inbreaths. "
Bmr014.A,"it 's just i ran the recognizer the speech nonspeech detector on different channels and , it 's just in in this new multi channel format and output , as the recognizer had problems with those long chunks of speech , which took too much memory or whatever , the basic thing is this base . and i tried t to normalize the features , there 's loudness and modified loudness , within one channel , because they 're , to be able to distinguish between foreground and background speech . and it works quite but , not always . ",Speaker mn014 briefly described his efforts to normalize loudness levels across speech channels to distinguish between foreground and background speech. 
Bmr014.B,""" spikes "" , like instantaneous click type spikes , or ? we 're not increasing the number of channels . are people going to be allowed to bleep out sections of a meeting where they weren't speaking ? that means other people are editing what you say ? ",
Bmr014.C,"i wanna talk a little bit about getting how we 're gonna to get people to edit bleeps , parts of the meeting that they don't want to include . the new microphones , the two new ones are in . and they are being assembled as we speak , i hope . first of all , if the other headsets are a lot more comfortable , we should probably just go ahead and get them . we 'll have to evaluate that when they come in , and get people 's opinions on what they think of them . i 'm pretty that you can daisy chain them together what we would do is replace the wired mikes with wireless . we currently have one base station with six wireless mike , and we could replace our wired mikes with wireless if we bought another base station and more wireless mikes . no , we 're just replacing the wired the two wired that are still working , along with a couple of the wired that aren't working , one of the wired that 's not working , with a wireless . we should talk to them about it because i know that sri is also in the process of looking at and what we should try to keep everyone on the same page with that . i don't think it 'll take too long . just a matter of a few days i suspect . we have about thirty two hours that 's including digits . of non digits ? the digits don't take up that much time . how are you doing the acronyms if i say pzm what would it appear on the transcript ? we need to provide the transcripts to every participant of every meeting to give them an opportunity to bleep out sections they don't want . if someone feels strongly enough about it , then they should be allowed to do that . the consent form is right in there if anyone wants to look at it , "" if you agree to participate you 'll have the opportunity to have anything ex anything excised , which you would prefer not to have included in the data set . "" "" once a transcript is available we will ask your permission to include the data in the corpus for the r larger research community . there again you will be allowed to indicate any sections that you 'd prefer to have excised from the database , and they will m be removed both from the transcript and the recording . "" it just sends me the time intervals . and then at some point i 'll incorporate them all and put bleeps . since you seem to feel heart strongest about it , would you like to do the first pass ? ","Finally, the group discussed legal and procedural issues concerning the provision of transcripts to meeting participants for 'bleeping out' any sections of speech they want excluded from the Meeting Recorder database. The Berkeley Meeting Recorder group discussed recording equipment issues, including the purchase of two additional headsets and the prospect of getting a new base station and a set of wireless microphones to replace those wired microphones currently in use. "
Bmr014.D,,
Bmr014.E,"audio monitoring , jane . would like to have a discussion about where we are on recording , transcription where we are on the corpus . the other topic i was thinking of was the sta status on microphones and channels , and all that . how many channels do you get to have in a wireless setup ? heard from uw that they 're very close to getting their , setup purchased . was the decision last time was that the transcribers were going to be doing with the digits as has that started , or is that ? transcriptions , beyond the digits , where we are , and on . there 's got to be more than thirty hour the transcription part ? guess the next thing is this bleep editing . but the way i imagined it was that the largest set of people is gonna go "" didn't say anything funny in that meeting just go ahead , where 's the release ? "" there 'll be a subset of people , who will say i really would like to see that . "" and for them , the easiest way to flip through , if it 's a really large document , unless you 're searching . searching , should be electronic , and , where it 's really gonna hurt somebody , in some way the one who said it or someone who is being spoken about , we definitely want to allow the option of it being bleeped out . but i really think we wanna make it the rare incidence . and i am just a little worried about making it easy for people to do , just want it to be worded in such a way where it gives the strong impre it gives very , nothing hidden , v very strongly the bias that we would really like to use all of these data . that we really would rather it wasn't a patchwork of things tossed out , again let 's circulate the wording on each of these things and get it right , ","The Berkeley Meeting Recorder group discussed recording equipment issues, including the purchase of two additional headsets and the prospect of getting a new base station and a set of wireless microphones to replace those wired microphones currently in use. Finally, the group discussed legal and procedural issues concerning the provision of transcripts to meeting participants for 'bleeping out' any sections of speech they want excluded from the Meeting Recorder database. "
Bmr014.F,"in listening to some of these meetings that have already been recorded there are sometimes big spikes on particular things ,   should we don't wan wanna do the recording status first , or ? but it would come to about eleven hours that are finished transcribing from them right now . the next step is to that i 'm working on is to insure that the data are clean first , and then channelized . what by clean is that they 're spell checked , that the mark up is consistent all the way throughout , and also that we now incorporate these additional conventions that liz requested in terms of in terms of having a s a systematic handling of numbers , and acronyms which i hadn't been specific about . there are numbers , then there are acronyms , and then there 's a he she wants the actually a an explicit marker of what type of comment this is , curly b inside the curly brackets i 'm gonna put either "" voc "" for vocalized , like cough or like laugh or whatever , "" nonvoc "" for door slam , and "" gloss "" for things that have to do with if they said a s a spoken form with this m this pronunciation error . 'm going to convert that via a filter , into these tagged subcategorized comments , but the numbers and acronyms have to be handled by hand , the letters would be separated in space the next step is to work on tightening up the boundaries of the time bins . and thilo had a e a breakthrough with this last week in terms of getting the channel based speech nonspeech segmentation up and running actually have a segment of ten minutes that was transcribed by two of our transcribers , and i went through it last night , it 's almost spooky how similar these are , word for word . ","Speaker fe008 presented the current status on transcriptions, and explained procedures for cleaning up transcripts and ensuring they conform with set conventions. "
Bmr015.A,"right , was just gonna talk briefly about the nsf itr . that 's a couple hours of , speech , probably . and the decision here , was to continue with the words rather than the numerics . or neither . but it 's just two thing ways that you can say it . she 's trying to get at natural groupings , but it there 's nothing natural about reading numbers this way . we 're probably gonna be collecting meetings for a while let me , get my short thing out about the nsf . this was , a , proposal that we put in before on more higher level , issues in meetings , is i for it was a proposal for the itr program , it 's the second year of their doing , these grants . they 're very competitive , and they have a first phase where you put in pre proposals , and we , got through that . and th the next phase will be we 'll actually be doing a larger proposal . they have three size grants . and we 're in the middle middle category . it 's extending the research , this is dealing with , mapping on the level of , the conversation of mapping the conversations to different planes . there would be new hires , and there would be expansion , and , it 'll mean some more work , in march in getting the proposal out , the last one was that you had there , was about naming ? since we have such a short agenda list wi i will ask how are the transcriptions going ? you could make some guesses from , from the auto correlation but then , given those guesses , try , only looking at the energy at multiples of the of that frequency , take the one that 's maximum . you 're trying distinguish between the case where there is , where there are more than where there 's more than one speaker and the case where there 's only one speaker . other than that as far as the one person versus two persons , it would be primarily a low frequency phenomenon . and if you looked at the low frequencies , yes the higher frequencies are gonna there 's gonna be a spectral slope . the higher frequencies will be lower energy . you do need a voiced unvoiced determination too . but if it 's voiced , e the fraction of the energy that 's in the harmonic sequence that you 're looking at is relatively low , then it should be then it 's more likely to be an overlap . i don't know think it 'd be ideal . we see , we 're dealing with real speech and we 're trying to have it be as real as possible and breaths are part of real speech . if it gets in the way of what somebody is doing with it then you might wanna have some method which will allow you to block it , but you it 's real data . what if you put it in but didn't put the boundaries ? mean i 'm think if it 's too hard for us to annotate the breaths per se , we are gonna be building up models for these things and these things are somewhat self aligning , if we i if we say there is some thing which we call a "" breath "" or a "" breath in "" or "" breath out "" , the models will learn that thing . but you do want them to point them at some region where the breaths really are . the would say the core thing that we 're trying to do is to recognize the actual , meaningful components in the midst of other things that are not meaningful . it 's critical for us to get these other components that are not meaningful . because that 's what we 're trying to pull the other out of . ","The group also discussed a proposal for a grant from the NSF's ITR (Information Technology Research) program, transcriptions, and efforts by speaker mn005 to detect speaker overlap using harmonicity-related features. Topics discussed by the Berkeley Meeting Recorder group included the status of the first test set of digits data, naming conventions for files, speaker identification tags, and encoding files with details about the recording. "
Bmr015.B,"i have a short thing about digits and then wanna talk a little bit about naming conventions , the only thing i wanna say about digits is , we are done with the first test set . the first is what should we do about digits that were misread ? what the transcribers did with that is if they did a correction , and they eventually did read the right string , you extract the right string . four thousand lines . and each line is between one and about ten digits . and then the other thing is that , the forms in front of us here that we 're gonna read later , were suggested by liz because she wanted to elicit some different prosodics from digits . the problem was o and zero . but , the other problem we were thinking about is if you just put the numerals , they might say forty three instead of four three . it 's go higher level than we 've been talking about for meeting recorder . one thing she would like to have is for all the names to be the same length same number of characters and i don't think we have many meetings that 's a big deal just to change the names . for m the meetings we were thinking about three letters and three numbers for speakers , m or f and then three numbers , that also brings up the point that we have to start assembling a speaker database that we get those links back and forth and keep it consistent . and then , the microphone issues . we want some way of specifying , more than looking in the "" key "" file , what channel and what mike . what channel , what mike , and what broadcaster . and we just need some naming conventions on that . was just gonna do a fixed list of , microphones and types . ","Topics discussed by the Berkeley Meeting Recorder group included the status of the first test set of digits data, naming conventions for files, speaker identification tags, and encoding files with details about the recording. "
Bmr015.C,"i 'm continue working with the mixed signal now , after the last experience . and i 'm tried to , adjust the to improve , an harmonicity , detector that , i implement . and now i 'm i 'm trying to find , some , of h of help , using the energy to distinguish between possible harmonics , and other fre frequency peaks , that , corres not harmonics . i have to talk with y with you , with the group , about the instantaneous frequency , no . i don't proth process the fundamental . i , ehm i calculate the phase derivate using the fft . i will prepare for the next week all my results about the harmonicity ","The group also discussed a proposal for a grant from the NSF's ITR (Information Technology Research) program, transcriptions, and efforts by speaker mn005 to detect speaker overlap using harmonicity-related features. "
Bmr015.D,,
Bmr015.E,"but those backchannels will always be a problem especially if they 're really short and they 're not very loud and it can it will always happen that also the automatic s detection system will miss some of them , and that 's that quite co corresponds to the way i try to train the speech nonspeech detector , ",
Bmr015.F,"i switched to doing the channel by channel transcriptions to provide , the tighter time bins for partly for use in thilo 's work and also it 's of relevance to other people in the project . one of them is that , it seems that there are time lags involved in doing this , using an interface that has much more complexity to it . was thinking the best way to do this in the long run may be to give them single channel parts and then piece them together later . and it may be that it 's faster to transcribe a channel at a time with only one , sound file and one , set of , utterances to check through . but , with the mixed , when you have an overlap , you only have a choice of one start and end time for that entire overlap , and for purposes of , things like things like training the speech nonspeech segmentation thing . th it 's necessary to have it more tightly tuned than that . i really do think that it 's wise that we 've had them start the way we have with , m y working off the mixed signal , having the interface that doesn't require them to do the ti the time bins for every single channel at a t through the entire interaction . once in a while a backchannel will be overlooked by the transcriber . and if we 're gonna study types of overlaps , then that really does require listening to every single channel all the way through the entire length for all the different speakers . it 's like this it 's really valuable that thilo 's working on the speech nonspeech segmentation because we can close in on that wi without having to actually go to the time that it would take to listen to every single channel from start to finish through every single meeting . then , the answer is to , listen especially densely in places of overlap , i did i it did occur to me that this is the return to the transcription , that there 's one third thing i wanted to ex raise as a to as an issue which is , how to handle breaths . aside from the fact that they 're very time consuming to encode , but the question of whether it 'd be possible to eliminate them from the audio signal , because i it has a i it shows very clearly the contrast between , speech recognition research and discourse research now , i had a discussion with chuck about the data structure there 'll be a master transcript which has in it everything that 's needed for both of these uses . and the one that 's used for speech recognition will be processed via scripts . what i would r what i would wonder is would it be possible to encode those automatically ? could we get a breath detector ? which is that as we 've improved our microphone technique , we have a lot less breath in the more recent , recordings , it 's in a way it 's an artifact that there 's much on the earlier ones . ","Particular focus was paid to questions about transcription procedures, i.e. how to deal with overlooked backchannels, and audible breaths. "
Bmr015.G,you 're talking about where they completely read the wrong string and didn't correct it ? seems like we should just change the transcripts you how many digits have been transcribed now ? would say don't tell them to transcribe anything that 's outside of a grouping of words . ,
Bmr018.A,"he generated , a channel wise presegmented version of a meeting ,  there is this issue of , if the segmenter thought there was no speech on a particular stretch , on a particular channel , and there really was , then , if it didn't show up in a mixed signal to verify , then it might be overlooked , the question is "" should a transcriber listen to the entire thing or can it g can it be based on the mixed signal ? "" but presumably , most of those they should be able to hear from the mixed signal unless they 're embedded in the heavil heavy overlap section they could choose any signal to look at . i 've tried lookin but usually they look at the mixed . but i 've tried looking at the single signal and in order to judge when it was speech and when it wasn't , they show up on the separate ribbons . and i it 'll be because it 's being segmented as channel at a time with his with thilo 's new procedure , then you don't have the correspondence of the times across the bins across the ribbons you can switch quickly between the audio , but you just can't get the visual display to show quickly . i do think that this will be a doable procedure , and have them starting with mixed and , then when they get into overlaps , just have them systematically check all the channels to be that there isn't something hidden from audio view . cuz sometimes people will say , "" and then i "" and there 's a long pause and finish the sentence and sometimes it looks coherent and the and then it 's coupled with the problem that sometimes , with a fricative you might get the beginning of the word cut off but it 's possible that a script could be written to merge those two types of things . it 's just a matter of , from now on we 'll be able to have things channelized to begin with . the problem is i it 's a really good question , and i really find it a pain in the neck to delete things i had , one of the transcribers go through and tighten up the bins on one of the , nsa meetings , and then i went through afterwards and double checked it that one is really very accurate . ",
Bmr018.B,"right , we need to run thilo 's thing on it , and then we go in and adjust the boundaries . how quickly can the transcribers scan over and fix the boundaries , we 're just doing the individual channels , right ? but if i didn't know anything about transcriber and i was gonna make something to let them adjust boundaries , i would just show them one channel at a time , with the marks , and let them adju did you run the andreas the r sri recognizer on the digits ? zero percent error ? he 's just saying you have to look over a longer time window when you do it . you just have to look over longer time when you 're trying to align the things , andreas , how did it work on the non lapel because really the at least in terms of how we were gonna use this in our system was to get an ideal an idea , for each channel about the start and end boundaries . we don't really care about like intermediate word boundaries , ","The Berkeley Meeting Recorder group discussed the preparation of a data sample for IBM, the manual adjustment of time bins by transcribers, recognition results for a test set of digits data, and forced alignments. "
Bmr018.C,"the first thing is the automatic thing , and then it 's then it 's the transcribers tightening up , and then it 's ibm . you 're talking about tightening up time boundaries ? but the procedure that you 're imagining , people vary from this , is that they have the mixed signal wave form in front of them , let 's see , there isn't we don't have transcription yet . but there 's markers of some sort that have been happening automatically , and those show up on the mixed signal ? the way you 're imaging is they play it , and if it there 's a question on something , they stop and look at the individual wave form . does anybody , working on any eurospeech submission related to this ? we had that one conversation about , what did it mean for , one of those speakers to be pathological , y unless we do this , cancellation business . ","The Berkeley Meeting Recorder group discussed the preparation of a data sample for IBM, the manual adjustment of time bins by transcribers, recognition results for a test set of digits data, and forced alignments. "
Bmr018.D,"except for it doesn't do on short things , remember . like that . i don't know that you can locate them very from the mixed signal , but there 's no overlap during the digit readings , it shouldn't really matter . but it 's clear from dan that this is not something you can do in a short amount of time . he thought if we can do something quick and dirty because dan said the cross cancellation , it 's not straight forward . it 's good to hear that it was not straight forward , thinking if we can get decent forced alignments , then at least we can do overall report of what happens with actual overlap in time , and it should work pretty if you took care of this recording time difference . but then if you add the dynamic aspect of adapting distances , then it wasn't don't we can do if anything , that 's worth , a eurospeech paper at this point . actually y we can tell from the data that we have , and , if you align the two hypothesis files across the channels , just word alignment , you 'd be able to find that . if thilo can tell us that there 're boundaries here , we should be able to figure that out because the only thing transcribed in this channel is this word . but , if there are things if you have two and they 're at the edges , it 's like here and here , and there 's speech here , then it doesn't really help you , does it make sense to try to take what we have now , which are the ones that , we have recognition on which are synchronous and not time tightened , and try to get something out of those for purposes of illustrating the structure and the nature of the meetings , or is it better to just , forget that and tr because for feature extraction like for prosody the meetings we have now , it 's a good chunk of data we need some way to push these first chunk of meetings into a state where we get good alignments . but what you do wanna do is take the , even if it 's klugey , take the segments the synchronous segments , the ones from the hlt paper , where only that speaker was talking . use those for adaptation , cuz if you use everything , then you get all the cross talk in the adaptation , and it 's just blurred . like a third of it is bad for adaptation or the hlt paper is really more of a introduction to the project paper , and , that might actually be useful but they 're all non native speakers . this is tough for a language model probably ",Efforts to deal with cross-talk and improve forced alignments for non-digits data were also discussed. 
Bmr018.E,"i have the program to insert the beeps . what i don't have is something to parse the output of the channelized transcripts to find out where to put the beeps , but that should be really easy to do . excuse me , two or more times real time , they have the normal channeltrans interface where they have each individual speaker has their own line , but you 're listening to the mixed signal and you 're tightening the boundaries , correcting the boundaries . you shouldn't have to tighten them too much because thilo 's program does that .  that 's something that the transcribers will have to do . but then they for this meeting they would have to do seven times real time , and it would probably be more than that . the problem is that the tcl tk interface with the visuals , it 's very slow to load waveforms . that if we decide that we need that they need to see the visuals , we need to change the interface that they can do that . the mixed signal , the overlaps are pretty audible because it is volume equalized . think they should be able to hear . the only problem is , counting how many and if they 're really correct or not . i would like to try to do something on digits but don't know if we have time . whereas it 's probably something pathologic and actually stephane 's results , confirm that . he s he did the aurora system also got very lousy average error , like fifteen or , fifteen to twenty percent average ? but then he ran it just on the lapel , and got about five or six percent word error ? that means to me that somewhere in the other recordings there are some pathological cases . it may be just some of the segments they 're just doing a lousy job on . 'll listen to it and find out since you 'd actually split it up by segment . just something really wrong with a bug is what which probably means like there was a th the recording interface crashed , or there was a short someone was jiggling with a cord or , i extracted it incorrectly , it was transcribed incorrectly , two words . and we 'll just have to see how hard that is . i was just thinking about the fact that if thilo 's missed these short segments , that might be quite time consuming for them to insert them . we 'll have to , eventually . and my hope was that we would be able to use the forced alignment to get it . for eurospeech we want some results cuz , i 'm just thinking , we were we 're we 've been talking about changing the mikes , for a while , acoustically they seem really good , but if they 're not comfortable , we have the same problems we have with these stupid things . wh what it 's supposed to do is the backstrap is supposed to be under your crown , it doesn't slide up . if you feel the back of your head , you feel a little lump , and it 's supposed to be right under that . ","The Berkeley Meeting Recorder group discussed the preparation of a data sample for IBM, the manual adjustment of time bins by transcribers, recognition results for a test set of digits data, and forced alignments. "
Bmr018.F,"since i considered those preliminary , i didn't . it 's actually , it it was trimodal , actually there were t there was one h one bump at ze around zero , which were the native speakers , then there was another bump at , like fifteen those were the non natives . and then there was another distinct bump at hundred , which must have been some problem . in the recording and there was this one meeting , i forget which one it was , where like , six out of the eight channels were all , like had a hundred percent error . if i excluded the pathological ones , by definition , those that had like over ninety five percent error rate , and the non natives , then the average error rate was like one point four which seemed reasonable given that , the models weren't tuned for it . and the grammar wasn't tuned either . but if you p if you actually histogrammed it , and it was a it was zero was the most of them , 've been struggling with the forced alignments . most of the time it doesn't work very i 'm still tinkering with it , but it might be that we can't get clean alignments out of this out of those , channels , the issue was that you have to you have you first have to have a pretty good speech detection on the individual channels . i haven't checked those yet . it 's very tedious to check these . we would really need , ideally , a transcriber to time mark the the be at least the beginning and s ends of contiguous speech . and , then with the time marks , you can do an automatic comparison of your forced alignments . but you don't wanna , infer from the alignment that someone spoke who didn't . would need a k i would need a channel that has a speaker whose who has a lot of overlap but s is a non lapel mike . and , where preferably , also there 's someone sitting next to them who talks a lot . and it 's possible that you get considerably better results if you , manage to adapt the , phone models to the speaker and the reject model to the to all the other speech . ","The Berkeley Meeting Recorder group discussed the preparation of a data sample for IBM, the manual adjustment of time bins by transcribers, recognition results for a test set of digits data, and forced alignments. Preliminary recognition results were presented for a subset of digits data. "
Bmr018.G,what is patho what do by pathological ? ,
Bmr020.A,"the only agenda items were jane was jane wanted to talk about some of the ibm transcription process . dave gelbart sent me email , he sent it to you too , that there 's a special topic , section in si in eurospeech on new , corp corpors corpora . and it 's not due until like may fifteenth . i bet there 's a weak dependence . you have a lot of two party , subsets within the meeting . what normalization do you do ? and , the other thing chuck pointed out is that , since this one is hand marked , there are discourse boundaries . what we 're probably gonna do is just write a script , that if two , chunks are very close to each other on the same channel we 'll just merge them . that 's because of channel overlap . and we should just double check with brian on a few simple conventions on how they should mark things . what can you do ? a hand transcriber would have trouble with that . we don't do anything for it with it . and they 'll just mark it however they mark it , and we 'll correct it when it comes back . ","The main topics of the agenda were a paper submitted to Eurospeech and the organising of the recording transcriptions to be done by IBM. Regarding the transcriptions to be carried out by IBM, the discussion mainly concerned the format of the recordings that should be sent to them. "
Bmr020.B,"i also used something around zero point five seconds for the speech nonspeech detector whi which could have one drawback . if there is backchannel in between those three things , the n the backchannel will occur at the end of those three . and you just use the s the segments of the dominant speaker then ? for sending to ibm but then we could just use the output of the detector , and do the beeping on it , and send it to i b the speech the amount of speech that is missed by the detector , for a good meeting , i th is around or under one percent , i would say . i can't really hhh , tsk . i don't have really representative numbers , i 've got a p a method with loops into the cross correlation with the pzm mike , and then to reject everything which seems to be breath . i could run this on those breathy channels , and there 's one point which i which i r we covered when i r listened to one of the edu meetings , and that 's that somebody is playing sound from his laptop . and i the speech nonspeech detector just assigns randomly the speech to one of the channels , ","Suggestions included sending only the channels with the dominant speakers for transcription, but it was finally agreed on sending the original files with minimal modifications, as there will be extensive in-house post-processing. "
Bmr020.C,"no , it doesn't necessarily go against what he said , that adam created a b a script to generate the beep file ? but you were gonna to use the originally transcribed file because i tightened the time bins they were , reasonably tight , but not excruciatingly tight . i wanted it to be able to l he be heard normally , that if you play back that bin and have it in the mode where it stops at the boundary , it sounds like a normal word . it 's as if the person could 've stopped there . that means that the amount of time after something is variable depending partly on context , but my general goal when there was sufficient space , room , pause after it to have it be natural feeling gap . wally chafe says that in producing narratives , the spurts that people use tend to be , that the what would be a pause might be something like two seconds . i hand adjusted two of them but i like this idea of for our purposes for the ibm preparation , n having these joined together , but thi this brings me to the other f stage of this which i discussed with you earlier today , the e edu meetings , that thilo ha has now presegmented all of them for us , on a channel by channel basis . i 've assigned i 've assigned them to our transcribers and in a way , by having this chunk and then the backchannel after it , it 's like a stagal staggered mixed channel . the maximal gain , it 's from the ibm people , may be in long stretches of connected speech . what i 'm thinking , and it may be that not all meetings will be good for this , but what i 'm thinking is that in the edu meetings , they tend to be driven by a couple of dominant speakers . and , if the chunked files focused on the dominant speakers , then , when it got s patched together when it comes back from ibm , we can add the backchannels . the original plan was that the transcriber would adjust the t the boundaries , and all that for all the channels but , that is time consuming ,  but i wanted to say , his segmentation is good , that the part that i listened to with her yesterday didn't need any adjustments of the bins . i 'm open to that , the other problem is , that when it i on the breathy ones , where you get breathing , inti indicated as speech . ","Within this discussion, the rationale behind the coding of the time bins according to the flow of discourse was also explained. Suggestions included sending only the channels with the dominant speakers for transcription, but it was finally agreed on sending the original files with minimal modifications, as there will be extensive in-house post-processing. "
Bmr020.D,"wonder if you have to normalize by the numbers of speakers i wonder about the and effect there . in other words if you weren't going to pause you will because you 're g being interrupted . liz presented this at some conference a while ago about backchannels and that they tend to happen when the pitch drops . we need to give brian the beeps file , was gonna probably put it after our meeting this morning thilo came in and said that there could be other differences between the already transcribed meeting with the beeps in it and one that has just r been run through his process . tomorrow , when we go to make the chunked file for ibm , we 're going to actually compare the two . and then we 're gonna do the beep ify on both , and listen to them and see if we notice any real differences . when i was listening to the original file that adam had , it 's like you hear a word then you hear a beep and then you hear the continuation of what is the same sentence . without having her check anything . the one suggestion is we run thilo 's thing and then we have somebody go and adjust all the time boundaries and we send it to ibm . the other one is we just run his thing and send it to ibm . there 's a another possibility if we find that there are some problems , and that is if we go ahead and we just run his , and we generate the beeps file , then we have somebody listen beeps file . and they listen to each section and say "" yes , no "" whether that section is that 's the best way to go , and we can just , get the meeting , process it , put the beeps file , send it off to ibm . ","Suggestions included sending only the channels with the dominant speakers for transcription, but it was finally agreed on sending the original files with minimal modifications, as there will be extensive in-house post-processing. "
Bmr020.E,"but , they probably w want the originals . ",
Bmr020.F,"and you just sent off a eurospeech paper , we should probably talk about the ibm transcription process that but i have another suggestion on that , which is , since , really what this is , is trying to in the large , send the right thing to them and there is gonna be this post processing step , why don't we check through a bunch of things by sampling it ? if it sounds like it 's almost always right and there 's not any big problem you send it to them . and , then they 'll send us back what we w what they send back to us , and we 'll fix things up notion of how on a good meeting , how often do you get segments that come in the middle of words and forth , and in a bad meeting how often ? and what that 'll do is just cut the time a little further . ","The main topics of the agenda were a paper submitted to Eurospeech and the organising of the recording transcriptions to be done by IBM. Regarding the transcriptions to be carried out by IBM, the discussion mainly concerned the format of the recordings that should be sent to them. Suggestions included sending only the channels with the dominant speakers for transcription, but it was finally agreed on sending the original files with minimal modifications, as there will be extensive in-house post-processing. "
Bmr020.G,"i hope they accept it . we actually exceeded the delayed deadline by o another day , there were some interesting results in this paper , though . that morgan accounted for fifty six percent of the robustness meetings in terms of number of words . no . according to the transcripts . we as identify him as the person dominating the conversation . it was about it had three sections the one was that the just the amount of overlap s in terms of number of words and also we computed something called a "" spurt "" , which is essentially a stretch of speech with no pauses exceeding five hundred milliseconds . and we computed how many overlapped i spurts there were and how many overlapped words there were . for four different corpora , the meeting recorder meetings , the robustness meetings switchboard and callhome , as you might expect the meeting recorder meetings had the most overlap but next were switchboard and callhome , which both had roughly the same , and the robustness meetings were had the least , also , i in the levinson , the pragmatics book , in textbook , there 's i found this great quote where he says how people it talks about how how people are good at turn taking , and they 're good that generally , u the overlapped speech does not is less than five percent . in terms of number of words , it 's like seventeen or eigh eighteen percent for the meeting recorder meetings and about half that for , the robustness . we didn't get to look at that , but , even if you take out all the backchannels you still have significant overlap . and then the second one was just the we had in the hlt paper on how overlaps effect the recognition performance . and we rescored things a little bit more carefully . the conjecture from the hlt results was that most of the added recognition error is from insertions due to background speech . we scored everything , and i must say the nist scoring tools are pretty for this , where you just ignore everything outside of the , region that was deemed to be foreground speech . but what we found is after we take out these regions we only score the regions that were certified as foreground speech , the recognition error went down to almost the level of the non overlapped speech . we do vtl vocal tract length normalization , w and we we make all the features have zero mean and unit variance . over the entire c over the entire channel . now we didn't re align the recognizer for this . the recognizer didn't have the benefit of knowing where the foreground speech a start and then , the third thing was , we looked at , what we call "" interrupts "" , we used the punctuation from the original transcripts and we inferred the beginnings and ends of sentences . if you have overlapping speech and someone else starts a sentence , where do these where do other people start their turns not turns really , but sentences , the question was how can we what can we say about the places where the second or actually , several second speakers , start their "" interrupts "" , as we call them . to for the purposes of this analysis , we tagged the word sequences , and we time aligned them . that , if any part of the word was overlapped , it was considered an interrupted word . because we had tagged these word strings , that occurred right before these interrupt locations . and the tags we looked at are the spurt tag , or actually end of spurt . whether there was a pause essentially here , and then we had things like discourse markers , backchannels , disfluencies . filled pauses we didn't really hand tag all of these things . but we just based on the lexical identity of the words , we tagged them as one of these things . at the end after a discourse marker or after backchannel or after filled pause , you 're much more likely to be interrupted than before . and also after spurt ends , which means in p inside pauses . there 's no statement about and effect . y we didn't talk about , prosodic , properties although that 's i take it that 's something that don will look at now that we have the data and we have the alignment , there 's actually there 's this a former student of here from berkeley , nigel ward . and he did this backchanneling , automatic backchanneling system . but for japanese . and it 's for japa in japanese it 's really important that you backchannel . anyway . the paper 's on line 'm actually about to send brian kingbury an email saying where he can find the s the m the material he wanted for the s for the speech recognition experiment , he prefe he said he would prefer ftp and also , the other person that wants it there is one person at sri who wants to look at the the the data we have far , and figured that ftp is the best approach . what i did is i @ i made a n new directory actually and this directory , is not readable . it 's only accessible . that someone can get that file and then know the file names and therefore download them . all i was gonna do there was stick the transcripts after we the way that we munged them for scoring , and also and then the waveforms that don segmented . but for the other meetings it 's the downsampled version that you have . we should probably give them the non downsampled versions . ","All these measurements were based on the sample of available transcripts. The results presented in the former show a significant percentage of overlapping speech even without counting in backchanneling. Additionally, the high error rate in the recognition of such overlapping speech by the SRI recogniser was minimised simply by changing the scoring method used. Finally, a strong correlation between pauses and interruptions was confirmed. Other features, like prosody, will be studied in the near future. An FTP directory containing such experimental data is being set up for the benefit of other researchers. "
Bmr020.G,"what i would i was interested in is having a se having time marks for the beginnings and ends of speech by each speaker . because we could use that to fine tune our alignment process it 's good . you always want to have a little bit of pause or nonspeech around the speech , say for recognition purposes . we chose half a second because if you go much larger , you have a y your statement about how much overlap there is becomes less , precise , liz suggested that value based on the distribution of pause times that you see in switchboard and other corpora . at some point we will try to fine tune our forced alignment using those as references ",
Bro023.A,"just for a visit ? we got lots to catch up on . and we haven't met for a couple of weeks . why don't we start with you , dave , it seems like for your in normal situations you would never get twelve seconds of speech , do you wanna go , sunil ? improves over the base line mfcc system ? can i ask just a high level question ? can you just say like one or two sentences about wiener filtering and why are people doing that ? do you wanna go , stephane ? ","There are hopes that a visitor coming for three weeks, may lead to a longer term collaboration. The ICSI Meeting Recorder Group of Berkeley met for the first time in two weeks. A number of the group also took time to explain the basics of their approaches to the group. "
Bro023.B,"'ve been working on that wiener filtering . found that , s single like , do a s normal wiener filtering , like the standard method of wiener filtering . and that doesn't actually give me any improvement over like b it actually improves over the baseline but it 's not like it doesn't meet something like fifty percent that 's the improvement is somewhere around thirty percent over the baseline . no , just one stage wiener filter which is a standard wiener filter . i ran this with one more stage of wiener filtering on it but the second time , what i did was i estimated the new wiener filter based on the cleaned up speech , and did , smoothing in the frequency to reduce the variance and by adding another stage of wiener filtering , the results on the speechdat car was like , but the overall improvement was like fifty six point four six . the basic principle of wiener filter is like you try to minimize the , d difference between the noisy signal and the clean signal ","Group members reported their progress in the areas of spectral subtraction, Wiener filtering and noise estimation. A number of the group also took time to explain the basics of their approaches to the group. "
Bro023.C,"and hans hans guenter will be here , by next tuesday or he 's going to be here for about three weeks , we 'll see . we might end up with some longer collaboration he 's gonna look in on everything we 're doing and give us his thoughts .   why would you do it , if you knew that you were going to have short windows in testing . the other thing , which relates a little bit to something else we 've talked about in terms of windowing and on is , that , i wonder if you trained with twelve seconds , and then when you were two seconds in you used two seconds , and when you were four seconds in , you used four seconds , and when you were six and you build up to the twelve seconds . that if you have very long utterances you have the best , but if you have shorter utterances you use what you can . i don't know much about as much as i should about the rest of the system but if you did first pass with , the with either without the mean sub subtraction or with a very short time one , and then , once you , actually had the whole utterance in , if you did , the , longer time version then , based on everything that you had , and then at that point only used it to distinguish between , top n , possible utterances you might it might not take very much time . i know in the large vocabulary stu systems , people were evaluating on in the past , some people really pushed everything in to make it in one pass but other people didn't and had multiple passes . the argument , against multiple passes was u has often been "" but we want to this to be r have a interactive response "" . and the counterargument to that which , say , bbn had , was "" but our second responses are second , passes and third passes are really , really fast "" . is that using in combination with something else ? do we know yet ? about as far as what they 're what the rules are going to be and what we can use ? they will send files everybody will have the same boundaries to work with ? none of these systems , have y you both are working with , our system that does not have the neural net , one would hope , presumably , that the neural net part of it would improve things further as they did before . it 's depending on how all this comes out we may or may not be able to add any latency . i would worry about it a little . because if we completely ignore latency , and then we discover that we really have to do something about it , we 're going to be find ourselves in a bind . and that 's one of the all of that is things that they 're debating in their standards committee . but the spectral subtraction scheme that you reported on also re requires a noise estimate . couldn't you try this for that ? do you think it might help ? ","There are hopes that a visitor coming for three weeks, may lead to a longer term collaboration. They also discusses topics relating to the rules and preferences of the project they are working on, including single vs multiple passes. "
Bro023.D,"actually i received a new document , describing this . and what they did finally is to , not to align the utterances but to perform recognition , only on the close talking microphone , and to take the result of the recognition to get the boundaries of speech .  i 've been , working still on the spectral subtraction . to r to remind you a little bit of what i did before , is just to apply some spectral subtraction with an overestimation factor doing just this , either on the fft bins or on the mel bands , t doesn't yield any improvement actually i tried , something else based on this , is to put some smoothing , because it seems to help or it seems to help the wiener filtering what i did is , some nonlinear smoothing . although i 've just tested on italian and finnish . and on italian it seems my result seems to be a little bit better than the wiener filtering ,  another thing that i it 's important to mention is , that this has a this has some additional latency . and i noticed that it 's better if we take into account this latency . b but i don't think we have to worry too much on that right now while you kno . the second thing i was working on is to , try to look at noise estimation , and using some technique that doesn't need voice activity detection . and for this i u simply used some code that , i had from belgium , which is technique that , takes a bunch of frame , and for each frequency bands of this frame , takes a look at the minima of the energy . and then average these minima and take this as an energy estimate of the noise for this particular frequency band .  for will . try also , the spectral subtraction . ","They also discusses topics relating to the rules and preferences of the project they are working on, including single vs multiple passes. Group members reported their progress in the areas of spectral subtraction, Wiener filtering and noise estimation. A number of the group also took time to explain the basics of their approaches to the group. "
Bro023.E,"th that 's his spectral subtraction group ? is that right ? guess i should probably talk to him a bit too ? since we 're looking at putting this , mean log m magnitude spectral subtraction , into the smartkom system , i did a test seeing if , it would work using past only and plus the present to calculate the mean . i did a test , where i used twelve seconds from the past and the present frame to , calculate the mean . twelve seconds , counting back from the end of the current frame , it was , twen it was twenty one frames and compared to , do using a twelve second centered window , there was a drop in performance but it was just a slight drop . say twe twelve seconds in the earlier test seemed like a good length of time , but what happens if you have less than twelve seconds ? w bef before , back in may , i did some experiments using , say , two seconds , or four seconds , or six seconds . in those i trained the models using mean subtraction with the means calculated over two seconds , or four seconds , or six seconds . here , i was curious , what if i trained the models using twelve seconds but i f i gave it a situation where the test set i was subtracted using two seconds , or four seconds , or six seconds . and it seems like it hurts compared to if you actually train the models using th that same length of time and that 's actually what we 're planning to do in ","Group members reported their progress in the areas of spectral subtraction, Wiener filtering and noise estimation. "
Bro024.A,"for the past , week an or two , i 've been just writing my , formal thesis proposal . 'm taking this qualifier exam that 's coming up in two weeks . and i finish writing a proposal and submit it to the committee . briefly , i 'm proposing to do a n a new p approach to speech recognition using a combination of , multi band ideas and ideas , about the acoustic phonec phonetic approach to speech recognition . will be using these graphical models that that implement the multi band approach to recognize a set of intermediate categories that might involve , things like phonetic features or other f feature things that are more closely related to the acoustic signal itself . and the hope in all of this is that by going multi band and by going into these , intermediate classifications , that we can get a system that 's more robust to unseen noises , and situations like that . ",
Bro024.B,"the last week , i showed some results with only speechdat car which was like some fifty six percent . i wasn't getting that r results on the ti digit . was like looking into "" why , what is wrong with the ti digits ? "" . and i found that , the noise estimation is a reason for the ti digits to perform worse than the baseline . the other thing is the i 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . just tried another sk system another filter which i 've like shown at the end . which is very similar to the existing filter . only only thing is that the phase is like a nonlinear phase it 's just like it 's like a three percent relative degradation , ","The groups regulars reported progress on their work on mean subtraction, noise estimation, voice activity detection and the Vector Taylor Series. "
Bro024.C,"the this past week i 've been main mainly occupied with , getting some results , u from the sri system trained on this short hub five training set for the mean subtraction method . i ran some tests last night . the results are suspicious . it 's , cuz they 're the baseline results are worse than , andreas than results andreas got previously . ap apart from that , the main thing i have t ta i have to talk is , where i 'm planning to go over the next week . 've been working on integrating this mean subtraction approach into the smartkom system . and there 's this question of , in my tests before with htk i found it worked the best with about twelve seconds of data used to estimate the mean , but , we 'll often have less in the smartkom system . think we 'll use as much data as we have at a particular time , and we 'll concatenate utterances together , to get as much data as we possibly can from the user . but , there 's a question of how to set up the models . we could train the models . if we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean , to mean subtract the training data . or we could , use some other amount . and then there 's another thing i wanna start looking at , wi is , the choice of the analysis window length . with the htk set up i should be able to do some experiments , on just varying that length , say between one and three seconds , in a few different reverberation conditions , a actually i was just thinking about what i was asking about earlier , wi which is about having less than say twelve seconds in the smartkom system to do the mean subtraction . you said in systems where you use cepstral mean subtraction , they concatenate utterances and , do how they address this issue of , testing versus training ? and , in tha in that case , wh what do they do when they 're t performing the cepstral mean subtraction on the training data ? because you 'd have hours and hours of training data . do they cut it off and start over ? you 'd you and in training you would start over at every new phone call or at every new speaker . ","The groups regulars reported progress on their work on mean subtraction, noise estimation, voice activity detection and the Vector Taylor Series. While on these topics, related areas discussed included recognition window length, training versus test set sizes, artificial distortion and latency concerns. "
Bro024.D,"one thing that might also be an issue , cuz part of what you 're doing is you 're getting a spectrum over a bunch of different kinds of speech sounds . and it might matter how fast someone was talking if you if there 's a lot of phones in one second you 'll get a really good sampling of all these different things , and , on the other hand if someone 's talking slowly you 'd need more . and if you 're splitting things up into utterances in a dialogue system , where you 're gonna be asking , th for some information , there 's some initial th something . and the heuristics of exactly how people handle that and how they handle their training i 'm vary from place to place . our position is that , we shouldn't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . there is a minority in that group who is a arguing who are arguing for having a further constraining of the latency . we 're s just continuing to keep aware of what the trade offs are and , what do we gain from having longer or shorter latencies ? france telecom was very short latency ","While on these topics, related areas discussed included recognition window length, training versus test set sizes, artificial distortion and latency concerns. "
Bro024.E,"go next . there are two figures showing actually the , performance of the current vad . it 's a n neural network based on plp parameters , which estimate silence probabilities , and then put a median filtering on this to smooth the probabilities , right ? for italian and spanish it 's th this value works good but not necessarily for finnish . but unfortunately there is this forty millisecond latency would try to somewhat reduce this @ @ . i already know that if i completely remove this latency , it there is a three percent hit on italian . ","The groups regulars reported progress on their work on mean subtraction, noise estimation, voice activity detection and the Vector Taylor Series. "
Bro024.F,"why don't you go ahead , dave ? do you wanna go , barry ? do you wanna go , sunil ? did you wanna go next , stephane ? carmen ? ",
Bro024.G,"what they do is they do it always on line , that you just take what you have from the past , that you calculate the mean of this and subtract the mean . it seems to be the best what wh what we can do in this moment is multi condition training . and every when we now start introducing some noise reduction technique we introduce also somehow artificial distortions . and these artificial distortions i have the feeling that they are the reason why we have the problems in this multi condition training . that means the h m ms we trained , they are based on gaussians , and if we introduce now this u spectral subtraction , or wiener filtering this is your noise estimate and you somehow subtract it or do whatever . and then what you do is you introduce some artificial distribution in this in the models . but is there a problem with the one hundred eighty milliseconds ? it was in the order of thirty milliseconds s to summarize the performance of these , speechdat car results is similar than yours to say . you are leaving in about two weeks carmen . what i would do is i would pick @ @ the best consolation , which you think , and c create all the results for the whole database that you get to the final number as sunil did it and also to write somehow a document where you describe your approach , and what you have done . ","While on these topics, related areas discussed included recognition window length, training versus test set sizes, artificial distortion and latency concerns. "
Bro024.H,"i only say that the this is , a summary of the of all the vts experiments and say that the result in the last for italian the last experiment for italian , are bad . if we put everything , we improve a lot u the spectral use of the vts but the final result are not still good like the wiener filter i was thinking to do that next week . i wi i will do that next week . ","The groups regulars reported progress on their work on mean subtraction, noise estimation, voice activity detection and the Vector Taylor Series. "
Bro025.A,you guys have combined or you 're going to be combining the software ? how is how good is that ? compared to the last evaluation numbers ? what was the issue with the vad ? what amount of latency are you thinking about when you say that ? ,"They have developed a piece of software which allows them to implement their two main approaches to dealing with noise. The base rate is currently set at the second best rate as of the last project evaluation, and it does not yet include everything the group have been working on. With this in mind, they have decided to set most things, and concentrate on studying only a few key aspects, the neural network, the voice activity detector, and the noise estimation. "
Bro025.B,"let 's summarize . anyway we after coming back from qualcomm we had , very strong feedback and , it was hynek and guenter 's and my opinion also that , we spread out to look at a number of different ways of doing noise suppression . but given the limited time , it was time to choose one . and th the vector taylor series hadn't really worked out that much . the subspace had not been worked with much . it came down to spectral subtraction versus wiener filtering . we had a long discussion about how they were the same and how they were d completely different . that , @ @ again we felt the gang should just figure out which it is they wanna do and then let 's pick it , instead they went to yosemite and bonded , and they came out with a single piece of software . it 's another victory for international collaboration . but the important thing is that there is a piece of software that you that we all will be using now . it it 's not using our full bal bag of tricks , if you will . and , and it is , very close in performance to the best thing that was there before . but , looking at it another way , more importantly , we didn't have any explicit noise , handling we didn't explicitly have anything to deal with stationary noise . i gather you have it sounds like you have a few more days of nailing things down with the software and on . but and then but , arguably what we should do is , even though the software can do many things , we should for now pick a set of things , and not change that . and then focus on everything that 's left . and that our goal should be by next week , when hynek comes back , to really just to have a firm path , for the for the time he 's gone , of , what things will be attacked . we do still , however , have to consider its latency . we can't have unlimited amounts of latency . y that 's still being debated by the by people in europe but , no matter how they end up there , it 's not going to be unlimited amounts , there 's the neural net issue . there 's the vad issue . and , there 's the second stream thing . they still allow two hundred milliseconds on either side or some ? that 's really not bad . we may we 'll see what they decide . we may have , the , latency time available for to have a neural net . they 're saying , one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds . did you happen to notice how much , the change was due to just this frame dropping problem ? that 's a real good point . that 's a good set of work that , i was wondering about that . good . barry , you just got through your quals , don't know if you have much to say . ","ICSI's Meeting Recorder Group have returned from a meeting with some important decisions to make. They have developed a piece of software which allows them to implement their two main approaches to dealing with noise. The base rate is currently set at the second best rate as of the last project evaluation, and it does not yet include everything the group have been working on. With this in mind, they have decided to set most things, and concentrate on studying only a few key aspects, the neural network, the voice activity detector, and the noise estimation. "
Bro025.C,"the piece of software has plenty of options , depending on that , it becomes either spectral subtraction or wiener filtering .  we if if we if which is like if we reduce the delay of va the you smooth it and then delay the decision by the frame dropping is the last thing that we do . just one more thing . like , should we do something f more for the noise estimation , ","They have developed a piece of software which allows them to implement their two main approaches to dealing with noise. The base rate is currently set at the second best rate as of the last project evaluation, and it does not yet include everything the group have been working on. With this in mind, they have decided to set most things, and concentrate on studying only a few key aspects, the neural network, the voice activity detector, and the noise estimation. "
Bro025.D,"no , just , looking into some of the things that , john ohala and hynek , gave as feedback , in my proposal , i was thinking about starting from a set of , phonological features , or a subset of them . but that might not be necessarily a good idea according to , john . ",
Bro025.E,"but , still there will be a piece of software with , will give this system , the fifty three point sixty six , by default it 's just one percent off of the best proposal . it 's between i we are second actually if we take this system . and all the speech pauses , which is sometimes on the speechdat car you have pauses that are more than one or two seconds . we cou we can do better , our current vad is more than twenty percent , while their is fourteen . just the frame dropping problem . and then we have to be careful with that also with the neural net because in the proposal the neural net was also , working on after frame dropping . ","The base rate is currently set at the second best rate as of the last project evaluation, and it does not yet include everything the group have been working on. With this in mind, they have decided to set most things, and concentrate on studying only a few key aspects, the neural network, the voice activity detector, and the noise estimation. "
Bro026.A,"there is th then the all the new features that go in . the , noise suppression , the re synthesis of speech after suppression .  i don't know if they use it , actually i tried wh while when i installed the repository , i tried from belgium . i logged in there and i tried to import it works . we 've been working like six weeks on the noise compensation and we end up with something that seems reasonable . finally it 's , wiener filtering on fft bins . we are going to fix this for the moment and work on the other aspects of the whole system . ri right now it 's second . ","Some members of the group met recently with research partners to settle on the current state of their software, and decide on the future work they would investigate, and these decisions were relayed to the rest of the group. "
Bro026.B,"we had a meeting with , with hynek , in which , sunil and stephane , summarized where they were and , talked about where we were gonna go . that happened mid week . what was the update ? they 're working on a different task . but the thing since you weren't yo you guys weren't at that meeting , might be just to , recap , the conclusions of the meeting . since he 's going out of town like now , and i 'm going out town in a couple weeks , and time is marching , given all the mu many wonderful things we could be working on , what will we actually focus on ? and , and what do we freeze ? and , what do we ? and then within that , the idea was to freeze a certain set of options for now , to run it , a particular way , and decide on what things are gonna be experimented with , as opposed to just experimenting with everything . keep a certain set of things constant . describe roughly what we are keeping constant for now , but structurally it seemed like the things the main things that we brought up that , are gonna need to get worked on are , a significantly better vad , putting the neural net on , which , we haven't been doing anything with , the , neural net at the end there , and , the , opening up the second front . cuz we have , half the , data rate that they allow . and , the initial thing which came from , the meeting that we had down south was , that , we 'll initially just put in a mel spectrum as the second one . it 's , cheap , easy . there 's a question about exactly how we do it . we probably will go to something better later , and , in some sense we 're all doing fairly similar things . why are we using half ? we have the on line normalization and then we have the lda rasta . the lda rasta , throws away high modulation frequencies . and they 're not doing that . that if you throw away high modulation frequencies , then you can downsample . and , we 've found in a lot of ways for quite a while that having a second stream helps a lot . that 's put in , and it may even end up with mel spectrum even though i 'm saying we could do much better , just because it 's simple . no , it 's in parallel . we 're not talking about computation time here . it 's just in terms of what data it 's depending on . it 's depending on the same data as the other . have you ever worked with the mississippi state h software ? you may be called upon to help , on account of , all the work in this here has been , with small vocabulary . cuz one of the things that might be helpful , if you 've got time in all of this is , is if these guys are really focusing on improving , all the digit and you got the front end from them , you could do the runs for the and , iron out hassles that you have to , tweak joe about or whatever , because you 're more experienced with running the large vocabulary th certainly the thing that i would want to know about is whether we get really hurt , on in insertion penalty , language model , scaling , sorts of things . in which case , h hari or hynek will need to , push the case more about this . joe , just to ask him about the issue of , different features having different kinds of , scaling characteristics and on . and just , se see . just cc hari and say that you 've just been asked to handle the large vocabulary part here , why don't you just ask joe but cc hari , and then in the note say , "" hari , hopefully this is with you "" . and then if joe feels like he needs a confirmation , hari can answer it . got anything to tell us ? are you looking at these in narrow bands ? it seems somehow that needs th there 's a couple things that i wonder about with this . if you 're going for this thing where you have little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands . the standard answer about this thing is that if you 're trying to find the right system in some sense , whether you 're trying by categories or parameters and your goal is discrimination , then having choices based on discrimination as opposed to , unsupervised nearness of things , is actually better . and i don't know if that since you 're dealing with issues of robustness , this isn't right , but it 'd be something i 'd be concerned about . because , you can imagine , i if you remember from , from your quals , john ohala saying that , "" buh "" and "" puh "" differed , not really cuz of voicing but because of aspiration . if you looked if you were doing some coarse clustering , you probably would put those two sounds together . and yet , i would gue i would guess that many of your recognition errors were coming from , pfft , screwing up on this distinction . if you go and take any recognizer that 's already out there and you say , "" how is it distinguishing between schwas and stops ? "" ","Some members of the group met recently with research partners to settle on the current state of their software, and decide on the future work they would investigate, and these decisions were relayed to the rest of the group. Of the three areas for the future, they touched mostly upon the use of a second, parallel, data stream. The group also discussed a new part to the evaluation, the use of a chunk of the Wall Street Journal. Speaker me006 is working on data clustering, and discussion of related issues led to more general acoustic matters. "
Bro026.B,"boy , i bet they 're all doing nearly perfectly on this , ","Speaker me006 is working on data clustering, and discussion of related issues led to more general acoustic matters. "
Bro026.C,"i 've been reading some literature about clustering of data . we 're talking about discovering intermediate categories to , to classify . and , i was looking at some of the work that , sangita was doing on these traps things . she has , she has temporal patterns for , a certain set of phonemes , from timit , and , i was thinking about ways to generalize this because w you 're it 's like a it 's not a completely automatic way of clustering , ","Speaker me006 is working on data clustering, and discussion of related issues led to more general acoustic matters. "
Bro026.D,"it 's it 's it was updated yesterday , i don't think anybody up there is like working on it right now . right now nobody 's working on aurora there . i 'll , 'll actually after the meeting i 'll add the second stream to the vad and 'll start with the feature net in that case . just figure how to take the features from the final th it 's almost ready . they have released their , document , describing the system . 'll point you to the web site and the mails corresponding . these sugges these this , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or sh shall we add chuck also to the mailing lists ? hari or hynek , one of them , has to send a mail to joe . ","The group also discussed a new part to the evaluation, the use of a chunk of the Wall Street Journal. "
Bro026.E,"d did you guys get your code pushed together ? is the , the cvs mechanism working are people , up at ogi grabbing code via that ? has anybody tried remotely accessing the cvs using , ssh ? it worked good ? you 're talking about the meeting with hynek ? are you gonna use which of the two techniques ? the other half of the channel ? if you took the system the way it is now , the way it 's fro you 're gonna freeze it , and it ran it on the last evaluation , where it would it be ? in terms of ranking ? how did they fill up this all these bits ? this second stream , will it add latency to the system what about the , the new part of the evaluation , the , wall street journal part ? not yet .  they 're gonna just deliver a system  using our features . and we may be able to revisit this idea about , somehow modifying our features to work with that 'd be great . i could send him an email . i was just talking with him on email the other day actually . ","Some members of the group met recently with research partners to settle on the current state of their software, and decide on the future work they would investigate, and these decisions were relayed to the rest of the group. Of the three areas for the future, they touched mostly upon the use of a second, parallel, data stream. The group also discussed a new part to the evaluation, the use of a chunk of the Wall Street Journal. "
Bro027.A,"i , started working on the mississippi state recognizer . i got in touch with joe and , from your email and things like that . and , they added me to the list the mailing list . and he gave me all of the pointers and everything that i needed . and downloaded the , there were two things , that they had to download . downloaded the software and compiled all of that . and it compiled fine . you asked me to write to him i 'll d i 'll double check that and ask him again . what if you used a smaller window for the delta ? there 's a lot of things you could do to wh what 's the baseline you need to be under ? how do that what you have is too much if they 're still deciding ? where is this fifty seven point o two in comparison to the last evaluation ? that 's how you get down to twenty eight ? and did an other numbers stay the same ? in the , lot of the , the hub five systems , recently have been using lda . and they , they run lda on the features right before they train the models . but what if you put ran the other lda , on your features right before they go into the the tandem is like i nonlinear lda . but w but the other features that you have , th the non tandem ones , you 're saying , feed that , also , into the neural net . ","The group discussed possible further investigations that arose from these areas, including better linking the two. "
Bro027.B,"say about just q just quickly to get through it , that dave and i submitted this asru . we 're dealing with rever reverberation , and , when we deal with pure reverberation , the technique he 's using works really , really and actually it brought up a question which may be relevant to the aurora too . i know that when you figured out the filters that we 're using for the mel scale , there was some experimentation that went on at , at ogi . but one of the differences that we found between the two systems that we were using , the aurora htk system baseline system and the system that we were the other system we were using , the the sri system , was that the sri system had , hundred hertz high pass . still , it 's possible that we 're getting in some more noise . wonder , is it @ @ was there their experimentation with , say , throwing away that filter think when he gets done with his prelim study one of the next things we 'd want to do is to take this , noise , processing and , synthesize some speech from it . is there any word yet about the issues about , adjustments for different feature sets or anything ? it 's like that could r turn out to be an important issue for us . now , we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters . but it 's still worth , just since just chatting with joe about the issue . and , it 's a l it 's a lot better . if you put the delta before the , ana on line if then it could go in parallel . we don't know . they 're still arguing about it . if it 's two if it 's , if it 's two fifty , then we could keep the delta where it is if we shaved off twenty . if it 's two hundred , if we shaved off twenty , we could we could , meet it by moving the delta back . the main thing is that since that we got burned last time , and by not worrying about it very much , we 're just staying conscious of it . if a week before we have to be done someone says , "" you have to have fifty milliseconds less than you have now "" , it would be pretty frantic around here . and you could experiment with cutting various pieces of these back a bit , we 're s we 're not in terrible shape . it 's it 's better than anything , anybody got . you 're just using the full ninety features ? and from the other side it 's forty five . it 's you have seventy three features , what 's your thought about what to do next with it ? we might we might have to experiment with , better training sets . i the other thing is , before you found that was the best configuration , but you might have to retest those things now that we have different the rest of it is different , what 's the effect of just putting the neural net on without the o other path ? what the straight features do . they felt they wanted to set a limit . they chose sixty . it 's arbitrary too . me either . did they increase the number of deletions even for the cases that got better ? it 's only the highly mismatched ? when you in the old experiments when you ran with the neural net only , and didn't have this side path , with the pure features as did it make things better to have the neural net ? until you put the second path in with the pure features , the neural net wasn't helping i still think it would be k interesting to see what would happen if you just had the neural net without the side thing . and the thing i have in mind is , you 'll see that the results are not just a little bit worse . that they 're a lot worse . but if on the ha other hand , it 's , say , somewhere in between what you 're seeing now and , what you 'd have with just the pure features , then there is some problem of a , combination of these things , or correlation between them somehow . if it really is that the net is hurting you at the moment , then the issue is to focus on , improving the net . the other thing you could do is just , p modify the , output probabilities of the , neural net , tandem neural net , based on the fact that you have a silence probability . now the only thing that bothers me about all this is that i the fact i it 's bothersome that you 're getting more deletions . won't be here for i 'm leaving next wednesday . i 'm leaving next wednesday . next week i won't , and the week after i won't , cuz i 'll be in finland . by that time you 'll be you 'll both be gone from here . it 'll be a few weeks , really , before we have a meeting of the same cast of characters . and then we 'll start up again with dave and barry and stephane and us on the , twentieth . ","They also consider how aspects of an absent member's work might be applied to the current project. The group discussed possible further investigations that arose from these areas, including better linking the two. The meeting closed with a discussion of upcoming absences, and how meetings would continue. "
Bro027.C,"i 've been playing with , first , the , vad . it 's exactly the same approach , but the features that the vad neural network use are , mfcc after noise compensation . before it was just p l it 's based on the system that has a fifty three point sixty six percent improvement . the only thing that changed is the n a p es the estimation of the silence probabilities . which now is based on , cleaned features . but the problem is still that the latency is too large . the latency of the vad is two hundred and twenty milliseconds . but we could probably put the delta , before on line normalization . cuz the time constant of the on line normalization is pretty long compared to the delta window , the best was fifty four point five . but i started to play with the , tandem neural network . did the configuration that 's very similar to what we did for the february proposal . there is a f a first feature stream that use straight mfcc features . and the other stream is the output of a neural network , using as input , also , these , cleaned mfcc . it improves on the matched and the mismatched conditions , but it get worse on the highly mismatched . from the networks , it 's twenty eight . there 's a klt after the neural network , as before . i wanted to do something very similar to the proposal as a first try . but we have to for we have to go down , because the limit is now sixty features . we have to find a way to decrease the number of features . i 'm surprised , because i expected the neural net to help more when there is more mismatch , as it was the case for the actually to s what i observed in the hm case is that the number of deletion dramatically increases . it doubles . when i added the num the neural network it doubles the number of deletions . don't how to interpret that , they p stayed the same , no . it was b a little bit worse . it was helping , if the features are b were bad , as soon as we added lda on line normalization , and all these things , then but it 's like a nonlinear discriminant analysis . in the proposal , they were transformed u using pca , it might be that lda could be better . might look at , is it due to the fact that the probability of the silence at the output of the network , is , too high ","The main areas being worked on were the voice activity detector and the tandem data streams. The group discussed possible further investigations that arose from these areas, including better linking the two. "
Bro027.D,"cuz they have , already frozen those in i insertion penalties and all those is what i feel . and they have these tables with , various language model weights , insertion penalties . it was just the noisy features this lda is different from the lda that you are talking about . the lda that you saying is you take a block of features , like nine frames and then do an lda on it , and then reduce the dimensionality to something like twenty four like that . this is a two dimensional tile . and the lda that we are f applying is only in time , it 's like more like a filtering in time , the other thing i was wondering was , if the neural net , has any because of the different noise con unseen noise conditions for the neural net , where you train it on those four noise conditions , while you are feeding it with additional some four plus some f few more conditions which it hasn't seen , actually , instead of just h having c those cleaned up t cepstrum , sh should we feed some additional information , like the should we f feed the vad flag , also , at the input that it has some additional discriminating information at the input ? we have the vad information also available at the back end . if it is something the neural net is not able to discriminate the classes by having an additional , feature which says "" this is speech and this is nonspeech "" , it certainly helps in some unseen noise conditions for the neural net . it 's an additional discriminating information . ","The group discussed possible further investigations that arose from these areas, including better linking the two. "
