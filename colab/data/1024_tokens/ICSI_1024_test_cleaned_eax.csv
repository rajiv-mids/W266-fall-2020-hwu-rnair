meeting,extractive,abstractive
Bed004.A,"what i did for this is a pedagogical belief net all i did was i took the last belief net and i grouped things according to what how they would fit in to image schemas that would be related . and the two that i came up with were trajector landmark and then source path goal as initial ones . we have the concept of what their intention was , whether they were trying to tour or do business or whatever , or they were hurried . that 's related to that . in terms of context , what we had currently said was whether they were a businessman or a tourist of some other person . discourse was related to whether they had asked about open hours prosody i don't really i 'm not really what prosody means , in this context , the parse would be what verb they chose , and in terms of world knowledge , this would just be like opening and closing times of things , the time of day it is , and whatnot . this is not a working bayes net . it 's hard for me to imagine how he could get around that . the immediate problem is just deciding w which ","It is not a working net yet, but identifying clusters of features that define the output mode provides a visual aid for further work. "
Bed004.B,"s if you just number them "" one "" , "" two "" , "" three "" it 's and also if she 's willing to take on the job of organizing all those subjects and that would be wonderful . we could talk to the people who run it and see if they have a way that they could easily tell people that there 's a task , pays ten bucks but you have to be comfortable reading relatively complicated now , i signed us up for the wednesday slot , and part of what we should do is this . my idea on that was partly we 'll talk about system for the computer scientists , but partly i did want it to get the linguists involved in some of this issue about what the task is and all what the dialogue is , and what 's going on linguistically , because to the extent that we can get them contributing , that will be good . you just do "" this is what we did , and here 's the thing , and here 's some of the dialogue and forth . "" is , if we just do this , we could wind up with a huge combinatoric input to the mode thing . which is there are technical ways of doing it , slipped a paper to bhaskara and about noisy or 's and noisy maxes not necessarily in th in this meeting , but to try to informally think about what the decision variables are . what are the most relevant things . and the other trick , which is not a technical trick , it 's knowledge engineering trick , is to make the n each node sufficiently narrow that you don't get this combinatorics . and then the question would be if those are the things that you care about , can you make a relatively compact way of getting from the various inputs to the things you care about . alright , le let me think about this some more , and see if we can find a way to present this to this linguists group that is helpful to them . i do understand that you can take the m three l and add not and it w we have to add , not too much about object types and and what you did is add some rules of the style that are already there that say "" if it 's of type "" landmark "" , then you take you 're gonna take a picture of it . "" that 's another thing "" here 's a another minimal way of tackling this "" . add extra properties , a deterministic rule for every property i if you had the generalized "" go "" x schema and you wanted to specialize it to these three ones , then you would have to supply the parameters . what are we going to use to make this decision the harder problem is we decide what we want to use , how are we gonna get it ? we have a d a technical problem with the belief nets that we don't want all the com too many factors if we allow them to just go combinatorially . there will be rules , but they aren't rules that come to final decisions , they 're rules that gather information for a decision process . my guess is it 'll be the same basic agent that can go off and get information , run it through a c this belief net that which can then be applied at what we would call the simulation or action end . and that may actually involve getting more information . what you 're trying to get out of this deep co cognitive linguistics is the fact that w if about source , paths and goals , and nnn all this that a lot of this is the same , for different tasks . and that there 's some important generalities that you 're getting , and i don't yet see how that goes . but what i 'd like to be able to do is to have the way that you extract properties , that will go into different bayes nets , be the general . that if you have sources , you have trajectors and like that , you shouldn't have to do that differently for going to something , than for circling it , for telling someone else how to go there , what you 'd really like is the same thing you 'd always like which is that you have intermediate representation which looks the same o over a bunch of inputs and a bunch of outputs . ","The group decided to hire the ""wizard"" and continue with the refinement of the design and recruitment of subjects. There are potential problems from a combinatorics perspective. These can be tackled either with technical adjustments or through careful knowledge engineering. A base solution for the task would be to simply add some extra action-mode rules in the SmartKom system. For instance, the final combination of features used in the current study may form a representation of the ontology, general enough to employ in any task that includes trajectors and paths. "
Bed004.C,"like , there was a wizard for both both parts , ",
Bed004.D,"on friday we had our wizard test data test and these are some of the results . this is what she had to read aloud . this was the first three tasks she had to master after she called the system , i should say the system was supposed to break down and then these were the remaining three tasks that she was going to solve , with a human the reading was five minutes , exactly . one time , pretending to be a system , one time , to pretending to be a human , which is actually not pretending . five minutes is just too long . that was already anticipated by some people suggested that if we just have bullets here , they 're gonna not they 're subjects are probably not gonna going to follow the order . she jumped around quite a bit . that is something that fey actually thought of a in the last second that sh the system should introduce itself , when it 's called . and another suggestion , by liz , was that we through subjects , switch the tasks . when they have task one with the computer , the next person should have task one with a human , and forth . we have to refine the tasks more and more , which we haven't done far , in order to avoid this rephrasing , and my suggestion is we keep the wizard , because she did a wonderful job , and told her that we gonna figure out a meeting time in the near future to refine the tasks and s look for the potential sources to find people . she also agrees that if it 's all just gonna be students the data is gonna be less valuable because of that but the that it just is a visual aid for thinking about these things which has comple clearly have to be specified m more carefully because integrated this into the existing smartkom system in the same way as much the same way we can have this this thing . and it would do us no good . and the rules we want to throw away completely . and and here is exactly where what 's gonna be replaced with our bayes net , aspects of the x schema to add . the belief net takes as input , a vector , and then we want to look up some more in the ontology we want to ask the real world , you want to look something up in the grs , but also we definitely want to look up in the dialogue history some s some this may be a process of two to three steps before we get our vector , that we feed into the belief net , we come up with a code for a module that we call the "" cognitive dispatcher "" , which does nothing , but it looks of complect object trees and decides how are there parts missing that need to be filled out , and then collect sub objects and then recombine them and put them together . that we have that this is the representational formats we 're we 're talking about that are independent of the problem , that generalize over those problems , and are t of a higher quality than an any actual whatever belief net , or "" x "" that we may use for the decision making , ultimately . ","A test run of the data collection design was very successful. The group decided to hire the ""wizard"" and continue with the refinement of the design and recruitment of subjects. It is not a working net yet, but identifying clusters of features that define the output mode provides a visual aid for further work. Action modes, however, can be inferred more efficiently by feeding a collection of features -from the ontology, discourse history, parsing, etc.- into Bayes-nets that would replace those rules. "
Bed009.A,"my suggestion then is that you look into the currently ongoing discussion about how the action plans are supposed to look like . and they 're currently agreeing or in the process of agreeing on an x m l ification of something like a state transition network of how dialogues would proceed . what 's called the action plan and what 's really the dialogue manager . is based on slots that have to be filled whereas in the a tourist domain it might be an entire route . and i 'm not if complex slots of that type are really being taken into consideration . it oughta be called a dialogue manager . and he 's gonna be responsible for the implementation of this action planner . no he 's completely gonna rewrite everything . in java . no , that 's gonna be phased out . and the underlying idea is that there is something like kernel modules with kernel functionality that you can plug certain applications like tourist information or the home scenario with controlling a vcr and on . that 's an additional reason to have this defined interface and keep these things like tourist information external . but what 's happening on line is just retrieval from the lexicon which would give all the stemming information ",
Bed009.B,"and just gonna say we have again the recognizer to parser thing where we 're working on and then that can be developed as needed when we get enter the tourism domain . either we do a syllable concatenating grammar for the english generation which is starting from scratch and doing it the easy way , or we simply adopt the more in depth style that is implemented in the german system for one thing we 're also using this room to collect data . no not meeting data but sort our version of a wizard experiment such and it 's a computer call system that gives you tourist information let 's say a simple parse from a s from an utterance won't really give us is what the person actually wants . and the idea is to construct suitable interfaces and a belief net for a module that actually tries to guess what the underlying intention was . these these types of these bits of additional information are going to be embedded into the m three l structure in an subfield that we have reserved . far i 've thought of it as adding it onto the modeler knowledge module . the true key issues is how does the whatever comes out of the language input pipeline look like and then what the action planner does with it and how that is specified . think just the spatial planner and the route planner printout of the communication between those two fills up i don't know how many pages whether we 're gonna stick to prolog or not . and language input is crucial also when you do the deep understanding analysis that we envision . ","The meeting was largely focused on SmartKom's decision making capacity and how to adapt this functionality to the tourist information domain. The Berkeley Even Deeper Understanding group discussed plans and concerns regarding the architecture of SmartKom, its proposed modules, and the types of interactions expected to take place between modules. "
Bed009.C,,
Bed009.D,it 's to integrate and syntactic analysis . the problem is th that it has to be very fast and they also have to be very robust . cuz of speech recognition errors we have knowledge bases from verbmobil system we can use some extensions have to be made . for a english version ,
Bed009.E,,
Bed009.F,"anyt we 'll find a time later in the week to get together and talk about your understanding of what smartkom plans are . one of the decisions is what we call this ave thing . that 's a discrete decision . but , th the current design suggests that if it seems to be an important decision and if the belief net is equivocal that it doesn't say that one of these is much more probable than the other , then an option is to go back and ask for the information you want . we probably won't do this early on , because the current focus is more on the decision making and like that . but while we 're on the subject wanted to give you a head 's up that it could be that some months from now we said "" we 're now ready to try to close that loop "" in terms of querying about some of these decisions . we ha we have to get in on that . action he action here means dia speech ac dialogue act . when when you get to the tourist domain it 's not just an information retrieval system . people are gonna have to think this through a bit more carefully . th the functional module that interacts with where the tourism g is going probably is too restrictive . we talked about this several times that the input end is gonna need a fair amount of feedback from the planning end . would there be any chance of getting the terminology changed that the dialogue planner was called a "" dialogue planner "" ? i if that c in persists then we 're gonna need another term . for the thing that actually does the planning of the routes and whatever we are doing for the tourist . the dialogue manager may think it 's in a dialogue state of one sort , and this one of these planning modules comes along and says "" hey , right now we need to ask a question "" . that forces the dialogue manager to change state . what are the plans roughly ? people at dfki have written a fair number of parsers . none of them are suitable ? but given th the constraints , that you want it to be small and fast and forth , my guess is you 're probably into some chunk parsing . and here 's the case where the english and the german might really be significantly different . in terms of if you 're trying to build some fast parser and forth we talked about the fact that there 're going to be a certain number of decisions that you want the knowledge modeler to make , that will be then fed to the function module , that does route planning . and then one half of this we talked about at little bit is how if you had the right information , and about th the something about was the agent a tourist or a native or a business person and also about the what we 're calling "" the entity "" , all that information could be combined into decision networks and give you decisions . but the other half of the problem is how would you get that information from the parsed input ? and the idea that we 're really after is a very deep semantics based on cognitive linguistics and the notion that there are a relatively small number of primitive conceptual schemas that characterize a lot of activity . what we 're really trying to do is to map from the discourse to the conceptual semantics level . and that all sorts of things , particularly in the tourist domain , can be represented in terms of source , path and goal . if you can do this , then the notion would be that across a very large range of domains , you could use this deep conceptual basis as the interface . and the idea of the belief net is it combines the information from the dialogue which comes across in this general way , and th the coupling to the situation comes in this model from , at th at the belief net , combining evidence from the dialogue with the ontology with the situation . ","The meeting was largely focused on SmartKom's decision making capacity and how to adapt this functionality to the tourist information domain. The Berkeley Even Deeper Understanding group discussed plans and concerns regarding the architecture of SmartKom, its proposed modules, and the types of interactions expected to take place between modules. "
Bed009.G,,
Bed016.A,"and y email any time , but most usefully before we 've thought about it before , to use the examples in other papers , and it 's a little complicated . would try to i would stay away from one that involves weird construal a fixed expression , ",
Bed016.B,,
Bed016.C,"not rehearse , can show you what i 've got , get your input on it , and some suggestions , and the same is true for the proposal . it says , this is construal "" , and then it continues to say that one could potentially build a probabilistic relational model that has some general , domain general rules how things are construed , and then the idea is to use ontology , situation , user , and discourse model to instantiate elements in the classes of the probabilistic relational model to do some inferences in terms of what is being construed as what in our beloved tourism domain . actually this is the newest version after your comments , if you would have checked your email you may have received a note from yees asking you to send me the , up to d current formalism thing that you presented . the twenty ninth . that 's when i 'm meeting with wolfgang wahlster to sell him this idea . then i 'm also going to present a little talk at eml , about what we have done here we can make a point that here is ontological knowledge but if it 's nine pm in the evening then the discotheque question would be , one that might ask for directions instead of just location . that 's motivating it . then what have we done far ? we had our little bit of , smartkom that we did , the generation outputter . and , and fey is doing the synthesis as we speak . then i 'm going to talk about the data , i will talk about our problems with the rephrasing , and some preliminary observations , and then talk about the big picture here , e tell a little bit as much as about the ntl story . and then , talk a bit about the embodied and simulation approach favored here schemas , then , i would like to do talk about the construction aspect and then at the end about our bayes net . the fmri but i don't am capable of do pulling this off and doing justice to the matter . might even mention that this work you 're doing is also with the mpi in leipzig , because , eml is building up a huge thing in leipzig . it seems like nothing is context free . but "" walked into the cafe and ordered a drink , "" and "" walked into the cafe and broke his nose , "" the "" walk into it "" never really means , w as in walked smack the i what would have been really is to find an example for all of this , from our domain . w we have , a canonical use of something and y it 's , we have some constructions and then it 's construed as something , and then we may get the same constructions with a metaphorical use that 's also relevant to the domain . we had initially we 'd started discussing the "" out of film . "" the old bakery example might be "" is there a bakery around here "" . where is the castle ? how old is it ? how much does it cost ? but as nancy just su suggested it 's probably ellipticus . my argument here is it 's it 's the same thing as "" plato 's on the top shelf , "" the data that i 've looked at far that rec there 's tons of cases for polysemy . but the argument should be can be made that , despite the fact that this is not the most met metaphorical domain , the "" where is something "" question as a whole , can be construed as , u i locational versus instructional request . but then you 're not on the lexical level , that 's one level higher . also it would be to get ultimately to get a mental space example , even temporal references are just in the spatial domain are rare . we can include that also in our second , data run . ","The thesis proposal, on the other hand, presents the idea of ""construal"" and makes claims as to how inferences are drawn in a probabilistic relational model by using information from the ontology, situation, user and discourse models. The latter will present the work that is currently being done at ICSI including examples of inference of user intentions and of the recordings of the on-going data collection. The meeting was taken up by discussion about a thesis proposal and a talk about to take place at EML. The talk will also outline the theoretical (X-schemas, image schemas, Bayes-nets) and neural background. Several potential examples of polysemy were discussed in detail: ""walk/run into"", ""on the bus"", ""out of film"", ""where is X?"". Base constructions deal with the norm, while further general domain mechanisms determine how the constructions are invoked depending on the context. However, none of them was an example of lexical polysemy resolved by construal straightforward enough to include in the proposal; the tourist domain is not metaphor rich. "
Bed016.D,,
Bed016.E,"you had s you said there were two things that you might wanna do . one was rehearse your i talk the time to mention it , if you mention it , is when you talk about mirror neurons , depends . there is a , whole language learning story , there the there we 've got various i generations of slides that show language analysis , and matching to the underlying image schemas , the b the combination of the biology and the leipzig connection might be interesting to these guys , let 's talk about your thesis proposal . i it looks like the , metaphor didn't get in yet . the top of the second of pa page two you have a sentence . no , it s says it it doesn't give the punch line . which is , that , the constructions , that , nancy and keith and friends are doing , are , in a way , quite general but cover only base cases . and to make them apply to metaphorical cases and metonymic cases and all those things , requires this additional mechanism , of construal . and the punch line is , he claimed , that if you do this right , you can get essentially orthogonality , that if you introduce a new construction at the base level , it should com interact with all the metonymies and metaphors that all of the projections of it also should work . and , similarly , if you introduce a new metaphor , it should then compose with all of the constructions . nothing is context free , but there are basic cases . that is , there are physical containers , it doesn't mean that they 're unambiguous . but it does say that , if you walked into the cafe and broke your nose , then you are construing the cafe as an obstacle . but "" run into "" does . but like , "" run into an old friend "" , it probably needs its own construction . there are idioms anything else you want to ask us about the thesis proposal , you got in the bus "" and "" on the bus , "" right . that may or may not be what you want to do . and there 're all sorts of reasons why you might be asking about the existence of a bakery but it 's not clear that they 're mainly construal examples . the metonymy thing is probably the easiest can we think of a metaphorical use of "" where "" in the tourist 's domain ? "" where was heidelberg , in the thirty years ' war ? "" "" where could i learn its opening hours , "" but that 's not metaphorical . a st you can certainly say , "" i 'm in overload . "" why don't we plan to give you feedback electronically . ","The meeting was taken up by discussion about a thesis proposal and a talk about to take place at EML. Base constructions deal with the norm, while further general domain mechanisms determine how the constructions are invoked depending on the context. Several potential examples of polysemy were discussed in detail: ""walk/run into"", ""on the bus"", ""out of film"", ""where is X?"". "
Bed016.F,"you 've got the parser done . you might want to , double check the spellings of the authors ' names on your references , you had a few , misspells in your slides , there . ",
Bmr005.A,"h how many total have we recorded now , altogether ? ",
Bmr005.B,"he was interested in the question of relating to his to the research he presented recently , of inference structures , and the need to build in , this mechanism for understanding of language . i looked through the transcript that we have far , and fou identified a couple different types of things of that type he 's interested in these knowledge structures , inferences that you draw i from and it is it is sensitive . i came up with something from the human subjects people that i wanted to mention . i asked her very specifically about this clause of how , it says "" no individuals will be identified we need to talk about this later . and sometimes , it was like you could have an overlap where someone said something in the middle , but , w it just wasn't important for our purposes to have it that i disrupt that unit in order to have , a the words in the order in which they were spoken , it would have been hard with the interface that we have . the a we 're gonna do a revised form , think from a s practical standpoint , we could have them do it once every ten meetings , ","Topics discussed by the Berkeley Meeting Recorder group included a potential collaboration with another ICSI member regarding the analysis of inference structures, efforts by speaker mn005 to detect speaker overlap, the current status on recordings and transcriptions, and future efforts to collect meeting data. "
Bmr005.C,"actually to make it worse , morgan uses "" you "" and "" you "" now , why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the like very straightforward question is where we are on the amount of data and the amount of transcribed data , there 's probably there 's three to four a week , and they 're each about an hour not complaining , i was just trying to think , what kinds of projects can you do now versus six months from now i don't wanna charge the time that i have on the project too early , before there 's enough data to make good use of the time . but there 's there are research questions you can answer without the transcriptions , or at least that you can start to answer . if anyone knows of one more m or two more wee meetings per week that happen at icsi , that we could record , it would be worth it . ","Topics discussed by the Berkeley Meeting Recorder group included a potential collaboration with another ICSI member regarding the analysis of inference structures, efforts by speaker mn005 to detect speaker overlap, the current status on recordings and transcriptions, and future efforts to collect meeting data. In addition to weekly meetings by the BMR group, efforts are in progress to record meetings by other ICSI research groups, as well as routine discussions by non-ICSI members. "
Bmr005.D,"i remind that me my first objective in the project is to study difference parameters to find a good solution to detect the overlapping zone in speech recorded . and was am transcribing the first session and i have found one thousand acoustic events , besides the overlapping zones , mean the breaths aspiration talk clap , almost three hundred in one session in five in forty five minutes . i consider the , nnn the nnn , nnn the entirety all the time there were the voice has overlapped . but don't distinguish between the numbers of speaker . i con i consider acoustic events the silent too . and i say that or this only because c i in my opinion , it 's necessary to to put the transcription on the speech file , collected by the objective signal . because i found a difference . but my idea is to process only nnn , this nnn , this of speech . but the transcription i don't i 'm not interested in the in the words , transcription words , transcribed in follow in the in the speech file , but jane put a mark at the beginning of each talker , she nnn includes information about the zone where there are there is an overlapping zone . but there isn't any mark , time temporal mark , to c to heh , to label the beginning and the end of the i consider all the session because count the nnn the overlappings marked by jane , on only to mark overlapping zone , but  ","Topics discussed by the Berkeley Meeting Recorder group included a potential collaboration with another ICSI member regarding the analysis of inference structures, efforts by speaker mn005 to detect speaker overlap, the current status on recordings and transcriptions, and future efforts to collect meeting data. "
Bmr005.E,"the one th one thing i know that we have on that is we had talked a couple weeks before about the the you were doing with attempting to locate events , but what we had meant by "" events "" was points of overlap between speakers . and then the other thing would be it might be to have a preliminary discussion of some of the other research areas that we 're thinking about doing . and one of the things i know that also came up is some discussions that jane had with lokendra but anyway some potential collaboration there about the working with these data . we were trying to think of ways that his interests could interact with ours and thought that if we were going to project into the future when we had a lot of data , and such things might be useful for that in or before we invested too much effort into that he should with jane 's help , look into some of the data that we 're already have and got the impression from your mail that there was enough things like this just in the little sample that you looked at that it 's plausible at least . but we were looking to see if there is there something in common between our interest in meetings and his interest in this but a actually , i 'm i really would like to push finish this off . i would like to move it into what jose has been doing how many overlaps were there in it ? then , in the region between since there is some continuous region , in between regions where there is only one person speaking . and one contiguous region like that you 're calling an event . the far field , it 'd be hard , but on the other hand as you point out , if your if i if your concern is to get the overlapping people 's speech , you will get that somewhat better . it 's three hundred in forty five minutes , but you have time marked twelve minute the overlaps in twelve minutes of it . it took you a long time to mark twelve minutes . now , my suggestion was for the other thirty three and my question is , if you did that , if you followed my suggestion , would it take much less time ? then it 's a good idea . the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy , such as the energy in the lpc residuals , such as the idea is to have some ground truth first . and the i the idea of the manual marking was to say "" this , i it 's really here "" . the idea was , i we thought it would be useful for him to look at the data anyway , and then whatever he could mark would be helpful , but we if if he could speed up what he was doing by just getting the speaker overlaps that we had it , say , for forty five minutes , then at least we 'd have three hundred examples of it . and when adam was doing his automatic thing he could then compare to that and see what it was different . right there 's this forty five minute piece that jane transcribed . we 're saying about twelve hours . but there 's at least one meeting recorded of the natural language guys . we 've started having a morning meeting , today starting a w a week or two ago , on the front end issues , and we 're recording those , there 's a network services and applications group here who 's to have their meetings recorded , i if we 're if we collect fifty or sixty hours , the meeting meetings it will probably be , twenty or thirty percent of it , whoever we have working on the acoustics for the meeting recorder are gonna start with that . one of the things i wanted to do , that i talked to don about , is one of the possible things he could do or m also , we could have someone else do it , is to do block echo cancellation , and the tack that we 've taken is more "" lets come up with feature approaches and multi stream approaches and forth , that will be robust to it for the recognizer and not try to create a clean signal "" . that 's the party line . that 's something i 'd like somebody to do at some point , just take these digits , take the far field mike signal , and the close mike signal , and apply really good echo cancellation . that would subtract off the parts of the signal that were the aspects of the signal that were different between the close talk and the distant . you 're trying to you 'd there 's a distance between the close and the distant mikes there 's a time delay there , not to try to completely remove it , that is , invert the room response , but just to try to eliminate some of the effect of some of the echos . echo cancelling is , commonly done in telephony , and it 's the obvious thing to do in this situation if you if , you 're gonna be talking some distance from a mike . somebody look and the digits would be a reasonable thing to do that with . let 's see . you actually already said this thing about the about the consent forms , we have an hour that is transcribed , we have twelve hours that 's recorded but not transcribed , ","Topics discussed by the Berkeley Meeting Recorder group included a potential collaboration with another ICSI member regarding the analysis of inference structures, efforts by speaker mn005 to detect speaker overlap, the current status on recordings and transcriptions, and future efforts to collect meeting data. In addition to weekly meetings by the BMR group, efforts are in progress to record meetings by other ICSI research groups, as well as routine discussions by non-ICSI members. "
Bmr005.E,"i th that if we are able to keep that up for a few months , we are gonna have more like a hundred hours . i 'd mentioned to adam , and that was another thing i was gonna talk mention to them before that there 's it oc it occurred to me that we might be able to get some additional data by talking to acquaintances in local broadcast media . but it did occur to me that we could go to friends in broadcast media and say "" hey you have this panel show , or this this discussion show , and can you record multi channel ? "" the other thing that occurred to me after we had that discussion , is that it 's even possible , since many radio shows are not live , that we could invite them to have like some of their record some of their shows here . think it 's something we should look into , but think that it 's not unreasonable to aim at getting , significantly in excess of a hundred hours . i would think , exploratory things now . three months from now but as far as the collection , it doesn't seem to me l like , unreasonable to say that in january , ro roughly which is roughly three months from now , we should have at least something like , twenty five , thirty hours . and do you have any idea when the you 'll be able to send the ten hours to them ? think once they get it sorted out about how they 're gonna do it , which they 're pretty along on , cuz they were able to read the files and on . and think it 's not going to be i don't think it 's going to be that much more of a deal for them to do thirty hours then to do one hour , we should also check with mari again , because they were really intending , just didn't happen , but they were really intending to be duplicating this in some level . one of the things that is a little bit of a limitation , there is a think when the people are not involved in our work , we probably can't do it every week . that people are gonna feel are gonna feel a little bit constrained . ",
Bmr005.F,"was just realizing we 've you guys have been talking about "" he "" for at least i don't know , three four minutes without ever mentioning the person 's name again . this is this is gonna be a big , big problem if you want to later do indexing , or speech understanding of any sort . ",
Bmr005.G,"the reason that i generated the mixed file was for ibm to do word level transcription , not speech event transcription . agree that if someone wants to do speech event transcription , that the mixed signals here if i 'm tapping on the table , you it 's not gonna show up on any of the mikes , but it 's gonna show up rather loudly in the pzm . about twelve by now . twelve or thirteen . and have some scripts that let you very quickly extract the sections of each utterance . if i did that , is someone gonna be working on it ? i would really like someone to do adaptation . it 'll be early next week . it 's pipeline , pipeline issues . it 's gonna be a problem to get people regularly . ",
Bmr005.H,,
Bmr019.A,"in the h l t paper we took segments that are channel time aligned , and we took cases where the transcribers said there was only one person talking here , and called that "" non overlap "" . the bad numbers were from the segments where there was overlap . and w we were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errors that were occurring actually it was better with slightly better or about th it was the same with tighter pruning . and what we really want is some clever way to do this , where , from the data or from some hand corrected alignments from transcribers that things like words that do occur just by themselves a alone , like backchannels that we did allow to have background speech around it those would be able to do that , but the rest would be constrained . we have a version that 's pretty good for the native speakers . and then there 's a background speech model . we probably want to adapt at least the foreground speaker . but , andreas tried adapting both the foreground and a background generic speaker , and that 's actually a little bit of a f funky model . like , it gives you some weird alignments , just because often the background speakers match better to the foreground than the foreground speaker . just working through a bunch of debugging kinds of issues . and it would be really useful to have transcriber who could use waves , interface wise if you 're looking at speech , you wanna be able to know really where the words are . and we can give you some examples of what this output looks like , and see if you can in incorporate it into the transcriber tool some way , it wou the advantage would just be that when you brought up a bin you would be able if you were zoomed in enough in transcriber to see all the words , you would be able to have the words located in time , we only r hav i only looked at actually alignments from one meeting that we chose , but there are fewer there are fewer "" huhs "" . if you looked at just a word frequency list of one word short utterances . and "" is way up there , we were the other thing we 're i should say is that we 're gonna , try compare this type of overlap analysis to switchboard , and callhome , where we have both sides , that we can try to answer this question of , is there really more overlap in meetings or is it just because we don't have the other channel in switchboard and we don't people are doing . and i figured we 'll try , because that 'll at least get us to the point where we have this really database format that andreas and i were working out that it 's just a ascii line by line format , it we 're calling these "" spurts "" after chafe . i was trying to find what 's a word for a continuous region with pauses around it ? because even if you weren't studying overlaps , if you wanna get a transcription for the far field mikes , how are you gonna know which words from which speakers occurred at which times relative to each other ? you have to be able to get a transcript like this anyway , just for doing far field recognition . tha there are some cases like where the wrong speaker these ca not a lot , but where the wrong person the speech is addre attached to the wrong speaker ","While debugging efforts resulted in improved forced alignments, dealing with mixed channel speech and speaker overlap remains a key objective for future work. Members also discussed efforts to produce forced alignments from a selection of Meeting Recorder data. "
Bmr019.B,"there the point of interest to the group was primarily that , the , the system that we had that was based on h t k , that 's used by , all the participants in aurora , was much worse than the s r and the interesting thing is that even though , yes , it 's a digits task and that 's a relatively small number of words and there 's a bunch of digits that you train on , it 's just not as good as having a l very large amount of data and training up a good big also you had the adaptation in the sri system , which we didn't have in this . bu although i 'd be it 'd be interesting to just take this exact actual system and try it out on ti digits . one is , the sri system is a lot better than the htk but the other is that , the digits recorded here in this room with these close mikes , i are actually a lot harder than the studio recording ti digits . but it 's an important data point , if you 're if the other thing that , what barry was looking at was just that , the near versus far . i was thinking was that i we could actually t try at least looking at , some of the large vocabulary speech from a far microphone , but i 'm saying if you do the same limited thing as people have done in switchboard evaluations or as a and that 's what we were getting those numbers from . we could start with the good ones . but anyway think that we should try it once with the same conditions that were used to create those , and in those same segments just use one of the p z but i 'd like to see it on the same exact same data set that we did the other thing on . we want to have the ability to feed it different features . and then , from the point of view of the front end research , it would be s substituting for htk . and then , also dave is thinking about using the data in different ways , to explicitly work on reverberation but were you intending to do a eurospeech submission , you and , and dan have a paper that 's going in . that 's pretty solid , on the segmentation and the aurora folks here will definitely get something in on aurora , ",The Berkeley Meeting Recorder group discussed efforts to train and test the Aurora group's HTK-based recognition system on ICSI's digits corpus. Performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers. The group is additionally focused on a continued ability to feed different features into the recognizer and then train the system accordingly. 
Bmr019.C,"since our representation in transcriber uses time marks , it seems like there should be some way of using that benefitting from that . fixed that . the other thing that was w interesting to me was that i picked up a lot of , backchannels which were hidden in the mixed signal , when i was looking at these backchannels , they were turning up usually very often in w i won't say "" usually "" but anyway , very often , i picked them up in a channel w which was the person who had asked a question . ",
Bmr019.D,,
Bmr019.E,"two items , which was , digits and possibly on , forced alignment , that it was the only thing that was even slightly surprising was that the lapel did because we may have to do an extract to get the amount of data per speaker about right . could we do exactly the same thing that we 're doing now , but do it with a far field mike ? cuz we extract the times from the near field mike , but you use the acoustics from the far field mike . i sh actually should 've picked a different one , because that could be why the pda is worse . because it 's further away from most of the people reading digits . we do we tend to do that anyway . ",Members also discussed efforts to produce forced alignments from a selection of Meeting Recorder data. 
Bmr019.F,"there was a significant loss from not doing the adaptation . but remember , we 're using a telephone bandwidth front end here , on this , on this sri system , i suspect that to get the last bit out of these higher quality recordings you would have to use models that , were trained on wider band data . right . but i 'm not much worried about the adaptation , actually , than the , the , vtl estimation . if you have only one utterance per speaker you might actually screw up on estimating the warping , factor . we might have to modify that script to recognize the , speakers , in the in the , ti digits database . we can improve these numbers if we care to compr improve them by , not starting with the switchboard models but by taking the switchboard models and doing supervised adaptation on a small amount of digit data collected in this setting . that 's where the most m acoustic mismatch is between the currently used models and the r the set up here . you want to probably choose the pzm channel that is closest to the speaker . what 's where do we go from here ? the key thing that 's missing here is the ability to feed , other features i into the recognizer and also then to train the system . it 's the front end is f i tha that 's in the sri recognizer is very in that it does a lot of things on the fly the what that means probably for the foreseeable future is that you have to , dump out , if you want to use some new features , you have to dump them into individual files the cumbersome thing is , is that you actually have to dump out little files . for free recognition , this the lower pruning value is better . but it turned out for to get accurate alignments it was really important to open up the pruning significantly . that was one big factor that helped improve things as liz said the we f enforce the fact that , the foreground speech has to be continuous . and you and what we wanted to try with once we have this paper written and have a little more time , t cloning that reject model and then one copy of it would be adapted to the foreground speaker to capture the rejects in the foreground , and the other copy would be adapted to the background speaker . we would need a hand marked , word level alignments or at least the boundaries of the speech betw between the speakers . and tune the parameters of the model , to op to get the best performance . it 's the spurt format . from each alignment we 're producing , one of these ctm files , which essentially has it 's just a linear sequence of words with the begin times for every word and the duration . and the second column is the channel . third column is the , start times of the words and the fourth column is the duration of the words . then we have a messy alignment process where we actually insert into the sequence of words the , tags for where sentence ends of sentence , and then we merge all the alignments from the various channels and we sort them by time . and you extract the individual channels , one sp spurt by spurt as it were . and inside the words or between the words you now have begin and end tags for overlaps . you have everything lined up and in a form where you can look at the individual speakers and how their speech relates to the other speakers ' speech . all we care about is whether that there 's a certain word was overlapped by someone else 's word . you at that point , you discretize things into just having overlap or no overlap . because we figure that 's about the level of analysis that we want to do for this paper . th the other good thing about the alignments is that , it 's not always the machine 's fault if it doesn't work . you can find , problems with the transcripts , ",Performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers. The group is additionally focused on a continued ability to feed different features into the recognizer and then train the system accordingly. 
Bmr019.G,"i know there were some speaker labelling problems , after interruptions . but you 're actually saying that certain , speakers were mis identified . ",Performance in both tasks was adversely affected by the manner of recording conditions implemented and difficulties attributing utterances to the appropriate speakers. 
Bmr019.H,,
Bmr019.I,,
Bro018.A,"then talked a little bit about continuing with these dynamic ev acoustic events , and we 're we 're thinking about a way to test the completeness of a set of dynamic events . completeness in the sense that if we pick these x number of acoustic events , do they provide sufficient coverage for the phones that we 're trying to recognize or the f the words that we 're gonna try to recognize later on . and morgan and i were discussing a form of a cheating experiment where we get we have chosen set of features , or acoustic events , and we train up a hybrid system to do phone recognition on timit . ",
Bro018.B,"that 's as far as my goes , tried this mean subtraction method . due to avendano , i 'm taking s six seconds of speech , 'm using two second fft analysis frames , stepped by a half second and i calculate the spectral mean , of the log magnitude spectrum over that n . i use that to normalize the s the current center frame by mean subtraction . and the i tried that with hdk , the aurora setup of hdk training on clean ti digits , and it helped in a phony reverberation case where used the simulated impulse response the error rate went from something like eighty it was from something like eighteen percent to four percent . and on meeting rec recorder far mike digits , mike on channel f , it went from forty one percent error to eight percent error . ","There has been further work on voiced/unvoiced detection, along with spectral subtraction. "
Bro018.C,"he 's not here , and wh when did stephane take off ? he 's going to icassp which is good . wanna talk a little bit about what we were talking about this morning ? have you had a chance to do this thing we talked about yet with the but i was gonna ask about the changes to the data in comparing plp and mel cepstrum for the sri system . we talked on the phone about this , that there was still a difference of a few percent and you told me that there was a difference in how the normalization was done . and i was asking if you were going to do redo it for plp with the normalization done as it had been done for the mel cepstrum . but that the normalization difference was one of the possibilities , ","The group discussed one members attendance at a conference, and another groups code, which is proving hard to follow. "
Bro018.D,"i will try to explain the thing that i did this week during this week . that i work i begin to work with a new feature to detect voice unvoice . what i trying two mlp to the with this new feature and the fifteen feature from the bus base system and i 'm trying two mlp , one that only have t three output , voice , unvoice , and silence , and other one that have fifty six output . the probabilities of the allophone . and i tried to do some experiment of recognition with that and only have result with the mlp with the three output . and , the result are li a little bit better , but more or less similar . and also h hynek last week say that if i have time to begin to study the france telecom proposal to look at the code i begin to work also in that . but the first thing that i don't understand is that they are using r the log energy that this quite i don't know why they have some constant in the expression of the lower energy . that stephane will arrive today or tomorrow . ","The group discussed one members attendance at a conference, and another groups code, which is proving hard to follow. "
Bro018.E,"no i haven't had a chance to do that . what i 've been doing is trying to figure out it just seems to me like there 's a it seems like there 's a bug , because the difference in performance is it 's not gigantic but it 's big enough that it seems wrong . don't think that the normalization difference is gonna account for everything . what i was working on is just going through and checking the headers of the wavefiles , ",
