{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in /Users/haileywu/opt/anaconda3/lib/python3.8/site-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "# T5ForConditionalGeneration is specific for sequence-to-sequence\n",
    "\n",
    "#from nlp import load_metric\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "import glob, os\n",
    "#import xml.etree.ElementTree as et\n",
    "from lxml import etree as et\n",
    "from collections import OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_test_set = [\n",
    "    'ES2004a',\n",
    "    'ES2004b',\n",
    "    'ES2004c',\n",
    "    'ES2004d',\n",
    "    'ES2014a',\n",
    "    'ES2014b',\n",
    "    'ES2014c',\n",
    "    'ES2014d',\n",
    "    'IS1009a',\n",
    "    'IS1009b',\n",
    "    'IS1009c',\n",
    "    'IS1009d',\n",
    "    'TS3003a',\n",
    "    'TS3003b',\n",
    "    'TS3003c',\n",
    "    'TS3003d',\n",
    "    'TS3007a',\n",
    "    'TS3007b',\n",
    "    'TS3007c',\n",
    "    'TS3007d'\n",
    "]\n",
    "\n",
    "icsi_test_set = [\n",
    "    'Bed004',\n",
    "    'Bed009',\n",
    "    'Bed016',\n",
    "    'Bmr005',\n",
    "    'Bmr019',\n",
    "    'Bro018'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ami_test_set)+len(icsi_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_BERT = \"/Users/haileywu/Desktop/W266_project/BERTSUMresult/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_ICSI = \"/Users/haileywu/Desktop/W266_project/data/ICSI_plus_NXT/\"\n",
    "DIR_AMI = \"/Users/haileywu/Desktop/W266_project/data/AMI_manual/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sum_icsi = pd.read_csv(DIR_ICSI + \"T5_csv/goldsummary_ICSI_as_test.csv\")\n",
    "abs_sum_ami = pd.read_csv(DIR_AMI + \"T5_csv/goldsummary_AMI_as_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meeting</th>\n",
       "      <th>abstractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bed004</td>\n",
       "      <td>A test run of the data collection design was v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bed009</td>\n",
       "      <td>The Berkeley Even Deeper Understanding group d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bed016</td>\n",
       "      <td>The meeting was taken up by discussion about a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bmr005</td>\n",
       "      <td>Topics discussed by the Berkeley Meeting Recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bmr019</td>\n",
       "      <td>The Berkeley Meeting Recorder group discussed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bro018</td>\n",
       "      <td>The ICSI Meeting Recorder Group met once more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  meeting                                        abstractive\n",
       "0  Bed004  A test run of the data collection design was v...\n",
       "1  Bed009  The Berkeley Even Deeper Understanding group d...\n",
       "2  Bed016  The meeting was taken up by discussion about a...\n",
       "3  Bmr005  Topics discussed by the Berkeley Meeting Recor...\n",
       "4  Bmr019  The Berkeley Meeting Recorder group discussed ...\n",
       "5  Bro018  The ICSI Meeting Recorder Group met once more ..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_sum_icsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES2004a\n",
      "ES2004b\n",
      "ES2004c\n",
      "ES2004d\n",
      "ES2014a\n",
      "ES2014b\n",
      "ES2014c\n",
      "ES2014d\n",
      "IS1009a\n",
      "IS1009b\n",
      "IS1009c\n",
      "IS1009d\n",
      "TS3003a\n",
      "TS3003b\n",
      "TS3003c\n",
      "TS3003d\n",
      "TS3007a\n",
      "TS3007b\n",
      "TS3007c\n",
      "TS3007d\n"
     ]
    }
   ],
   "source": [
    "for prediction in sorted(glob.glob(ROOT_DIR_BERT+\"TPRED_ami*.txt\")):\n",
    "    print(prediction.split(\"/\")[-1].split(\".\")[0].split(\"_\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filler_words(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        filler = f.read().splitlines()\n",
    "\n",
    "    return filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utterance(utterance, filler_words):\n",
    "    utt = utterance\n",
    "    # replace consecutive unigrams with a single instance\n",
    "    utt = re.sub('\\\\b(\\\\w+)\\\\s+\\\\1\\\\b', '\\\\1', utt)\n",
    "    # same for bigrams\n",
    "    utt = re.sub('(\\\\b.+?\\\\b)\\\\1\\\\b', '\\\\1', utt)\n",
    "    # strip extra white space\n",
    "    utt = re.sub(' +', ' ', utt)\n",
    "    # strip leading and trailing white space\n",
    "    utt = utt.strip()\n",
    "\n",
    "    # remove filler words # highly time-consuming\n",
    "    utt = ' ' + utt + ' '\n",
    "    for filler_word in filler_words:\n",
    "        utt = re.sub(' ' + filler_word + ' '+ '.'+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' '+ ','+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' '+ '?'+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' ', ' ', utt) \n",
    "        #utt = re.sub(' ' + filler_word.capitalize() + ' ', ' ', utt)\n",
    "\n",
    "    return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_ami_icsi(diag, filler_words):\n",
    "#     asr_output = pd.read_csv(\n",
    "#         path,\n",
    "#         sep='\\t',\n",
    "#         header=None,\n",
    "#         names=['ID', 'start', 'end', 'letter', 'role', 'A', 'B', 'C', 'utt']\n",
    "#     )\n",
    "    utt = diag\n",
    "    utterances = []\n",
    "#     for tmp in zip(asr_output['role'].tolist(), asr_output['utt'].tolist()):\n",
    "#         role, utt = tmp\n",
    "#         # lower case\n",
    "    utt = str(utt).lower()\n",
    "\n",
    "    # remove special tag\n",
    "    for ch in ['{nonvocalsound}','{vocalsound}', '{gap}', '{disfmarker}', '{comment}', '{pause}', '@reject@']:\n",
    "        utt = re.sub(ch, '', utt)\n",
    "\n",
    "    utt = re.sub(\"'Kay\", 'okay', utt)\n",
    "    utt = re.sub(\"'kay\", 'okay', utt)\n",
    "    utt = re.sub('\"Okay\"', 'okay', utt)\n",
    "    utt = re.sub(\"'cause\", 'cause', utt)\n",
    "    utt = re.sub(\"'Cause\", 'cause', utt)\n",
    "    utt = re.sub('\"cause\"', 'cause', utt)\n",
    "    utt = re.sub('\"\\'em\"', 'them', utt)\n",
    "    utt = re.sub('\"\\'til\"', 'until', utt)\n",
    "    utt = re.sub('\"\\'s\"', 's', utt)\n",
    "    utt = re.sub('\"\\\"', \"\" , utt)\n",
    "    utt = re.sub(\"-\", ' ', utt)\n",
    "    # l. c. d. -> lcd\n",
    "    # t. v. -> tv\n",
    "    utt = re.sub('h. t. m. l.', 'html', utt)\n",
    "    utt = re.sub(r\"(\\w)\\. (\\w)\\. (\\w)\\.\", r\"\\1\\2\\3\", utt)\n",
    "    utt = re.sub(r\"(\\w)\\. (\\w)\\.\", r\"\\1\\2\", utt)\n",
    "    utt = re.sub(r\"(\\w)\\.\", r\"\\1\", utt)\n",
    "\n",
    "    # clean_utterance, remove filler_words\n",
    "    utt = clean_utterance(utt, filler_words=filler_words)\n",
    "\n",
    "    # strip extra white space\n",
    "    utt = re.sub(' +', ' ', utt)\n",
    "    # strip leading and trailing white space\n",
    "    utt = utt.strip()\n",
    "\n",
    "    if not re.match(r'^[_\\W]+$', utt) and utt != '':\n",
    "    #if utt != '' and utt != '.' and utt != ' ' and utt!= \"?\" and utt!= \",\" and :\n",
    "        utterances.append(utt)\n",
    "    if len(utterances)>0:\n",
    "    # remove duplicate utterances per speaker\n",
    "        utterances = sorted(set(utterances), key=utterances.index)[0]\n",
    "        result = str(utterances)\n",
    "    else:\n",
    "        result = \"\"\n",
    "    #utterances_indexed = list(zip(range(len(utterances)), list(zip(*utterances))[0], list(zip(*utterances))[1]))\n",
    "    #list(zip(*utterances))[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_UTILS = \"/Users/haileywu/Desktop/W266_project/data/utils/\"\n",
    "filler_words = load_filler_words(ROOT_DIR_UTILS+\"filler_words.less.txt\")\n",
    "#stopwords = load_stopwords(ROOT_DIR_UTILS+\"stopwords.en.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comma(utterance_indexed):\n",
    "    #utterances_processed = []\n",
    "#    for utterance_indexed in rst:\n",
    "        # remove the comma at the beginning\n",
    "    if len(utterance_indexed) >0:\n",
    "        if utterance_indexed[0] == \",\" or utterance_indexed[0] == \".\" :\n",
    "            utt_cleaned = utterance_indexed[2:]  \n",
    "        else:\n",
    "            utt_cleaned = utterance_indexed\n",
    "        #print(utt_cleaned)\n",
    "        #utterances_processed.append(utt_cleaned)\n",
    "    else:\n",
    "        utt_cleaned = \"\"\n",
    "    return utt_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/BERSUM/ICSI_test_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for prediction in sorted(glob.glob(ROOT_DIR_BERT+\"TPRED_icsi*.txt\")):\n",
    "        i=0\n",
    "        text = \"\"\n",
    "        paragraphcount = 0\n",
    "        meetingname = prediction.split(\"/\")[-1].split(\".\")[0].split(\"_\")[2]\n",
    "        if meetingname in icsi_test_set:\n",
    "            abstractive_summary = abs_sum_icsi[abs_sum_icsi[\"meeting\"]==meetingname]['abstractive'].values[0]\n",
    "            file = open(prediction,mode='r')\n",
    "            for f in file.readlines():\n",
    "                diag = f.split(\"\\n\")[0] \n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraphcount > 1024:\n",
    "                    writer.writerow({'meeting':meetingname+'.'+str(i), 'extractive': text, 'abstractive': abstractive_summary})\n",
    "                    text = \"\"\n",
    "                    paragraphcount = 0\n",
    "                    i+=1\n",
    "                text += diag\n",
    "                paragraphcount += diag_word_count\n",
    "            writer.writerow({'meeting':meetingname+'.'+str(i), 'extractive': text, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/BERSUM/ICSI_test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     927\n",
       "1     355\n",
       "2     928\n",
       "3     497\n",
       "4     899\n",
       "5     195\n",
       "6     929\n",
       "7     914\n",
       "8     667\n",
       "9     938\n",
       "10    910\n",
       "11    398\n",
       "12    936\n",
       "13    593\n",
       "Name: extractive, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['abstractive'].notna()]['extractive'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
