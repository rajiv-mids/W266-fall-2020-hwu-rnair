{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICSI corpus and AMI corpus\n",
    "\n",
    "## Defined Training, Dev, and Test Dataset \n",
    "Go through the list of files and extract passage and the summaries, using the DialogueActs as intermediary\n",
    "Uses the following mappings:\n",
    "\n",
    "__1__) meeting id -> ordered dict of word id: word\n",
    "\n",
    "__2__) meeting id -> ordered dict of dialog id -> list of (start, end) word ids that form a dialog. \n",
    "\n",
    "__3__) summary_starts, summary_ends -> sets of start and end dialog ids that belong to extractive summaries\n",
    "\n",
    "\n",
    "function `get_words(meeting, meetingtype, start_word=None, end_word=None)` # goes through the meeting and returns words from start to end if provided. Else prints everything; `meetingtype` is either \"AMI\" or \"ICSI\"\n",
    "\n",
    "function `extract_dialogues(meeting, meetingtype)` # goes through the meeting and returns list of  (dialogue, SummaryFlag) for the meeting, `meetingtype` is either \"AMI\" or \"ICSI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "* http://groups.inf.ed.ac.uk/ami/corpus/annotation.shtml\n",
    "* http://groups.inf.ed.ac.uk/ami/icsi/\n",
    "* https://www.groundai.com/project/end-to-end-abstractive-summarization-for-meetings/1\n",
    "* https://bitbucket.org/dascim/offline_meeting_summarization/src/master/\n",
    "* https://github.com/gcunhase/AMICorpusXML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please download the related ICSI corpus and AMI corpus from the websites, and change the root directory below to point to them each.* The saving will also require a file directory as:\n",
    "\n",
    "* `data/ICSI_plus_NXT/Full_doc/Dev`, \n",
    "* `data/ICSI_plus_NXT/Full_doc/Train`, \n",
    "* `data/ICSI_plus_NXT/Full_doc/Test`,\n",
    "* `data/AMI_manual/Full_doc/Dev`,\n",
    "* `data/AMI_manual/Full_doc/Test`,\n",
    "* `data/AMI_manual/Full_doc/Train`\n",
    "\n",
    "\n",
    "The dataset is split on the speaker (turn) level, meaning for each hour of the meeting (e.g.ES2004a), the saving will have ES2004a.A, ES2004a.B, etc. The abstractive summary is on the meeting level (e.g. ES2004a)\n",
    "\n",
    "**During Model Training, do separate ROUGE evaluation on AMI and ICSI datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "#import xml.etree.ElementTree as et\n",
    "from lxml import etree as et\n",
    "from collections import OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import operator\n",
    "from nltk import PerceptronTagger\n",
    "\n",
    "ROOT_DIR_ICSI = \"/Users/haileywu/Desktop/W266_project/data/ICSI_plus_NXT/ICSIplus\"\n",
    "ROOT_DIR_AMI = \"/Users/haileywu/Desktop/W266_project/data/AMI_manual/ami_public\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev, Test Split\n",
    "\n",
    "based on https://bitbucket.org/dascim/acl2018_abssumm/src/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_test_set = [\n",
    "    'ES2004a',\n",
    "    'ES2004b',\n",
    "    'ES2004c',\n",
    "    'ES2004d',\n",
    "    'ES2014a',\n",
    "    'ES2014b',\n",
    "    'ES2014c',\n",
    "    'ES2014d',\n",
    "    'IS1009a',\n",
    "    'IS1009b',\n",
    "    'IS1009c',\n",
    "    'IS1009d',\n",
    "    'TS3003a',\n",
    "    'TS3003b',\n",
    "    'TS3003c',\n",
    "    'TS3003d',\n",
    "    'TS3007a',\n",
    "    'TS3007b',\n",
    "    'TS3007c',\n",
    "    'TS3007d'\n",
    "]\n",
    "\n",
    "ami_development_set = [\n",
    "    'ES2005b',\n",
    "    'ES2005d',\n",
    "    'ES2007b',\n",
    "    'ES2008a',\n",
    "    'ES2008d',\n",
    "    'ES2015d',\n",
    "    'IS1003c',\n",
    "    'IS1004c',\n",
    "    'IS1006b',\n",
    "    'IS1006d',\n",
    "    'TS3004c',\n",
    "    'TS3005d',\n",
    "    'TS3006c',\n",
    "    'TS3008b',\n",
    "    'TS3011a',\n",
    "\n",
    "    'ES2005a',\n",
    "    'ES2005c',\n",
    "    'ES2007a',\n",
    "    'ES2007c',\n",
    "    'ES2007d',\n",
    "    'ES2008b',\n",
    "    'ES2008c',\n",
    "    'ES2015a',\n",
    "    'ES2015b',\n",
    "    'ES2015c',\n",
    "    'IS1003a',\n",
    "#    'IS1003b',\n",
    "    'IS1003d',\n",
    "    'IS1004a',\n",
    "    'IS1004b',\n",
    "    'IS1004d',\n",
    "    'IS1006a',\n",
    "    'IS1006c',\n",
    "    'TS3004a',\n",
    "    'TS3004b',\n",
    "    'TS3004d',\n",
    "    'TS3005a',\n",
    "    'TS3005b',\n",
    "    'TS3005c',\n",
    "    'TS3006a',\n",
    "    'TS3006b',\n",
    "    'TS3006d',\n",
    "    'TS3008a',\n",
    "    'TS3008c',\n",
    "    'TS3008d',\n",
    "    'TS3011b',\n",
    "    'TS3011c',\n",
    "    'TS3011d'\n",
    "]\n",
    "\n",
    "icsi_test_set = [\n",
    "    'Bed004',\n",
    "    'Bed009',\n",
    "    'Bed016',\n",
    "    'Bmr005',\n",
    "    'Bmr019',\n",
    "    'Bro018'\n",
    "]\n",
    "\n",
    "icsi_development_set = [\n",
    "    'Bed003',\n",
    "    'Bed006',\n",
    "    'Bed011',\n",
    "    'Bed014',\n",
    "    'Bed015',\n",
    "    'Bed017',\n",
    "    'Bmr013',\n",
    "    'Bmr014',\n",
    "    'Bmr015',\n",
    "    'Bmr020',\n",
    "    'Bro023',\n",
    "    'Bro024',\n",
    "    'Bro025',\n",
    "    'Bro026',\n",
    "    'Bro027',\n",
    "\n",
    "    'Bed002',\n",
    "    'Bed005',\n",
    "    'Bed008',\n",
    "    'Bed010',\n",
    "    'Bed012',\n",
    "    'Bed013',\n",
    "    'Bmr003',\n",
    "    'Bmr006',\n",
    "    'Bmr007',\n",
    "    'Bmr018',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract words\n",
    "\n",
    "__1__) meeting id -> ordered dict of word id: word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meeting_dict_ICSI = dict() # key = meeting, value = ordered dict of word id: word\n",
    "cur_word = None\n",
    "for word_f in sorted(glob.glob(ROOT_DIR_ICSI+\"/Words/*xml\")):\n",
    "    m = word_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(word_f).getroot()\n",
    "    nodes = root.findall('*')\n",
    "    meet_words = OrderedDict()\n",
    "\n",
    "    for node in nodes:\n",
    "        if node.tag=='w':\n",
    "            meet_words[node.attrib['{http://nite.sourceforge.net/}id']] = node.text\n",
    "            cur_word = node.text\n",
    "        for w in node.values():\n",
    "            if \"disfmarker\" in w or \"pause\" in w or \"vocalsound\" in w:\n",
    "                meet_words [node.attrib['{http://nite.sourceforge.net/}id']] = None\n",
    "    meeting_dict_ICSI[meeting_name] = meet_words\n",
    "    \n",
    "#meeting_dict_ICSI['Bdb001.D']['Bdb001.w.2,391']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_dict_ICSI['Bdb001.D']['Bdb001.w.2,391']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_dict_AMI = dict() # key = meeting, value = ordered dict of word id: word\n",
    "cur_word = None\n",
    "for word_f in sorted(glob.glob(ROOT_DIR_AMI+\"/words/*xml\")):\n",
    "    m = word_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(word_f).getroot()\n",
    "    nodes = root.findall('*')\n",
    "    meet_words = OrderedDict()\n",
    "\n",
    "    for node in nodes:\n",
    "        if node.tag=='w':\n",
    "            meet_words[node.attrib['{http://nite.sourceforge.net/}id']] = node.text\n",
    "            cur_word = node.text\n",
    "        for w in node.values():\n",
    "            if \"disfmarker\" in w or \"pause\" in w or \"vocalsound\" in w:\n",
    "                meet_words [node.attrib['{http://nite.sourceforge.net/}id']] = None\n",
    "    meeting_dict_AMI[meeting_name] = meet_words\n",
    "    \n",
    "#meeting_dict_AMI['EN2001a.A']['EN2001a.A.words0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_dict_AMI['EN2001a.A']['EN2001a.A.words0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(meeting, meetingtype, start=None, end=None):\n",
    "    ret = \"\"\n",
    "    include = False\n",
    "    if start is None:\n",
    "        include = True\n",
    "    if meetingtype == \"AMI\":\n",
    "        for meet_word in meeting_dict_AMI[meeting].keys():\n",
    "\n",
    "            if start is not None and meet_word == start:\n",
    "                include = True\n",
    "            if include:\n",
    "                if meeting_dict_AMI[meeting][meet_word] is not None:\n",
    "                    ret += meeting_dict_AMI[meeting][meet_word] + \" \"\n",
    "            if end is not None and meet_word == end:\n",
    "                include = False\n",
    "        return ret\n",
    "    elif meetingtype == \"ICSI\":\n",
    "        for meet_word in meeting_dict_ICSI[meeting].keys():\n",
    "\n",
    "            if start is not None and meet_word == start:\n",
    "                include = True\n",
    "            if include:\n",
    "                if meeting_dict_ICSI[meeting][meet_word] is not None:\n",
    "                    ret += meeting_dict_ICSI[meeting][meet_word] + \" \"\n",
    "            if end is not None and meet_word == end:\n",
    "                include = False\n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So you 're essentially defining a lattice . Yeah . How - how Oh , that 's \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words('Bdb001.A', \"ICSI\", \"Bdb001.w.915\", \"Bdb001.disfmarker.49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay . '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words('EN2001a.A',\"AMI\",'EN2001a.A.words0','EN2001a.A.words1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dialogues\n",
    "\n",
    "__2__) meeting id -> ordered dict of dialog id -> tuple of (start, end) word ids that form a dialog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_dialogues_ICSI = dict() # key = meeting name, value = ordered dict of dialog id -> List of (start, end, SummaryFlag) word ids that form a dialog. \n",
    "for dialog_f in sorted(glob.glob(ROOT_DIR_ICSI+\"/DialogueActs/*acts.xml\")):\n",
    "    m = dialog_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(dialog_f).getroot()\n",
    "    dlist = root.findall('dialogueact')\n",
    "    for dl in dlist:\n",
    "        nodes = dl.findall(\"{http://nite.sourceforge.net/}child\")\n",
    "        dialog_id = dl.attrib['{http://nite.sourceforge.net/}id']\n",
    "\n",
    "        for dl_c in nodes:\n",
    "            words = dl_c.attrib['href']\n",
    "            words = words.split(\"#\")[1]\n",
    "            try:\n",
    "                start, end = words.split(\"..\")\n",
    "                start, end = start.replace(\"id\", \"\"), end.replace(\"id\", \"\")\n",
    "                start, end = start.replace(\"(\", \"\"), end.replace(\"(\", \"\")\n",
    "                start, end = start.replace(\")\", \"\"), end.replace(\")\", \"\")\n",
    "            except:\n",
    "                start = end = words.replace(\"id\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            if meeting_name not in meeting_dialogues_ICSI:\n",
    "                meeting_dialogues_ICSI[meeting_name] = OrderedDict()\n",
    "            meeting_dialogues_ICSI[meeting_name][dialog_id] = [start, end, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bdb001.w.691', 'Bdb001.w.700', None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_dialogues_ICSI['Bdb001.A']['Bdb001.A.dialogueact74']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_dialogues_AMI = dict() # key = meeting name, value = ordered dict of dialog id -> List of (start, end, SummaryFlag) word ids that form a dialog. \n",
    "for dialog_f in sorted(glob.glob(ROOT_DIR_AMI+\"/DialogueActs/*act.xml\")):\n",
    "    m = dialog_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(dialog_f).getroot()\n",
    "    dlist = root.findall('dact')\n",
    "    for dl in dlist:\n",
    "        nodes = dl.findall(\"{http://nite.sourceforge.net/}child\")\n",
    "        dialog_id = dl.attrib['{http://nite.sourceforge.net/}id']\n",
    "\n",
    "        for dl_c in nodes:\n",
    "            words = dl_c.attrib['href']\n",
    "            words = words.split(\"#\")[1]\n",
    "            try:\n",
    "                start, end = words.split(\"..\")\n",
    "                start, end = start.replace(\"id\", \"\"), end.replace(\"id\", \"\")\n",
    "                start, end = start.replace(\"(\", \"\"), end.replace(\"(\", \"\")\n",
    "                start, end = start.replace(\")\", \"\"), end.replace(\")\", \"\")\n",
    "            except:\n",
    "                start = end = words.replace(\"id\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            if meeting_name not in meeting_dialogues_AMI:\n",
    "                meeting_dialogues_AMI[meeting_name] = OrderedDict()\n",
    "            meeting_dialogues_AMI[meeting_name][dialog_id] = [start, end, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ES2002a.A.words0', 'ES2002a.A.words12', None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meeting_dialogues_AMI['ES2002a.A']['ES2002a.A.dialog-act.dharshi.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3__) meeting id -> set of dialog ids that belong to extractive summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_starts_ICSI, summary_ends_ICSI = set(),set() # key = meeting name, list (2) of set of Dialogue starts and ends\n",
    "\n",
    "for dialog_f in sorted(glob.glob(ROOT_DIR_ICSI+\"/Contributions/Summarization/extractive/*extsumm.xml\")):\n",
    "    m = dialog_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(dialog_f).getroot()\n",
    "    dlist = root.findall('extsumm/{http://nite.sourceforge.net/}child')\n",
    "    for dl in dlist:\n",
    "        dialogs = dl.attrib['href']\n",
    "        dialogs = dialogs.split(\"#\")[1]\n",
    "        start = end = None\n",
    "        try:\n",
    "            start, end = dialogs.split(\"..\")\n",
    "            start, end = start.replace(\"id\", \"\"), end.replace(\"id\", \"\")\n",
    "            start, end = start.replace(\"(\", \"\"), end.replace(\"(\", \"\")\n",
    "            start, end = start.replace(\")\", \"\"), end.replace(\")\", \"\")\n",
    "        except:\n",
    "            start = end = dialogs.replace(\"id\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        summary_starts_ICSI.add(start)\n",
    "        summary_ends_ICSI.add(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_starts_AMI, summary_ends_AMI = set(),set() # key = meeting name, list (2) of set of Dialogue starts and ends\n",
    "\n",
    "for dialog_f in sorted(glob.glob(ROOT_DIR_AMI+\"/extractive/*extsumm.xml\")):\n",
    "    m = dialog_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]+\".\"+m[1]\n",
    "    root = et.parse(dialog_f).getroot()\n",
    "    dlist = root.findall('extsumm/{http://nite.sourceforge.net/}child')\n",
    "    for dl in dlist:\n",
    "        dialogs = dl.attrib['href']\n",
    "        dialogs = dialogs.split(\"#\")[1]\n",
    "        start = end = None\n",
    "        try:\n",
    "            start, end = dialogs.split(\"..\")\n",
    "            start, end = start.replace(\"id\", \"\"), end.replace(\"id\", \"\")\n",
    "            start, end = start.replace(\"(\", \"\"), end.replace(\"(\", \"\")\n",
    "            start, end = start.replace(\")\", \"\"), end.replace(\")\", \"\")\n",
    "        except:\n",
    "            start = end = dialogs.replace(\"id\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        summary_starts_AMI.add(start)\n",
    "        summary_ends_AMI.add(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_dialogues(meeting, meetingtype):\n",
    "    # loop through all the meetings and (optionally) return only the ones that fall in range of starts/ends\n",
    "    ret = []\n",
    "    include = False\n",
    "    if meetingtype == \"AMI\":\n",
    "        dialogues = meeting_dialogues_AMI[meeting]\n",
    "        for dialog, (start_w, end_w, _) in dialogues.items():\n",
    "            if dialog in summary_starts_AMI:\n",
    "                include = True\n",
    "            if include:\n",
    "                ret.append((dialog, start_w, end_w, get_words(meeting, meetingtype, start_w, end_w), True))\n",
    "                meeting_dialogues_AMI[meeting][dialog][2] = True\n",
    "            else:\n",
    "                ret.append((dialog, start_w, end_w, get_words(meeting, meetingtype, start_w, end_w), False))\n",
    "                meeting_dialogues_AMI[meeting][dialog][2] = False\n",
    "            if dialog in summary_ends_AMI:\n",
    "                include = False\n",
    "        return ret\n",
    "\n",
    "    elif meetingtype == \"ICSI\":\n",
    "        dialogues = meeting_dialogues_ICSI[meeting]\n",
    "        for dialog, (start_w, end_w, _) in dialogues.items():\n",
    "            if dialog in summary_starts_ICSI:\n",
    "                include = True\n",
    "            if include:\n",
    "                ret.append((dialog, start_w, end_w, get_words(meeting, meetingtype, start_w, end_w), True))\n",
    "                meeting_dialogues_ICSI[meeting][dialog][2] = True\n",
    "            else:\n",
    "                ret.append((dialog, start_w, end_w, get_words(meeting, meetingtype, start_w, end_w), False))\n",
    "                meeting_dialogues_ICSI[meeting][dialog][2] = False\n",
    "            if dialog in summary_ends_ICSI:\n",
    "                include = False\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bed009.G.dialogueact1133',\n",
       "  'Bed009.w.9,951',\n",
       "  'Bed009.w.9,955',\n",
       "  'Is it i in , ',\n",
       "  False),\n",
       " ('Bed009.G.dialogueact1134',\n",
       "  'Bed009.w.9,956',\n",
       "  'Bed009.w.9,966',\n",
       "  'then , your place , in five five - A ? ',\n",
       "  False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dialogues('Bed009.G',\"ICSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ES2004a.A.dialog-act.s9553330.1',\n",
       "  'ES2004a.A.words0',\n",
       "  'ES2004a.A.words3',\n",
       "  'Hmm hmm hmm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.2',\n",
       "  'ES2004a.A.words4',\n",
       "  'ES2004a.A.words5',\n",
       "  'Yeah . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.3',\n",
       "  'ES2004a.A.words6',\n",
       "  'ES2004a.A.words12',\n",
       "  \"Okay . Yep , yep . Okay . Tu tu tu tu Hi , good morning . 'Kay . Oops . Mm . Oh sorry . Mm-hmm . Yeah , me . Cat . Where did this come from ? Uh , yep . Thank you . Uh , maybe you can guess what I'm trying to make ? Yep . It's actually sitting , so it's sitting , it's not standing . Okay , I see it as one thing it's very supportive . It's your best friend and your you can talk to a dog , it can be your best friend , it doesn't discriminate between you , based on what you are . Second it's loyal and third thing it's got intuition . dogs can som sometimes can make out between a thief and a person so basically these are the three unique features I think belong to a dog . Thank you . Okay . Sorry . Does it look like a dog actually ? Mm . Eagle , okay . One point four or something like that . One point four Euro would make a Pound or something like that . Yeah . Okay , pretty huge margin . So then Mm-hmm . Yeah , that c Okay , you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.4',\n",
       "  'ES2004a.A.words13',\n",
       "  'ES2004a.A.words14',\n",
       "  'Okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.5',\n",
       "  'ES2004a.A.words15',\n",
       "  'ES2004a.A.words18',\n",
       "  'Tu tu tu tu ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.6',\n",
       "  'ES2004a.A.words19',\n",
       "  'ES2004a.A.words23',\n",
       "  'Hi , good morning . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.7',\n",
       "  'ES2004a.A.words24',\n",
       "  'ES2004a.A.words25',\n",
       "  \"'Kay . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.8',\n",
       "  'ES2004a.A.words26',\n",
       "  'ES2004a.A.words26',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.9',\n",
       "  'ES2004a.A.words27',\n",
       "  'ES2004a.A.words28',\n",
       "  'Oops . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.10',\n",
       "  'ES2004a.A.words29',\n",
       "  'ES2004a.A.words30',\n",
       "  'Mm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.11',\n",
       "  'ES2004a.A.words31',\n",
       "  'ES2004a.A.words33',\n",
       "  'Oh sorry . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.12',\n",
       "  'ES2004a.A.words34',\n",
       "  'ES2004a.A.words35',\n",
       "  'Mm-hmm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.13',\n",
       "  'ES2004a.A.words36',\n",
       "  'ES2004a.A.words39',\n",
       "  'Yeah , me . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.14',\n",
       "  'ES2004a.A.words40',\n",
       "  'ES2004a.A.words42',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.15',\n",
       "  'ES2004a.A.words43',\n",
       "  'ES2004a.A.words48',\n",
       "  'Where did this come from ? ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.16',\n",
       "  'ES2004a.A.words49',\n",
       "  'ES2004a.A.words52',\n",
       "  'Uh , yep . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.17',\n",
       "  'ES2004a.A.words53',\n",
       "  'ES2004a.A.words55',\n",
       "  'Thank you . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.18',\n",
       "  'ES2004a.A.words56',\n",
       "  'ES2004a.A.words67',\n",
       "  \"Uh , maybe you can guess what I'm trying to make ? \",\n",
       "  True),\n",
       " ('ES2004a.A.dialog-act.s9553330.19',\n",
       "  'ES2004a.A.words68',\n",
       "  'ES2004a.A.words69',\n",
       "  'Yep . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.20',\n",
       "  'ES2004a.A.words70',\n",
       "  'ES2004a.A.words75',\n",
       "  \"It's actually sitting , so it's sitting , it's not standing . Okay , I see it as one thing it's very supportive . It's your best friend and your you can talk to a dog , it can be your best friend , it doesn't discriminate between you , based on what you are . Second it's loyal and third thing it's got intuition . dogs can som sometimes can make out between a thief and a person so basically these are the three unique features I think belong to a dog . Thank you . Okay . Sorry . Does it look like a dog actually ? Mm . Eagle , okay . One point four or something like that . One point four Euro would make a Pound or something like that . Yeah . Okay , pretty huge margin . So then Mm-hmm . Yeah , that c Okay , you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.21',\n",
       "  'ES2004a.A.words76',\n",
       "  'ES2004a.A.words82',\n",
       "  \"it's sitting , it's not standing . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.22',\n",
       "  'ES2004a.A.words83',\n",
       "  'ES2004a.A.words94',\n",
       "  \"Okay , I see it as one thing it's very supportive . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.23',\n",
       "  'ES2004a.A.words95',\n",
       "  'ES2004a.A.words98',\n",
       "  \"It's your best friend \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.24',\n",
       "  'ES2004a.A.words99',\n",
       "  'ES2004a.A.words108',\n",
       "  'and your you can talk to a dog , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.25',\n",
       "  'ES2004a.A.words109',\n",
       "  'ES2004a.A.words115',\n",
       "  'it can be your best friend , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.26',\n",
       "  'ES2004a.A.words116',\n",
       "  'ES2004a.A.words127',\n",
       "  \"it doesn't discriminate between you , based on what you are . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.27',\n",
       "  'ES2004a.A.words128',\n",
       "  'ES2004a.A.words130',\n",
       "  \"Second it's loyal \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.28',\n",
       "  'ES2004a.A.words131',\n",
       "  'ES2004a.A.words137',\n",
       "  \"and third thing it's got intuition . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.29',\n",
       "  'ES2004a.A.words138',\n",
       "  'ES2004a.A.words151',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.30',\n",
       "  'ES2004a.A.words152',\n",
       "  'ES2004a.A.words166',\n",
       "  'so basically these are the three unique features I think belong to a dog . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.31',\n",
       "  'ES2004a.A.words167',\n",
       "  'ES2004a.A.words169',\n",
       "  'Thank you . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.32',\n",
       "  'ES2004a.A.words170',\n",
       "  'ES2004a.A.words171',\n",
       "  'Okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.33',\n",
       "  'ES2004a.A.words172',\n",
       "  'ES2004a.A.words173',\n",
       "  'Sorry . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.34',\n",
       "  'ES2004a.A.words174',\n",
       "  'ES2004a.A.words182',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.35',\n",
       "  'ES2004a.A.words183',\n",
       "  'ES2004a.A.words184',\n",
       "  'Mm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.36',\n",
       "  'ES2004a.A.words185',\n",
       "  'ES2004a.A.words185',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.37',\n",
       "  'ES2004a.A.words186',\n",
       "  'ES2004a.A.words189',\n",
       "  'Eagle , okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.38',\n",
       "  'ES2004a.A.words190',\n",
       "  'ES2004a.A.words197',\n",
       "  'One point four or something like that . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.39',\n",
       "  'ES2004a.A.words198',\n",
       "  'ES2004a.A.words210',\n",
       "  'One point four Euro would make a Pound or something like that . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.40',\n",
       "  'ES2004a.A.words211',\n",
       "  'ES2004a.A.words212',\n",
       "  'Yeah . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.41',\n",
       "  'ES2004a.A.words213',\n",
       "  'ES2004a.A.words214',\n",
       "  'Okay , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.42',\n",
       "  'ES2004a.A.words215',\n",
       "  'ES2004a.A.words218',\n",
       "  'pretty huge margin . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.43',\n",
       "  'ES2004a.A.words219',\n",
       "  'ES2004a.A.words221',\n",
       "  \"So then Mm-hmm . Yeah , that c Okay , you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.44',\n",
       "  'ES2004a.A.words222',\n",
       "  'ES2004a.A.words223',\n",
       "  'Mm-hmm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.45',\n",
       "  'ES2004a.A.words224',\n",
       "  'ES2004a.A.words225',\n",
       "  'Yeah , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.46',\n",
       "  'ES2004a.A.words226',\n",
       "  'ES2004a.A.words227',\n",
       "  'that c ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.47',\n",
       "  'ES2004a.A.words228',\n",
       "  'ES2004a.A.words229',\n",
       "  'Okay , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.48',\n",
       "  'ES2004a.A.words230',\n",
       "  'ES2004a.A.words237',\n",
       "  \"you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.49',\n",
       "  'ES2004a.A.words238',\n",
       "  'ES2004a.A.words239',\n",
       "  'Okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.50',\n",
       "  'ES2004a.A.words240',\n",
       "  'ES2004a.A.words248',\n",
       "  'So simplification of symbols you could think of . ',\n",
       "  True),\n",
       " ('ES2004a.A.dialog-act.s9553330.52',\n",
       "  'ES2004a.A.words249',\n",
       "  'ES2004a.A.words250',\n",
       "  'Mm-hmm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.51',\n",
       "  'ES2004a.A.words251',\n",
       "  'ES2004a.A.words254',\n",
       "  'Menu , alright . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.53',\n",
       "  'ES2004a.A.words255',\n",
       "  'ES2004a.A.words257',\n",
       "  \"Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.54',\n",
       "  'ES2004a.A.words258',\n",
       "  'ES2004a.A.words259',\n",
       "  'Right , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.55',\n",
       "  'ES2004a.A.words260',\n",
       "  'ES2004a.A.words268',\n",
       "  'I was thinking on the same lines you , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.56',\n",
       "  'ES2004a.A.words269',\n",
       "  'ES2004a.A.words296',\n",
       "  'instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , ',\n",
       "  True),\n",
       " ('ES2004a.A.dialog-act.s9553330.57',\n",
       "  'ES2004a.A.words297',\n",
       "  'ES2004a.A.words305',\n",
       "  'like a mobile , yeah and with menus . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.58',\n",
       "  'ES2004a.A.words306',\n",
       "  'ES2004a.A.words331',\n",
       "  \"And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.59',\n",
       "  'ES2004a.A.words332',\n",
       "  'ES2004a.A.words340',\n",
       "  '',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.60',\n",
       "  'ES2004a.A.words341',\n",
       "  'ES2004a.A.words342',\n",
       "  'Right . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.61',\n",
       "  'ES2004a.A.words343',\n",
       "  'ES2004a.A.words344',\n",
       "  'Mm-hmm . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.62',\n",
       "  'ES2004a.A.words345',\n",
       "  'ES2004a.A.words346',\n",
       "  'Okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.65',\n",
       "  'ES2004a.A.words347',\n",
       "  'ES2004a.A.words350',\n",
       "  'Mm , okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.64',\n",
       "  'ES2004a.A.words351',\n",
       "  'ES2004a.A.words351',\n",
       "  'S ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.63',\n",
       "  'ES2004a.A.words352',\n",
       "  'ES2004a.A.words353',\n",
       "  'It might ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.66',\n",
       "  'ES2004a.A.words354',\n",
       "  'ES2004a.A.words362',\n",
       "  'it might save a b bit of space , ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.67',\n",
       "  'ES2004a.A.words363',\n",
       "  'ES2004a.A.words374',\n",
       "  \"it's i instead of looking bulky , it might look small . \",\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.68',\n",
       "  'ES2004a.A.words375',\n",
       "  'ES2004a.A.words382',\n",
       "  'But it might have its cost implications . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.69',\n",
       "  'ES2004a.A.words383',\n",
       "  'ES2004a.A.words384',\n",
       "  'Right . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.70',\n",
       "  'ES2004a.A.words385',\n",
       "  'ES2004a.A.words386',\n",
       "  'Okay . ',\n",
       "  False),\n",
       " ('ES2004a.A.dialog-act.s9553330.71',\n",
       "  'ES2004a.A.words387',\n",
       "  'ES2004a.A.words390',\n",
       "  'Mm , yeah . ',\n",
       "  False)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dialogues('ES2004a.A', \"AMI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractive Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the dictionary abstractive_summary[sentenceid]\n",
    "as_dict_ICSI = dict()\n",
    "for sum_f in sorted(glob.glob(ROOT_DIR_ICSI+\"/Contributions/Summarization/abstractive/*abssumm.xml\")):\n",
    "    m = sum_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]\n",
    "    root = et.parse(sum_f).getroot()\n",
    "    dlist = root.findall('abstract/sentence')\n",
    "    # getting the abstractive summary sentences only in the abstractive summary\n",
    "    # excluding question and progress part\n",
    "    for dl in dlist:\n",
    "        sentenceid = dl.attrib[\"{http://nite.sourceforge.net/}id\"]\n",
    "        if meeting_name not in as_dict_ICSI:\n",
    "            as_dict_ICSI[meeting_name]= OrderedDict()\n",
    "        as_dict_ICSI[meeting_name][sentenceid] = dl.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the dictionary abstractive_summary[sentenceid]\n",
    "as_dict_AMI = dict()\n",
    "for sum_f in sorted(glob.glob(ROOT_DIR_AMI+\"/abstractive/*abssumm.xml\")):\n",
    "    m = sum_f.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]\n",
    "    root = et.parse(sum_f).getroot()\n",
    "    dlist = root.findall('abstract/sentence')\n",
    "    # getting the abstractive summary sentences only in the abstractive summary\n",
    "    # excluding question and progress part\n",
    "    for dl in dlist:\n",
    "        sentenceid = dl.attrib[\"{http://nite.sourceforge.net/}id\"]\n",
    "        if meeting_name not in as_dict_AMI:\n",
    "            as_dict_AMI[meeting_name]= OrderedDict()\n",
    "        as_dict_AMI[meeting_name][sentenceid] = dl.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive Summary and Extractive Summary Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the relationship between extractive and abstractive \n",
    "# summary_links[extractive_sentence_id] = abstractive_sentence_id\n",
    "summary_links_ICSI = dict()\n",
    "for summary_link in sorted(glob.glob(ROOT_DIR_ICSI+\"/Contributions/Summarization/extractive/*summlink.xml\")):\n",
    "    m = summary_link.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]\n",
    "    root = et.parse(summary_link).getroot()\n",
    "    dlist = root.findall('summlink')\n",
    "    for dl in dlist:\n",
    "        #d = dl.findall('{http://nite.sourceforge.net/}pointer')\n",
    "        for d in dl: \n",
    "            if d.attrib['role'] == 'abstractive':\n",
    "                abstractive_sentence_id = d.attrib['href'].split(\"(\")[1].split(\")\")[0]\n",
    "                #abstractive_meeting_id = abstractive_sentence_id.split(\".\")[0]\n",
    "            elif d.attrib['role']=='extractive': \n",
    "                extractive_sentence_id = d.attrib['href'].split(\"(\")[1].split(\")\")[0]\n",
    "                #extractive_meeting_id = extractive_sentence_id.split(\".\")[0] + \".\" + extractive_sentence_id.split(\".\")[1]  \n",
    "        if meeting_name not in summary_links_ICSI:\n",
    "            summary_links_ICSI[meeting_name]= defaultdict(set)\n",
    "        summary_links_ICSI[meeting_name][extractive_sentence_id].add(abstractive_sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two main options were discussed as to the organisation of the collected data.\n"
     ]
    }
   ],
   "source": [
    "s1 = summary_links_ICSI['Bdb001']['Bdb001.F.dialogueact37']\n",
    "for i in s1:\n",
    "    print(as_dict_ICSI['Bdb001'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the relationship between extractive and abstractive \n",
    "# summary_links[extractive_sentence_id] = abstractive_sentence_id\n",
    "summary_links_AMI = dict()\n",
    "for summary_link in sorted(glob.glob(ROOT_DIR_AMI+\"/extractive/*summlink.xml\")):\n",
    "    m = summary_link.split(\"/\")[-1].split(\".\")\n",
    "    meeting_name = m[0]\n",
    "    root = et.parse(summary_link).getroot()\n",
    "    dlist = root.findall('summlink')\n",
    "    for dl in dlist:\n",
    "        #d = dl.findall('{http://nite.sourceforge.net/}pointer')\n",
    "        for d in dl: \n",
    "            if d.attrib['role'] == 'abstractive':\n",
    "                abstractive_sentence_id = d.attrib['href'].split(\"(\")[1].split(\")\")[0]\n",
    "                #abstractive_meeting_id = abstractive_sentence_id.split(\".\")[0]\n",
    "            elif d.attrib['role']=='extractive': \n",
    "                extractive_sentence_id = d.attrib['href'].split(\"(\")[1].split(\")\")[0]\n",
    "                #extractive_meeting_id = extractive_sentence_id.split(\".\")[0] + \".\" + extractive_sentence_id.split(\".\")[1]  \n",
    "        if meeting_name not in summary_links_AMI:\n",
    "            summary_links_AMI[meeting_name]= defaultdict(set)\n",
    "        summary_links_AMI[meeting_name][extractive_sentence_id].add(abstractive_sentence_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text, stopwords, remove_stopwords=True, pos_filtering=False, stemming=True, lower_case=True):\n",
    "#     if lower_case:\n",
    "#         # convert to lower case\n",
    "#         text = text.lower()\n",
    "#     # strip extra white space\n",
    "#     text = re.sub(' +', ' ', text)\n",
    "#     # strip leading and trailing white space\n",
    "#     text = text.strip()\n",
    "#     # tokenize (split based on whitespace)\n",
    "#     tokens = text.split(' ')\n",
    "\n",
    "#     # remove punctuation\n",
    "#     tokens = [t for t in tokens if t not in string.punctuation]\n",
    "\n",
    "#     if pos_filtering:\n",
    "#         tagger = PerceptronTagger()\n",
    "#         # apply POS-tagging\n",
    "#         tagged_tokens = tagger.tag(tokens)\n",
    "#         # retain only nouns and adjectives\n",
    "#         tokens = [item[0] for item in tagged_tokens if item[1] in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "#     if remove_stopwords:\n",
    "#         # remove stopwords\n",
    "#         tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "#     if stemming:\n",
    "#         # http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf\n",
    "#         stemmer = nltk.stem.PorterStemmer()\n",
    "#         # apply Porter's stemmer\n",
    "#         tokens_stemmed = list()\n",
    "#         for token in tokens:\n",
    "#             tokens_stemmed.append(stemmer.stem(token))\n",
    "#         tokens = tokens_stemmed\n",
    "\n",
    "#     return (tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(path):\n",
    "    stopwords = set([])\n",
    "\n",
    "    for line in codecs.open(path, 'r', 'utf-8'):\n",
    "        if not re.search('^#', line) and len(line.strip()) > 0:\n",
    "            stopwords.add(line.strip().lower())  # lowercase\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filler_words(path):\n",
    "    with open(path, 'r+') as f:\n",
    "        filler = f.read().splitlines()\n",
    "\n",
    "    return filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utterance(utterance, filler_words):\n",
    "    utt = utterance\n",
    "    # replace consecutive unigrams with a single instance\n",
    "    utt = re.sub('\\\\b(\\\\w+)\\\\s+\\\\1\\\\b', '\\\\1', utt)\n",
    "    # same for bigrams\n",
    "    utt = re.sub('(\\\\b.+?\\\\b)\\\\1\\\\b', '\\\\1', utt)\n",
    "    # strip extra white space\n",
    "    utt = re.sub(' +', ' ', utt)\n",
    "    # strip leading and trailing white space\n",
    "    utt = utt.strip()\n",
    "\n",
    "    # remove filler words # highly time-consuming\n",
    "    utt = ' ' + utt + ' '\n",
    "    for filler_word in filler_words:\n",
    "        utt = re.sub(' ' + filler_word + ' '+ '.'+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' '+ ','+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' '+ '?'+ ' ', ' ', utt)\n",
    "        utt = re.sub(' ' + filler_word + ' ', ' ', utt) \n",
    "        #utt = re.sub(' ' + filler_word.capitalize() + ' ', ' ', utt)\n",
    "\n",
    "    return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_ami_icsi(diag, filler_words):\n",
    "#     asr_output = pd.read_csv(\n",
    "#         path,\n",
    "#         sep='\\t',\n",
    "#         header=None,\n",
    "#         names=['ID', 'start', 'end', 'letter', 'role', 'A', 'B', 'C', 'utt']\n",
    "#     )\n",
    "    utt = diag\n",
    "    utterances = []\n",
    "#     for tmp in zip(asr_output['role'].tolist(), asr_output['utt'].tolist()):\n",
    "#         role, utt = tmp\n",
    "#         # lower case\n",
    "    utt = str(utt).lower()\n",
    "\n",
    "    # remove special tag\n",
    "    for ch in ['{nonvocalsound}','{vocalsound}', '{gap}', '{disfmarker}', '{comment}', '{pause}', '@reject@']:\n",
    "        utt = re.sub(ch, '', utt)\n",
    "\n",
    "    utt = re.sub(\"'Kay\", 'okay', utt)\n",
    "    utt = re.sub(\"'kay\", 'okay', utt)\n",
    "    utt = re.sub('\"Okay\"', 'okay', utt)\n",
    "    utt = re.sub(\"'cause\", 'cause', utt)\n",
    "    utt = re.sub(\"'Cause\", 'cause', utt)\n",
    "    utt = re.sub('\"cause\"', 'cause', utt)\n",
    "    utt = re.sub('\"\\'em\"', 'them', utt)\n",
    "    utt = re.sub('\"\\'til\"', 'until', utt)\n",
    "    utt = re.sub('\"\\'s\"', 's', utt)\n",
    "    utt = re.sub('\"\\\"', \"\" , utt)\n",
    "    utt = re.sub(\"-\", ' ', utt)\n",
    "    # l. c. d. -> lcd\n",
    "    # t. v. -> tv\n",
    "    utt = re.sub('h. t. m. l.', 'html', utt)\n",
    "    utt = re.sub(r\"(\\w)\\. (\\w)\\. (\\w)\\.\", r\"\\1\\2\\3\", utt)\n",
    "    utt = re.sub(r\"(\\w)\\. (\\w)\\.\", r\"\\1\\2\", utt)\n",
    "    utt = re.sub(r\"(\\w)\\.\", r\"\\1\", utt)\n",
    "\n",
    "    # clean_utterance, remove filler_words\n",
    "    utt = clean_utterance(utt, filler_words=filler_words)\n",
    "\n",
    "    # strip extra white space\n",
    "    utt = re.sub(' +', ' ', utt)\n",
    "    # strip leading and trailing white space\n",
    "    utt = utt.strip()\n",
    "\n",
    "    if not re.match(r'^[_\\W]+$', utt) and utt != '':\n",
    "    #if utt != '' and utt != '.' and utt != ' ' and utt!= \"?\" and utt!= \",\" and :\n",
    "        utterances.append(utt)\n",
    "    if len(utterances)>0:\n",
    "    # remove duplicate utterances per speaker\n",
    "        utterances = sorted(set(utterances), key=utterances.index)[0]\n",
    "        result = str(utterances)\n",
    "    else:\n",
    "        result = \"\"\n",
    "    #utterances_indexed = list(zip(range(len(utterances)), list(zip(*utterances))[0], list(zip(*utterances))[1]))\n",
    "    #list(zip(*utterances))[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_UTILS = \"/Users/haileywu/Desktop/W266_project/data/utils/\"\n",
    "filler_words = load_filler_words(ROOT_DIR_UTILS+\"filler_words.less.txt\")\n",
    "#stopwords = load_stopwords(ROOT_DIR_UTILS+\"stopwords.en.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comma(utterance_indexed):\n",
    "    #utterances_processed = []\n",
    "#    for utterance_indexed in rst:\n",
    "        # remove the comma at the beginning\n",
    "    if len(utterance_indexed) >0:\n",
    "        if utterance_indexed[0] == \",\" or utterance_indexed[0] == \".\" :\n",
    "            utt_cleaned = utterance_indexed[2:]  \n",
    "        else:\n",
    "            utt_cleaned = utterance_indexed\n",
    "        #print(utt_cleaned)\n",
    "        #utterances_processed.append(utt_cleaned)\n",
    "    else:\n",
    "        utt_cleaned = \"\"\n",
    "    return utt_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_words = 3\n",
    "# utterances_processed =[]\n",
    "# for utterance_indexed in list(rst):\n",
    "#     index,role,utt = utterance_indexed\n",
    "#     utt_cleaned = clean_text(\n",
    "#         utt,\n",
    "#         stopwords=stopwords,\n",
    "#         remove_stopwords=True,\n",
    "#         pos_filtering=False,\n",
    "#         stemming=False,\n",
    "#         # clustering based on lowercase form.\n",
    "#         lower_case=True\n",
    "#     )\n",
    "#     if len(utt_cleaned) >= min_words:\n",
    "#         utterances_processed.append((index, role, ' '.join(utt_cleaned)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Okay . Yep , yep . Okay . Tu tu tu tu Hi , good morning . 'Kay . Oops . Mm . Oh sorry . Mm-hmm . Yeah , me . Cat . Where did this come from ? Uh , yep . Thank you . Uh , maybe you can guess what I'm trying to make ? Yep . It's actually sitting , so it's sitting , it's not standing . Okay , I see it as one thing it's very supportive . It's your best friend and your you can talk to a dog , it can be your best friend , it doesn't discriminate between you , based on what you are . Second it's loyal and third thing it's got intuition . dogs can som sometimes can make out between a thief and a person so basically these are the three unique features I think belong to a dog . Thank you . Okay . Sorry . Does it look like a dog actually ? Mm . Eagle , okay . One point four or something like that . One point four Euro would make a Pound or something like that . Yeah . Okay , pretty huge margin . So then Mm-hmm . Yeah , that c Okay , you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay . Yep , yep . Okay . Tu tu tu tu Hi , good morning . 'Kay . Oops . Mm . Oh sorry . Mm-hmm . Yeah , me . Cat . Where did this come from ? Uh , yep . Thank you . Uh , maybe you can guess what I'm trying to make ? Yep . It's actually sitting , so it's sitting , it's not standing . Okay , I see it as one thing it's very supportive . It's your best friend and your you can talk to a dog , it can be your best friend , it doesn't discriminate between you , based on what you are . Second it's loyal and third thing it's got intuition . dogs can som sometimes can make out between a thief and a person so basically these are the three unique features I think belong to a dog . Thank you . Okay . Sorry . Does it look like a dog actually ? Mm . Eagle , okay . One point four or something like that . One point four Euro would make a Pound or something like that . Yeah . Okay , pretty huge margin . So then Mm-hmm . Yeah , that c Okay , you wanna integrate everything into one like Okay . So simplification of symbols you could think of . Mm-hmm . Menu , alright . Uh uh Right , I was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h maybe have an L_C_D_ di display or something like that , like a mobile , yeah and with menus . And if it's s somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also maybe . You mean to save it lesser number . Right . Mm-hmm . Okay . Mm , okay . S It might it might save a b bit of space , it's i instead of looking bulky , it might look small . But it might have its cost implications . Right . Okay . Mm , yeah .  \n"
     ]
    }
   ],
   "source": [
    "print(\" \"+test+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tu hi , good morning . me . cat . where did this come from ? you can guess what i'm trying to make ? it's actually sitting , it's sitting , it's not standing . i see it as one thing it's very supportive . it's your best friend and your you can talk to a dog , it can be your best friend , it doesn't discriminate between you , based on what you are . second it's loyal and third thing it's got intuition . dogs can som sometimes can make out between a thief and a person these are the three unique features belong to a dog . does it look like a dog actually ? eagle , one point four like that . one point four euro would make a pound like that . pretty huge margin . then that c you wanna integrate everything into one like simplification of symbols you could think of . menu , alright . right , i was thinking on the same lines you , instead of having too many b buttons and make it complicated for the user , may h have an l_c_d_ di display like that , like a mobile , and with menus . and if it's somewhat similar to what you have on mobile phone , people might find it easier to browse and navigate also to save it lesser number . right . s it might save a b bit of space , it's i instead of looking bulky , it might look small . but it might have its cost implications . right .\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = read_ami_icsi(test, filler_words)\n",
    "clean_comma(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Documents\n",
    "\n",
    "The saving will also require a file directory as:\n",
    "\n",
    "* `data/ICSI_plus_NXT/Full_doc/Dev`, \n",
    "* `data/ICSI_plus_NXT/Full_doc/Train`, \n",
    "* `data/ICSI_plus_NXT/Full_doc/Test`,\n",
    "* `data/AMI_manual/Full_doc/Dev`,\n",
    "* `data/AMI_manual/Full_doc/Test`,\n",
    "* `data/AMI_manual/Full_doc/Train`\n",
    "\n",
    "\n",
    "The dataset is split on the speaker (turn) level, meaning for each hour of the meeting (e.g.ES2004a), the saving will have ES2004a.A, ES2004a.B, etc. The abstractive summary is on the meeting level (e.g. ES2004a)\n",
    "\n",
    "If you would like to have another format, e.g. two columns in csv file, use the dictionaries created previously to build your output.\n",
    "\n",
    "Dictionary List:\n",
    "\n",
    "* `meeting_dialogues_ICSI[meeting_name][dialog_id] = [start, end, T/F]`\n",
    "* `meeting_dialogues_AMI[meeting_name][dialog_id] = [start, end, T/F]`\n",
    "\n",
    "* `as_dict_ICSI[meeting_name][sentenceid] = abstractive_summary_sentence`\n",
    "* `as_dict_AMI[meeting_name][sentenceid] = abstractive_summary_sentence`\n",
    "\n",
    "* `summary_links_ICSI[meeting_name][extractive_sentence_id] = abstractive_sentence_id`\n",
    "* `summary_links_AMI[meeting_name][extractive_sentence_id] = abstractive_sentence_id`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save less stop words, common words \n",
    "\n",
    "same format as the next chunk, but with cleaning of the data itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/data_cleansing/ICSI_1024_test_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in icsi_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "                # cleaning words \n",
    "                #print(diag)\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                #print(diag)\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/data_cleansing/ICSI_1024_dev_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in icsi_development_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/data_cleansing/ICSI_512_train_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid not in icsi_development_set and meetingid not in icsi_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                # if the count of original text words are more than 512, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 512:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ICSI_plus_NXT/data_cleansing/ICSI_512_train_cleaned.csv\")\n",
    "#print(df.meeting[0])\n",
    "#print(df.original[0])\n",
    "#print(df.abstractive[0])\n",
    "#print(df.extractive[0])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/data_cleansing/AMI_512_test_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in ami_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 512:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/data_cleansing/AMI_512_dev_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in ami_development_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                # if the count of original text words are more than 512, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 512:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/data_cleansing/AMI_512_train_cleaned.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid not in ami_development_set and meetingid not in ami_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "                diag = read_ami_icsi(diag, filler_words)\n",
    "                diag = clean_comma(diag)\n",
    "                # if the count of original text words are more than 1024, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 512:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Original, Extractive, Abstractive in X tokens\n",
    "\n",
    "* *ICSI_512_train.csv*\n",
    "* *ICSI_512_dev.csv*\n",
    "* *ICSI_512_test.csv*\n",
    "\n",
    "* *AMI_512_train.csv*\n",
    "* *AMI_512_dev.csv*\n",
    "* *AMI_512_test.csv*\n",
    "\n",
    "* *ICSI_1024_train.csv*\n",
    "* *ICSI_1024_dev.csv*\n",
    "* *ICSI_1024_test.csv*\n",
    "\n",
    "* *AMI_1024_train.csv*\n",
    "* *AMI_1024_dev.csv*\n",
    "* *AMI_1024_test.csv*\n",
    "\n",
    "Columns: 'meeting','original','extractive','abstractive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/T5_csv/ICSI_1024_test.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in icsi_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bed004.A\n",
      "It is not a working net yet, but identifying clusters of features that define the output mode provides a visual aid for further work. \n",
      "So , what I did for this this is uh , a pedagogical belief - net  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ICSI_plus_NXT/T5_csv/ICSI_512_test.csv\")\n",
    "print(df.meeting[0])\n",
    "#print(df.original[0])\n",
    "print(df.abstractive[0])\n",
    "print(df.extractive[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Bro027.B.dialogueact18': {'Bro027.s.14'},\n",
       "             'Bro027.B.dialogueact25': {'Bro027.s.14'},\n",
       "             'Bro027.B.dialogueact26': {'Bro027.s.14'},\n",
       "             'Bro027.B.dialogueact39': {'Bro027.s.4'},\n",
       "             'Bro027.B.dialogueact40': {'Bro027.s.4'},\n",
       "             'Bro027.B.dialogueact42': {'Bro027.s.4'},\n",
       "             'Bro027.B.dialogueact95': {'Bro027.s.4'},\n",
       "             'Bro027.B.dialogueact96': {'Bro027.s.4'},\n",
       "             'Bro027.B.dialogueact185': {'Bro027.s.4'},\n",
       "             'Bro027.A.dialogueact243': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact245': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact247': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact248': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact251': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact252': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact253': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact256': {'Bro027.s.15'},\n",
       "             'Bro027.A.dialogueact257': {'Bro027.s.15'},\n",
       "             'Bro027.B.dialogueact280': {'Bro027.s.6', 'Bro027.s.8'},\n",
       "             'Bro027.A.dialogueact283': {'Bro027.s.6'},\n",
       "             'Bro027.A.dialogueact288': {'Bro027.s.6'},\n",
       "             'Bro027.B.dialogueact292': {'Bro027.s.6', 'Bro027.s.8'},\n",
       "             'Bro027.D.dialogueact300': {'Bro027.s.8'},\n",
       "             'Bro027.D.dialogueact303': {'Bro027.s.8'},\n",
       "             'Bro027.B.dialogueact333': {'Bro027.s.8'},\n",
       "             'Bro027.B.dialogueact338': {'Bro027.s.8'},\n",
       "             'Bro027.C.dialogueact417': {'Bro027.s.12', 'Bro027.s.2'},\n",
       "             'Bro027.C.dialogueact418': {'Bro027.s.12', 'Bro027.s.2'},\n",
       "             'Bro027.C.dialogueact419': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact423': {'Bro027.s.12'},\n",
       "             'Bro027.D.dialogueact428': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact460': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact465': {'Bro027.s.12'},\n",
       "             'Bro027.B.dialogueact466': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact472': {'Bro027.s.9'},\n",
       "             'Bro027.C.dialogueact476': {'Bro027.s.9'},\n",
       "             'Bro027.C.dialogueact603': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact605': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact608': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact615': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact618': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact623': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact641': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact643': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact644': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact647': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact648': {'Bro027.s.9'},\n",
       "             'Bro027.A.dialogueact649': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact652': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact657': {'Bro027.s.9'},\n",
       "             'Bro027.B.dialogueact711': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact713': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact720': {'Bro027.s.12'},\n",
       "             'Bro027.B.dialogueact721': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact724': {'Bro027.s.12'},\n",
       "             'Bro027.C.dialogueact823': {'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact825': {'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact828': {'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact831': {'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact857': {'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact858': {'Bro027.s.13'},\n",
       "             'Bro027.B.dialogueact866': {'Bro027.s.10', 'Bro027.s.13'},\n",
       "             'Bro027.C.dialogueact873': {'Bro027.s.10'},\n",
       "             'Bro027.B.dialogueact875': {'Bro027.s.10'},\n",
       "             'Bro027.B.dialogueact877': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact883': {'Bro027.s.10'},\n",
       "             'Bro027.A.dialogueact884': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact900': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact906': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact907': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact911': {'Bro027.s.10'},\n",
       "             'Bro027.B.dialogueact949': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact952': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact953': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact962': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact966': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact971': {'Bro027.s.3', 'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact973': {'Bro027.s.3', 'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1042': {'Bro027.s.10'},\n",
       "             'Bro027.B.dialogueact1043': {'Bro027.s.10'},\n",
       "             'Bro027.B.dialogueact1049': {'Bro027.s.10'},\n",
       "             'Bro027.C.dialogueact1227': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1228': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1230': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1231': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1234': {'Bro027.s.11'},\n",
       "             'Bro027.A.dialogueact1236': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1238': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1245': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1248': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1249': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1303': {'Bro027.s.7'},\n",
       "             'Bro027.C.dialogueact1307': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1312': {'Bro027.s.7'},\n",
       "             'Bro027.C.dialogueact1315': {'Bro027.s.7'},\n",
       "             'Bro027.C.dialogueact1321': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1323': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1326': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1327': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1330': {'Bro027.s.7'},\n",
       "             'Bro027.B.dialogueact1332': {'Bro027.s.7'},\n",
       "             'Bro027.A.dialogueact1370': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact1371': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1376': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1377': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1380': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1387': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1388': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1391': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact1396': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact1404': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact1412': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact1420': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact1424': {'Bro027.s.3'},\n",
       "             'Bro027.C.dialogueact1427': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1678': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1679': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1685': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1687': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1691': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1693': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1706': {'Bro027.s.3'},\n",
       "             'Bro027.A.dialogueact1720': {'Bro027.s.3'},\n",
       "             'Bro027.D.dialogueact1722': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact1728': {'Bro027.s.3'},\n",
       "             'Bro027.B.dialogueact1783': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1784': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1787': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1788': {'Bro027.s.11'},\n",
       "             'Bro027.C.dialogueact1791': {'Bro027.s.11'},\n",
       "             'Bro027.B.dialogueact1960': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1963': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1975': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1987': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1988': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1989': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact1991': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact2003': {'Bro027.s.5'},\n",
       "             'Bro027.B.dialogueact2010': {'Bro027.s.5'}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_links_ICSI['Bro027']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The group discussed possible further investigations that arose from these areas, including better linking the two.\n"
     ]
    }
   ],
   "source": [
    "print(as_dict_ICSI['Bro027']['Bro027.s.3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/T5_csv/ICSI_1024_dev.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in icsi_development_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/ICSI_plus_NXT/T5_csv/ICSI_1024_train.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_ICSI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid not in icsi_development_set and meetingid not in icsi_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_ICSI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"ICSI\"):\n",
    "\n",
    "                # if the count of original text words are more than 512, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_ICSI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ICSI_plus_NXT/T5_csv/ICSI_1024_train.csv\")\n",
    "#print(df.meeting[0])\n",
    "#print(df.original[0])\n",
    "#print(df.abstractive[0])\n",
    "#print(df.extractive[0])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1149\n",
       "1      1120\n",
       "2       301\n",
       "3        63\n",
       "4      1133\n",
       "       ... \n",
       "692    1156\n",
       "693      28\n",
       "694    1135\n",
       "695    1154\n",
       "696    1037\n",
       "Name: original, Length: 697, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['original'].notna()]['original'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      244\n",
       "1      150\n",
       "4      305\n",
       "5      501\n",
       "6      366\n",
       "      ... \n",
       "691    171\n",
       "692    146\n",
       "694    203\n",
       "695    178\n",
       "696    210\n",
       "Name: extractive, Length: 364, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['extractive'].notna()]['extractive'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       55\n",
       "5      125\n",
       "6       56\n",
       "7       37\n",
       "11      39\n",
       "      ... \n",
       "690     16\n",
       "692     42\n",
       "694     16\n",
       "695     17\n",
       "696     24\n",
       "Name: abstractive, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['abstractive'].notna()]['abstractive'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/T5_csv/AMI_1024_test.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in ami_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "\n",
    "                # if the count of original text words are more than x, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/T5_csv/AMI_1024_dev.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid in ami_development_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "\n",
    "                # if the count of original text words are more than 512, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "with open(\"../data/AMI_manual/T5_csv/AMI_1024_train.csv\", mode='w') as csv_file:\n",
    "    # ctext is the orginal text, while text is the extractive summary\n",
    "    fieldnames = ['meeting','original','extractive','abstractive']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for meeting in meeting_dialogues_AMI.keys():\n",
    "        meetingid = meeting.split(\".\")[0] \n",
    "        #Bdb001\n",
    "        if meetingid not in ami_development_set and meetingid not in ami_test_set:\n",
    "            original_text = \"\"\n",
    "            extractive_summary = \"\"\n",
    "            abstractive_summary = \"\"\n",
    "            # make sure each abstractive sentence appears only once\n",
    "            abstractive_existing = set()\n",
    "            links = summary_links_AMI.get(meetingid,{})\n",
    "            this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "            paragraph_count = 0\n",
    "            for diag_id, _, _, diag, summ_flag in extract_dialogues(meeting, \"AMI\"):\n",
    "\n",
    "                # if the count of original text words are more than 1024, then write the row and move to the next\n",
    "                diag_word_count = len(diag.split())\n",
    "                if diag_word_count + paragraph_count > 1024:\n",
    "                    writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})\n",
    "                    original_text = \"\"\n",
    "                    extractive_summary = \"\"\n",
    "                    abstractive_summary = \"\"\n",
    "                    abstractive_existing = set()  # uniqueness of abstractive sentences per row\n",
    "                    paragraph_count = 0\n",
    "\n",
    "                # add extractive summary\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "\n",
    "                # add abstractive summary\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    for ai in abstractive_id:\n",
    "                        if ai not in abstractive_existing and ai in this_as_dict.keys():\n",
    "                            abstractive_summary += as_dict_AMI[meetingid][ai] + \" \"\n",
    "                            abstractive_existing.add(ai)\n",
    "\n",
    "                # add the original text\n",
    "                original_text+=diag + \" \"\n",
    "                # increase the original text word counts\n",
    "                paragraph_count += diag_word_count\n",
    "\n",
    "            # write to the row when one meeting ends \n",
    "            writer.writerow({'meeting':meeting,'original': original_text, 'extractive': extractive_summary, 'abstractive': abstractive_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES2002a.B\n",
      "6755\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/AMI_manual/T5_csv/AMI_1024_train.csv\")\n",
    "print(df.meeting[4])\n",
    "#print(df.original[4])\n",
    "#print(df.abstractive[4])\n",
    "#print(df.extractive[4])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        925\n",
       "1         66\n",
       "2       1474\n",
       "3        179\n",
       "4       1288\n",
       "        ... \n",
       "6750     997\n",
       "6751     887\n",
       "6752     822\n",
       "6753     970\n",
       "6754     591\n",
       "Name: original, Length: 6748, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['original'].notna()]['original'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        197\n",
       "1          2\n",
       "2       1474\n",
       "3         56\n",
       "11        13\n",
       "        ... \n",
       "6727      34\n",
       "6730      18\n",
       "6732       7\n",
       "6738      12\n",
       "6743       5\n",
       "Name: extractive, Length: 2832, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['extractive'].notna()]['extractive'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       48\n",
       "1       36\n",
       "2       36\n",
       "3       36\n",
       "11      36\n",
       "        ..\n",
       "6726    15\n",
       "6727    46\n",
       "6730    32\n",
       "6732    32\n",
       "6743    32\n",
       "Name: abstractive, Length: 2446, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['abstractive'].notna()]['abstractive'].apply(lambda x: x.split(\" \")).map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Original Dialogue\n",
    "\n",
    "*meeting_da.txt*\n",
    "\n",
    "e.g. ES2004a.B_da.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting in meeting_dialogues_ICSI.keys():\n",
    "    meetingid = meeting.split(\".\")[0]    \n",
    "    original_text = \"\"\n",
    "    if meetingid in icsi_test_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Test/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"ICSI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in icsi_development_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Dev/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"ICSI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Train/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"ICSI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting in meeting_dialogues_AMI.keys():\n",
    "    meetingid = meeting.split(\".\")[0]        \n",
    "    original_text = \"\"\n",
    "    if meetingid in ami_test_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Test/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"AMI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in ami_development_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Dev/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"AMI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Train/\"+meeting+\"_da.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,_ in extract_dialogues(meeting,\"AMI\"):\n",
    "                original_text+=diag + \" \"\n",
    "            f.write(original_text)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extractive Summary\n",
    "\n",
    "*meeting_es.txt*\n",
    "\n",
    "e.g. ES2004a.A_es.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting in meeting_dialogues_ICSI.keys():\n",
    "    meetingid = meeting.split(\".\")[0]   \n",
    "    extractive_summary = \"\"\n",
    "    \n",
    "    if meetingid in icsi_test_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Test/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in icsi_development_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Dev/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Train/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting in meeting_dialogues_AMI.keys():\n",
    "    meetingid = meeting.split(\".\")[0]   \n",
    "    extractive_summary = \"\"\n",
    "    \n",
    "    if meetingid in ami_test_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Test/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in ami_development_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Dev/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Train/\"+meeting+\"_es.txt\", mode='w') as f:\n",
    "            for _, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "            f.write(extractive_summary)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Abstractive Summary\n",
    "\n",
    "*meeting_as.txt*\n",
    "\n",
    "e.g. Bed002_as.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting, sentencedict in as_dict_ICSI.items():\n",
    "    meetingid = meeting\n",
    "    abstractive_summary = \"\"\n",
    "    \n",
    "    if meetingid in icsi_test_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Test/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in icsi_development_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Dev/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Train/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "for meeting, sentencedict in as_dict_AMI.items():\n",
    "    meetingid = meeting\n",
    "    abstractive_summary = \"\"\n",
    "    \n",
    "    if meetingid in ami_test_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Test/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    elif meetingid in ami_development_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Dev/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Train/\"+meeting+\"_as.txt\", mode='w') as f:\n",
    "            for s,v in sentencedict.items():\n",
    "                abstractive_summary+= v + \" \"\n",
    "            f.write(abstractive_summary)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Summary and Abstractive Summary with Links\n",
    "\n",
    "*meeting_esas.csv*\n",
    "\n",
    "e.g.Bed002.A_esas.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "fieldnames = ['meeting','abstractive', 'extractive']\n",
    "\n",
    "for meeting in meeting_dialogues_ICSI.keys():\n",
    "    meetingid = meeting.split(\".\")[0] \n",
    "    #Bdb001\n",
    "    extractive_summary = \"\"\n",
    "    abstractive_summary = \"\"\n",
    "    # make sure each abstractive sentence appears only once\n",
    "    abstractive_existing = set()\n",
    "    links = summary_links_ICSI.get(meetingid,{})\n",
    "    this_as_dict = as_dict_ICSI.get(meetingid,{})\n",
    "    \n",
    "    if meetingid in icsi_test_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Test/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_ICSI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            \n",
    "    elif meetingid in icsi_development_set:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Dev/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_ICSI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            \n",
    "    else:\n",
    "        with open(\"../data/ICSI_plus_NXT/Full_doc/Train/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"ICSI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_ICSI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all meetings and write out dialogues to files\n",
    "fieldnames = ['meeting','abstractive', 'extractive']\n",
    "\n",
    "for meeting in meeting_dialogues_AMI.keys():\n",
    "    meetingid = meeting.split(\".\")[0] \n",
    "    #Bdb001\n",
    "    extractive_summary = \"\"\n",
    "    abstractive_summary = \"\"\n",
    "    # make sure each abstractive sentence appears only once\n",
    "    abstractive_existing = set()\n",
    "    links = summary_links_AMI.get(meetingid,{})\n",
    "    this_as_dict = as_dict_AMI.get(meetingid,{})\n",
    "    \n",
    "    if meetingid in ami_test_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Test/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_AMI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            \n",
    "    elif meetingid in ami_development_set:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Dev/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_AMI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            \n",
    "    else:\n",
    "        with open(\"../data/AMI_manual/Full_doc/Train/\"+meeting+\"_esas.csv\", mode='w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for diag_id, _, _, diag,summ_flag in extract_dialogues(meeting,\"AMI\"):\n",
    "                if summ_flag == 1: \n",
    "                    extractive_summary+= diag + \" \"\n",
    "                if diag_id in links.keys():\n",
    "                    abstractive_id=links[diag_id]\n",
    "                    if abstractive_id not in abstractive_existing and abstractive_id in this_as_dict.keys():\n",
    "                        abstractive_summary += as_dict_AMI[meetingid][abstractive_id] + \" \"\n",
    "                        abstractive_existing.add(abstractive_id)\n",
    "            writer.writerow({'meeting':meeting,'abstractive': abstractive_summary, 'extractive': extractive_summary})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
