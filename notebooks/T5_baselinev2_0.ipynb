{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5_baselinev2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64bab992734544669ecba126eb40cf93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_717ef7e4446243e28309fed1721906b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f023dd790f554785b52ae56f43c26694",
              "IPY_MODEL_4bb508694c2444d4bcc9769705fadb58"
            ]
          }
        },
        "717ef7e4446243e28309fed1721906b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f023dd790f554785b52ae56f43c26694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_27c0f280b89543adbff0c6eb46af5c64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.45MB of 0.45MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_352671278b9f4bddaa0e2eeffcfe0d07"
          }
        },
        "4bb508694c2444d4bcc9769705fadb58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8ee00aa8806449688f0abb91dce9b86",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85b4424e63054ade9b1b82d53c275ff6"
          }
        },
        "27c0f280b89543adbff0c6eb46af5c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "352671278b9f4bddaa0e2eeffcfe0d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8ee00aa8806449688f0abb91dce9b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85b4424e63054ade9b1b82d53c275ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JMpA3Pi4rr9"
      },
      "source": [
        "# T5 Baseline\n",
        "\n",
        "The initial exploration will use T5-small as the pre-training model along with ICSI dataset. When the model is ready, we will expand the dataset and also validation set for other hyperparameter tuning.\n",
        "\n",
        "1. Library Loading  \n",
        "2. Dataset Loading\n",
        "3.   Dataset Transformation\n",
        "4.   Training and Test Splitting\n",
        "5.   Fine Tuning\n",
        "6.   Checkpoint saving\n",
        "7.   Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAat9uX5rJn"
      },
      "source": [
        "## Library Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BODRVvgf5RGZ",
        "outputId": "82f6050b-3b1b-47c6-8270-990a8a4d1749"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "#!pip install datasets\n",
        "!pip install nlp\n",
        "#!pip install rouge_score\n",
        "!pip install rouge\n",
        "#!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.1.4)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqwv6NyQ_Wjl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import time\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "# T5ForConditionalGeneration is specific for sequence-to-sequence\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "#from nlp import load_metric\n",
        "import nlp\n",
        "from rouge import Rouge\n",
        "\n",
        "import wandb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8XNh-h8_g0_",
        "outputId": "038c1d37-cf3b-4ece-9f92-247fad0140e4"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov 22 00:23:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    25W / 300W |     10MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtYB7VvI_jm4"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcXUpiwbe8Cs",
        "outputId": "5c0daf33-d03d-40ea-861f-960eed9ae0e0"
      },
      "source": [
        "!wandb login\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwuqq09\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqq1uu8hA49j"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "Loaded from GDrive the transformed dataset.\n",
        "\n",
        "Considering the dataset has only 499 points, we will only split with  80% training dataset and 20% validation dataset.\n",
        "\n",
        "This portion is using the dataset from extractive summary to abstractive summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGXLrMD7GC4O"
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_size = 0.8"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvq382j4A3MC",
        "outputId": "e6db4e01-ae49-4b10-9b55-cf60d026dc94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#/content/drive/My Drive/W266/data/ICSI_extrac_abstrac_512token.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DysKSkQoBpfE",
        "outputId": "c0ce91eb-5407-4e97-ba71-cb8b6fab840b"
      },
      "source": [
        "#df = pd.read_csv('/content/drive/My Drive/W266/512_tokens/ICSI_extrac_abstrac_512token.csv',encoding='latin-1')\n",
        "#df = df[df['extractive'].notna()][['abstractive','extractive']]\n",
        "train_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_train.csv',encoding='latin-1')\n",
        "dev_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_dev.csv',encoding='latin-1')\n",
        "test_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/AMI_1024_test.csv',encoding='latin-1')\n",
        "\n",
        "#train_dataset = train_dataset[train_dataset.abstractive.notna()]\n",
        "#dev_dataset = dev_dataset[dev_dataset.abstractive.notna()]\n",
        "#test_dataset = test_dataset[test_dataset.abstractive.notna()]\n",
        "\n",
        "train_dataset = train_dataset.dropna(subset=['abstractive'])\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "dev_dataset = dev_dataset.dropna(subset=['abstractive'])\n",
        "dev_dataset = dev_dataset.reset_index(drop=True)\n",
        "\n",
        "test_dataset = test_dataset.dropna(subset=['abstractive'])\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "# use the pre-defined \"summarize\" for abstractive summary\n",
        "train_dataset.original = 'summarize: ' + train_dataset.original\n",
        "dev_dataset.original = 'summarize: ' + dev_dataset.original\n",
        "test_dataset.original = 'summarize: ' + test_dataset.original\n",
        "print(train_dataset.head(1))\n",
        "print(len(train_dataset))\n",
        "print(len(dev_dataset))\n",
        "print(len(test_dataset))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     meeting  ...                                        abstractive\n",
            "0  ES2002a.A  ...  The project manager introduced the upcoming pr...\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "2446\n",
            "2210\n",
            "820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENujMiWE70j",
        "outputId": "164d4787-768a-4610-f969-59e3cb08ce27"
      },
      "source": [
        "#train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
        "#test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "#train_dataset = train_dataset.reset_index(drop=True)\n",
        "#print(\"FULL Dataset: {}\".format(df.shape))\n",
        "\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"DEV Dataset: {}\".format(dev_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (2446, 4)\n",
            "DEV Dataset: (2210, 4)\n",
            "TEST Dataset: (820, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4EXP96GZ8n"
      },
      "source": [
        "## Dataset Transformation\n",
        "\n",
        "Tokenize the input and also perform the attention masking to make sure everything can be done in tensors. \n",
        "\n",
        "Tunable Hyprparam:\n",
        "\n",
        "*   MAX_LEN\n",
        "*   SUMMARY_LEN\n",
        "* TRAIN_BATCH_SIZE\n",
        "* DEV_BATCH_SIZE\n",
        "* TEST_BATCH_SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07SniX-oGTcj"
      },
      "source": [
        "# most code from https://colab.research.google.com/drive/1ypT7oCjtBOTSMJv7J5_1vO7hDYSD_-oU?authuser=2#scrollTo=932p8NhxeNw4\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.abstractive = self.data.abstractive\n",
        "        self.original = self.data.original\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstractive)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        original = str(self.original[index])\n",
        "        original = ' '.join(original.split())\n",
        "\n",
        "        abstractive = str(self.abstractive[index])\n",
        "        abstractive = ' '.join(abstractive.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([original], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([abstractive], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOB45_8K6kK"
      },
      "source": [
        "### Training Dataset and Test Dataset \n",
        "\n",
        "# TRAIN Dataset: (1231, 4)\n",
        "# DEV Dataset: (744, 4)\n",
        "# TEST Dataset: (165, 4)\n",
        "\n",
        "MAX_LEN = 1024\n",
        "SUMMARY_LEN= 150\n",
        "\n",
        "# note here only uses the t5-small model.\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "dev_set = CustomDataset(dev_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws6q9x_TMgtP",
        "outputId": "c479958c-5f84-4f0a-c66d-3dccbd6639ab"
      },
      "source": [
        "# double checking the result size, only for one point\n",
        "# https://stackoverflow.com/questions/43627405/understanding-getitem-method\n",
        "print(train_set[0]['source_ids'].shape)\n",
        "print(train_set[0]['source_mask'].shape)\n",
        "print(train_set[0]['target_ids'].shape)\n",
        "print(train_set[0]['target_ids_y'].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024])\n",
            "torch.Size([1024])\n",
            "torch.Size([150])\n",
            "torch.Size([150])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx2e48uKnY3"
      },
      "source": [
        "## Fine Tuning\n",
        "\n",
        "Here we directly use the pre-trained model t5-small and will save checkpoint every 500 steps. \n",
        "\n",
        "Tunable Parameter:\n",
        "* T5ForConditionalGeneration or T5\n",
        "* epoch - train, dev, test\n",
        "* optimizer - LEARNING_RATE, Adam\n",
        "* output: num_beams, length_penalty,early_stopping\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9MmjbqbYjHB"
      },
      "source": [
        "### Training & Validation Functions\n",
        "\n",
        "The training part uses the t5-small pretrained model, didn't make any change to the model layer structures, and fine tune the parameters based on the dataset we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05VKilCKKptn"
      },
      "source": [
        "losslist = []\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "  # put into train mode \n",
        "  model.train()\n",
        "  # enumerate the dataloader for training set into the defined network\n",
        "  for _,data in enumerate(loader, 0):\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      # https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
        "      y_ids = y[:, :-1].contiguous()\n",
        "      lm_labels = y[:, 1:].clone().detach()\n",
        "      lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "      loss = outputs[0]\n",
        "      losslist.append(loss)\n",
        "      if _%10==0:\n",
        "        wandb.log({\"Training Loss\": loss.item()})\n",
        "      if _%500==0:\n",
        "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "      \n",
        "      # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "      # https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VNL1ru0Wvsc"
      },
      "source": [
        "# https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81\n",
        "\n",
        "def dev(epoch, tokenizer, model, device, loader):\n",
        "  #https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  #rouge_metric = load_metric('rouge') \n",
        "  # https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _, data in enumerate(loader, 0):\n",
        "\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask, \n",
        "          max_length=150, \n",
        "          num_beams=12,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "      if _%100==0:\n",
        "          print(f'Completed {_}')\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "      #print(preds)\n",
        "      #print(target)\n",
        "      #rouge_metric.add(preds, target)\n",
        "      \n",
        "    #rouge_results = rouge_metric.compute(rouge_types=[\"rouge2\"]) \n",
        "  return predictions, actuals"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJddO-eY3whv"
      },
      "source": [
        "def compute_rouge_scores(cand_list, ref_list):\n",
        "    \"\"\"\n",
        "    :param cand_list: list of candidate summaries\n",
        "    :param ref_list: list of reference summaries\n",
        "    :return: rouge scores\n",
        "    \"\"\"\n",
        "    rouge = Rouge()\n",
        "    rouge_1_f_score = 0.\n",
        "    rouge_2_f_score = 0.\n",
        "    rouge_L_f_score = 0.\n",
        "\n",
        "    rouge_1_r_score = 0.\n",
        "    rouge_2_r_score = 0.\n",
        "    rouge_L_r_score = 0.\n",
        "\n",
        "    rouge_1_p_score = 0.\n",
        "    rouge_2_p_score = 0.\n",
        "    rouge_L_p_score = 0.\n",
        "\n",
        "    doc_count = len(cand_list)\n",
        "\n",
        "    for cand, ref in zip(cand_list, ref_list):\n",
        "        rouge_scores = rouge.get_scores(cand, ref)[0]\n",
        "        rouge_1_f_score += rouge_scores['rouge-1']['f']\n",
        "        rouge_2_f_score += rouge_scores['rouge-2']['f']\n",
        "        rouge_L_f_score += rouge_scores['rouge-l']['f']\n",
        "\n",
        "        rouge_1_r_score += rouge_scores['rouge-1']['r']\n",
        "        rouge_2_r_score += rouge_scores['rouge-2']['r']\n",
        "        rouge_L_r_score += rouge_scores['rouge-l']['r']\n",
        "\n",
        "        rouge_1_p_score += rouge_scores['rouge-1']['p']\n",
        "        rouge_2_p_score += rouge_scores['rouge-2']['p']\n",
        "        rouge_L_p_score += rouge_scores['rouge-l']['p']\n",
        "    rouge_1_f_score = rouge_1_f_score / doc_count\n",
        "    rouge_2_f_score = rouge_2_f_score / doc_count\n",
        "    rouge_L_f_score = rouge_L_f_score / doc_count\n",
        "\n",
        "    results_dict = {}\n",
        "    results_dict['rouge_1_f_score'] = rouge_1_f_score\n",
        "    results_dict['rouge_2_f_score'] = rouge_2_f_score\n",
        "    results_dict['rouge_l_f_score'] = rouge_L_f_score\n",
        "\n",
        "    return results_dict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RLc9Acp-JE4",
        "outputId": "b635240c-055f-4ad5-cec2-ba774e4d19c8"
      },
      "source": [
        "cand_list=['this is a test']\n",
        "ref_list = ['this is an exam']\n",
        "rouge_Result = compute_rouge_scores(cand_list, ref_list)\n",
        "print(rouge_Result.get(\"rouge_1_f_score\"))\n",
        "rouge_Result"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4999999950000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge_1_f_score': 0.4999999950000001,\n",
              " 'rouge_2_f_score': 0.3333333283333334,\n",
              " 'rouge_l_f_score': 0.4999999950000001}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxyXnBnZYex-"
      },
      "source": [
        "### Run Epoch\n",
        "Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y5TwfR5FBIki",
        "outputId": "3d4926ef-09d4-4f18-a727-25ab542bdf5f"
      },
      "source": [
        "id = wandb.util.generate_id()\n",
        "id\n",
        "#dwlkfpg3"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'q4xbid2u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487,
          "referenced_widgets": [
            "64bab992734544669ecba126eb40cf93",
            "717ef7e4446243e28309fed1721906b2",
            "f023dd790f554785b52ae56f43c26694",
            "4bb508694c2444d4bcc9769705fadb58",
            "27c0f280b89543adbff0c6eb46af5c64",
            "352671278b9f4bddaa0e2eeffcfe0d07",
            "c8ee00aa8806449688f0abb91dce9b86",
            "85b4424e63054ade9b1b82d53c275ff6"
          ]
        },
        "id": "i8gLtGWbh7AZ",
        "outputId": "3713e197-ff6a-41f0-e1f6-61233357fc33"
      },
      "source": [
        "#run = wandb.init(project=\"T5_1024_MSFT_AMI_01\",resume=True)\n",
        "run = wandb.init(project=\"T5_1024_MSFT_AMI_01\", id=\"dwlkfpg3\", resume=\"allow\")\n",
        "\n",
        "config = wandb.config          # Initialize config\n",
        "config.TRAIN_BATCH_SIZE = 1    # input batch size for training (default: 64)\n",
        "config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n",
        "config.EPOCHS = 13        # number of epochs to train (default: 10)\n",
        "config.LEARNING_RATE = 0.0005   # learning rate (default: 0.01)\n",
        "config.SEED = 42               # random seed (default: 42)\n",
        "config.MAX_LEN = MAX_LEN\n",
        "config.SUMMARY_LEN = SUMMARY_LEN \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:dwlkfpg3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 3881<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64bab992734544669ecba126eb40cf93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20201122_002502-dwlkfpg3/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20201122_002502-dwlkfpg3/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_timestamp</td><td>1606004360</td></tr><tr><td>_step</td><td>3301</td></tr><tr><td>Training Loss</td><td>0.00727</td></tr><tr><td>_runtime</td><td>5016</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>epoch_traingTime</td><td>4070.82048</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">clear-snowflake-9</strong>: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01/runs/dwlkfpg3\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01/runs/dwlkfpg3</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:dwlkfpg3). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Resuming run <strong style=\"color:#cdcd00\">clear-snowflake-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01/runs/dwlkfpg3\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_AMI_01/runs/dwlkfpg3</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201122_002550-dwlkfpg3</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5vvXTv0OQLK"
      },
      "source": [
        "# https://deeplizard.com/learn/video/kWVgvsejXsE#:~:text=The%20num_workers%20attribute%20tells%20the,sequentially%20inside%20the%20main%20process\n",
        "# num_workers to default 0\n",
        "# This means that the training process will work sequentially inside the main process. \n",
        "# After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
        "\n",
        "TEST_BATCH_SIZE = 1 \n",
        "\n",
        "train_params = {\n",
        "  'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "  'shuffle': True,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "dev_params = {\n",
        "  'batch_size': config.VALID_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "test_params = {\n",
        "  'batch_size': TEST_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "training_loader = DataLoader(train_set, **train_params)\n",
        "dev_loader = DataLoader(dev_set, **dev_params)\n",
        "test_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKxdX61LW6dY"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model = model.to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePGtVuaUXqli"
      },
      "source": [
        "# optimizer \n",
        "# https://pytorch.org/docs/stable/optim.html\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmes1VwYbjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5179573-c0ae-477e-a93d-62832a2f35da"
      },
      "source": [
        "training_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_AMI\"\n",
        "start_train_time = time.time()\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Training =================== ##\n",
        "  print(\"start training epoch\" + str(i))\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_TEMP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/\"+ CP_TEMP_NAME +\".pt\"\n",
        "  train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "  torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_epoch': i\n",
        "      }, CP_TEMP_PATH)\n",
        "  training_time = time.time() - start_train_time\n",
        "  print(\"done training epoch\" +str(i))\n",
        "  wandb.log({'epoch_traingTime': training_time,\n",
        "             'epoch': i})\n",
        "  print(\"--- %s seconds ---\" % (training_time))\n",
        "  training_time_log.append(training_time)\n",
        "  i+=1\n",
        "  ## ================= Validation =================== ##\n",
        "  # print(\"strat validation epoch\" + str(i))\n",
        "  # predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  # final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "  #                           'Golden_Abstractive_Text':actuals})\n",
        "  # final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "  # print(\"done validation epoch\" +str(i))\n",
        "\n",
        "  # rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "  #                                      final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  # wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "  #            'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "  #            'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "  #            'epoch': i})\n",
        "  # i+=1\n",
        "\n",
        "#run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "start training epoch0\n",
            "Epoch: 0, Loss:  8.74458122253418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1156: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  4.156149387359619\n",
            "Epoch: 0, Loss:  2.087413787841797\n",
            "Epoch: 0, Loss:  2.609992265701294\n",
            "Epoch: 0, Loss:  2.9554169178009033\n",
            "done training epoch0\n",
            "--- 300.78909492492676 seconds ---\n",
            "start training epoch1\n",
            "Epoch: 1, Loss:  1.8641914129257202\n",
            "Epoch: 1, Loss:  2.58231520652771\n",
            "Epoch: 1, Loss:  1.7515785694122314\n",
            "Epoch: 1, Loss:  1.5301200151443481\n",
            "Epoch: 1, Loss:  1.799355387687683\n",
            "done training epoch1\n",
            "--- 607.7493896484375 seconds ---\n",
            "start training epoch2\n",
            "Epoch: 2, Loss:  0.45313042402267456\n",
            "Epoch: 2, Loss:  0.9140322208404541\n",
            "Epoch: 2, Loss:  0.29861918091773987\n",
            "Epoch: 2, Loss:  0.13438451290130615\n",
            "Epoch: 2, Loss:  1.1081722974777222\n",
            "done training epoch2\n",
            "--- 925.1556663513184 seconds ---\n",
            "start training epoch3\n",
            "Epoch: 3, Loss:  0.1490953266620636\n",
            "Epoch: 3, Loss:  0.38843467831611633\n",
            "Epoch: 3, Loss:  0.47003796696662903\n",
            "Epoch: 3, Loss:  0.09176482260227203\n",
            "Epoch: 3, Loss:  0.3680625557899475\n",
            "done training epoch3\n",
            "--- 1232.0787823200226 seconds ---\n",
            "start training epoch4\n",
            "Epoch: 4, Loss:  0.5169832110404968\n",
            "Epoch: 4, Loss:  0.04357776418328285\n",
            "Epoch: 4, Loss:  0.3531770408153534\n",
            "Epoch: 4, Loss:  1.014976978302002\n",
            "Epoch: 4, Loss:  0.18891960382461548\n",
            "done training epoch4\n",
            "--- 1552.6006665229797 seconds ---\n",
            "start training epoch5\n",
            "Epoch: 5, Loss:  0.30887821316719055\n",
            "Epoch: 5, Loss:  0.6815358400344849\n",
            "Epoch: 5, Loss:  1.2517077922821045\n",
            "Epoch: 5, Loss:  0.023243967443704605\n",
            "Epoch: 5, Loss:  0.022968243807554245\n",
            "done training epoch5\n",
            "--- 1869.625030040741 seconds ---\n",
            "start training epoch6\n",
            "Epoch: 6, Loss:  0.017722483724355698\n",
            "Epoch: 6, Loss:  0.019758014008402824\n",
            "Epoch: 6, Loss:  0.10364340990781784\n",
            "Epoch: 6, Loss:  0.49299657344818115\n",
            "Epoch: 6, Loss:  0.023444324731826782\n",
            "done training epoch6\n",
            "--- 2180.1797716617584 seconds ---\n",
            "start training epoch7\n",
            "Epoch: 7, Loss:  0.6183202266693115\n",
            "Epoch: 7, Loss:  0.0847165510058403\n",
            "Epoch: 7, Loss:  0.7595252990722656\n",
            "Epoch: 7, Loss:  0.06577695906162262\n",
            "Epoch: 7, Loss:  0.2971963584423065\n",
            "done training epoch7\n",
            "--- 2491.4107661247253 seconds ---\n",
            "start training epoch8\n",
            "Epoch: 8, Loss:  0.18343748152256012\n",
            "Epoch: 8, Loss:  0.011612926609814167\n",
            "Epoch: 8, Loss:  0.5945958495140076\n",
            "Epoch: 8, Loss:  0.4185439944267273\n",
            "Epoch: 8, Loss:  0.03651871159672737\n",
            "done training epoch8\n",
            "--- 2809.9692873954773 seconds ---\n",
            "start training epoch9\n",
            "Epoch: 9, Loss:  0.03672126308083534\n",
            "Epoch: 9, Loss:  0.0053909579291939735\n",
            "Epoch: 9, Loss:  0.05291697010397911\n",
            "Epoch: 9, Loss:  0.2830554246902466\n",
            "Epoch: 9, Loss:  0.34354275465011597\n",
            "done training epoch9\n",
            "--- 3125.3534598350525 seconds ---\n",
            "start training epoch10\n",
            "Epoch: 10, Loss:  0.14491401612758636\n",
            "Epoch: 10, Loss:  0.027715519070625305\n",
            "Epoch: 10, Loss:  0.030489182099699974\n",
            "Epoch: 10, Loss:  0.2276953011751175\n",
            "Epoch: 10, Loss:  0.011683013290166855\n",
            "done training epoch10\n",
            "--- 3434.7852940559387 seconds ---\n",
            "start training epoch11\n",
            "Epoch: 11, Loss:  0.036221813410520554\n",
            "Epoch: 11, Loss:  0.03656814247369766\n",
            "Epoch: 11, Loss:  0.03680846840143204\n",
            "Epoch: 11, Loss:  0.027323534712195396\n",
            "Epoch: 11, Loss:  0.5054065585136414\n",
            "done training epoch11\n",
            "--- 3750.559936285019 seconds ---\n",
            "start training epoch12\n",
            "Epoch: 12, Loss:  0.10204961895942688\n",
            "Epoch: 12, Loss:  0.08155760169029236\n",
            "Epoch: 12, Loss:  0.07857639342546463\n",
            "Epoch: 12, Loss:  0.01423585880547762\n",
            "Epoch: 12, Loss:  0.34945207834243774\n",
            "done training epoch12\n",
            "--- 4070.820483446121 seconds ---\n",
            "start training epoch13\n",
            "Epoch: 13, Loss:  0.23562714457511902\n",
            "Epoch: 13, Loss:  0.008467748761177063\n",
            "Epoch: 13, Loss:  0.2719566524028778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tVOuLscBjxq",
        "outputId": "656c6adc-157f-4ef0-9fd5-4c3938ef0eb9"
      },
      "source": [
        "validation_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_AMI\"\n",
        "start_validation_time = time.time()\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Validation =================== ##\n",
        "  print(\"strat validation epoch\" + str(i))\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "  model = model.to(device)\n",
        "  # optimizer \n",
        "  # https://pytorch.org/docs/stable/optim.html\n",
        "  optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/\" + CP_TEMP_NAME +\".pt\"\n",
        "  checkpoint = torch.load(CP_PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "  predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "                            'Golden_Abstractive_Text':actuals})\n",
        "  final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "  print(\"done validation epoch\" +str(i))\n",
        "\n",
        "  rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "                                       final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  validation_time = time.time() - start_validation_time\n",
        "  wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "             'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "             'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "             'epoch_validationTime': validation_time,\n",
        "             'epoch': i})\n",
        "  validation_time_log.append(validation_time)\n",
        "  i+=1\n",
        "\n",
        "run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "strat validation epoch0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Completed 500\n",
            "Completed 600\n",
            "Completed 700\n",
            "Completed 800\n",
            "Completed 900\n",
            "Completed 1000\n",
            "Completed 1100\n",
            "Completed 1200\n",
            "Completed 1300\n",
            "Completed 1400\n",
            "Completed 1500\n",
            "Completed 1600\n",
            "Completed 1700\n",
            "Completed 1800\n",
            "Completed 1900\n",
            "Completed 2000\n",
            "Completed 2100\n",
            "Completed 2200\n",
            "done validation epoch0\n",
            "strat validation epoch1\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Completed 500\n",
            "Completed 600\n",
            "Completed 700\n",
            "Completed 800\n",
            "Completed 900\n",
            "Completed 1000\n",
            "Completed 1100\n",
            "Completed 1200\n",
            "Completed 1300\n",
            "Completed 1400\n",
            "Completed 1500\n",
            "Completed 1600\n",
            "Completed 1700\n",
            "Completed 1800\n",
            "Completed 1900\n",
            "Completed 2000\n",
            "Completed 2100\n",
            "Completed 2200\n",
            "done validation epoch1\n",
            "strat validation epoch2\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Completed 500\n",
            "Completed 600\n",
            "Completed 700\n",
            "Completed 800\n",
            "Completed 900\n",
            "Completed 1000\n",
            "Completed 1100\n",
            "Completed 1200\n",
            "Completed 1300\n",
            "Completed 1400\n",
            "Completed 1500\n",
            "Completed 1600\n",
            "Completed 1700\n",
            "Completed 1800\n",
            "Completed 1900\n",
            "Completed 2000\n",
            "Completed 2100\n",
            "Completed 2200\n",
            "done validation epoch2\n",
            "strat validation epoch3\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Completed 300\n",
            "Completed 400\n",
            "Completed 500\n",
            "Completed 600\n",
            "Completed 700\n",
            "Completed 800\n",
            "Completed 900\n",
            "Completed 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiml8KGSeSGB"
      },
      "source": [
        "#### Checkpoint \n",
        "\n",
        "Remember to change the CP_NAME to a new model pt name.\n",
        "\n",
        "The model is then saved as checkpoints to Google Drive with the related tunable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVgDGjccbZQY"
      },
      "source": [
        "# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
        "# Checkpoint Saving\n",
        "CP_NAME = MODEL_NAME\n",
        "\n",
        "CP_TRAIN_EPOCHS = TRAIN_EPOCHS\n",
        "CP_DEV_EPOCHS = DEV_EPOCHS\n",
        "CP_LEARNING_RATE = LEARNING_RATE\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/\"+ CP_NAME +\".pt\"\n",
        "CP_MAX_LEN = MAX_LEN\n",
        "CP_SUMMARY_LEN = SUMMARY_LEN\n",
        "CP_TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
        "CP_DEV_BATCH_SIZE = DEV_BATCH_SIZE\n",
        "CP_MODEL = 'T5ForConditionalGeneration,t5-small'\n",
        "CP_OPTIMIZER_OPTION = 'Adam'\n",
        "CP_LOSSLIST = losslist\n",
        "CP_TEST_OPTIONS = {\n",
        "    \"num_beams\":          12,\n",
        "    \"repetition_penalty\": 2.5, \n",
        "    \"length_penalty\":     1.0, \n",
        "    \"early_stopping\":     True\n",
        "}\n",
        "CT_TRAIN_TIME = training_time\n",
        "#CT_EVALUATE_TIME = evaluating_time\n",
        "\n",
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_epoch': CP_TRAIN_EPOCHS,\n",
        "            'dev_epoch': CP_DEV_EPOCHS,\n",
        "            'learning_rate': CP_LEARNING_RATE,\n",
        "            'max_source_length':CP_MAX_LEN,\n",
        "            'max_target_length':CP_SUMMARY_LEN,\n",
        "            'train_batch_size':CP_TRAIN_BATCH_SIZE,\n",
        "            'dev_batch_size':CP_DEV_BATCH_SIZE,\n",
        "            'model_option':CP_MODEL,\n",
        "            'optimizer_option':CP_OPTIMIZER_OPTION,\n",
        "            'losslist': CP_LOSSLIST,\n",
        "            'training_time': CT_TRAIN_TIME,\n",
        "            #'evaluating_time': CT_EVALUATE_TIME,\n",
        "            'test_option': CP_TEST_OPTIONS\n",
        "            }, CP_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-EORf2EbxV3",
        "outputId": "7db9540d-f6a2-41a4-fd93-417f63e01b10"
      },
      "source": [
        "MODEL_NAME = \"epoch61\"\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/\" + MODEL_NAME +\".pt\"\n",
        "print(CP_PATH)\n",
        "checkpoint = torch.load(CP_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# train_epoch = checkpoint['train_epoch']\n",
        "# dev_epoch = checkpoint['dev_epoch']\n",
        "# losslist = checkpoint['losslist']\n",
        "# learning_rate = checkpoint['learning_rate']\n",
        "# max_source_length = checkpoint['max_source_length']\n",
        "# max_target_length = checkpoint['max_target_length']\n",
        "# train_batch_size = checkpoint['train_batch_size']\n",
        "# dev_batch_size = checkpoint['dev_batch_size']\n",
        "# optimizer_option = checkpoint['optimizer_option']\n",
        "# test_option = checkpoint['test_option']\n",
        "# training_time = checkpoint['training_time']\n",
        "\n",
        "\n",
        "# evaluating_time = checkpoint['evaluating_time']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/epoch61.pt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}