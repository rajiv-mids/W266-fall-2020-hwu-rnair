{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "T5_baselinev2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3754ae93acf34703a9e6181470e5bfe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_437b3dd852d64002a845521f0e0c05ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8fa7dd800c0b4a54aa30e678c1a169ec",
              "IPY_MODEL_bc13303ab3884b38af22cee892fb363b"
            ]
          }
        },
        "437b3dd852d64002a845521f0e0c05ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fa7dd800c0b4a54aa30e678c1a169ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_472bcc0f7d5e4f44b722eda1985959a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23.13MB of 23.13MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c071dbadc93740ef9b43e171ae9e3e5a"
          }
        },
        "bc13303ab3884b38af22cee892fb363b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3b0344f9ce84674a47e9335da2a9977",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2147e54c40244d04a1eeadcbc4540b01"
          }
        },
        "472bcc0f7d5e4f44b722eda1985959a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c071dbadc93740ef9b43e171ae9e3e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3b0344f9ce84674a47e9335da2a9977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2147e54c40244d04a1eeadcbc4540b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "961afc58a76f4e09a608cfd2c9b3ad48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a509624c90114a37b2d1dc8548ee2de8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4601da506b8c4535a1bbd50f2a5d9276",
              "IPY_MODEL_15f1cc56d5414f7cadf0b9e1152f9d7a"
            ]
          }
        },
        "a509624c90114a37b2d1dc8548ee2de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4601da506b8c4535a1bbd50f2a5d9276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23bcb7d20c874b93a2f43926f5062a57",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58234155adf6421a866f80c30792f96f"
          }
        },
        "15f1cc56d5414f7cadf0b9e1152f9d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_64e946fefa3046d5b9ccaf8e74c3af0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:03&lt;00:00, 333B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d25868c379fe429ba5c9d8b972545a5d"
          }
        },
        "23bcb7d20c874b93a2f43926f5062a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58234155adf6421a866f80c30792f96f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64e946fefa3046d5b9ccaf8e74c3af0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d25868c379fe429ba5c9d8b972545a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "928005046f2b4252962f3a7f1ca1b85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59628bd2a2c140f48076c0bb94e25cf3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c45562d249184bfe8bc769ea85944570",
              "IPY_MODEL_4d6ae993c3094dff960d18fd097e2153"
            ]
          }
        },
        "59628bd2a2c140f48076c0bb94e25cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c45562d249184bfe8bc769ea85944570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2f36e345f5d45bbb22185dd14521861",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fea38bc12f7f4ca9b396549573539257"
          }
        },
        "4d6ae993c3094dff960d18fd097e2153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b138aead78f6413fb1189771b56d8732",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:03&lt;00:00, 74.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ffcc1567fe44879af3aaa7d990ec079"
          }
        },
        "d2f36e345f5d45bbb22185dd14521861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fea38bc12f7f4ca9b396549573539257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b138aead78f6413fb1189771b56d8732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ffcc1567fe44879af3aaa7d990ec079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JMpA3Pi4rr9"
      },
      "source": [
        "# T5 Baseline\n",
        "\n",
        "The initial exploration will use T5-small as the pre-training model along with ICSI dataset. When the model is ready, we will expand the dataset and also validation set for other hyperparameter tuning.\n",
        "\n",
        "1. Library Loading  \n",
        "2. Dataset Loading\n",
        "3.   Dataset Transformation\n",
        "4.   Training and Test Splitting\n",
        "5.   Fine Tuning\n",
        "6.   Checkpoint saving\n",
        "7.   Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAat9uX5rJn"
      },
      "source": [
        "## Library Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BODRVvgf5RGZ",
        "outputId": "5d4cab8c-a2a0-4f5a-8776-bf18cf622857"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "#!pip install datasets\n",
        "!pip install nlp\n",
        "#!pip install rouge_score\n",
        "!pip install rouge\n",
        "#!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 31.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 64.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 28.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 34.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 9.2MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 377kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 71.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.11.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, nlp\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed nlp-0.4.0 pyarrow-2.0.0 xxhash-2.0.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqwv6NyQ_Wjl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import time\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "# T5ForConditionalGeneration is specific for sequence-to-sequence\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "#from nlp import load_metric\n",
        "import nlp\n",
        "from rouge import Rouge\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8XNh-h8_g0_",
        "outputId": "8db32203-7c29-40f1-a4b1-461c1188de3c"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec  3 13:18:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtYB7VvI_jm4"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcXUpiwbe8Cs"
      },
      "source": [
        "#!wandb login\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqq1uu8hA49j"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "Loaded from GDrive the transformed dataset.\n",
        "\n",
        "This portion is using the dataset from extractive summary to abstractive summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGXLrMD7GC4O"
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_size = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvq382j4A3MC",
        "outputId": "af8d421b-10d1-47d3-8ff9-a431d0937717"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#/content/drive/My Drive/W266/data/ICSI_extrac_abstrac_512token.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DysKSkQoBpfE",
        "outputId": "7fefb3fb-8d55-4d3c-997d-1f7084d6d723"
      },
      "source": [
        "#df = pd.read_csv('/content/drive/My Drive/W266/512_tokens/ICSI_extrac_abstrac_512token.csv',encoding='latin-1')\n",
        "#df = df[df['extractive'].notna()][['abstractive','extractive']]\n",
        "train_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/ICSI_1024_train.csv',encoding='latin-1')\n",
        "dev_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/ICSI_1024_dev.csv',encoding='latin-1')\n",
        "test_dataset = pd.read_csv('/content/drive/My Drive/W266/data/1024_tokens/ICSI_1024_test.csv',encoding='latin-1')\n",
        "\n",
        "#train_dataset = train_dataset[train_dataset.abstractive.notna()]\n",
        "#dev_dataset = dev_dataset[dev_dataset.abstractive.notna()]\n",
        "#test_dataset = test_dataset[test_dataset.abstractive.notna()]\n",
        "\n",
        "train_dataset = train_dataset.dropna(subset=['abstractive'])\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "dev_dataset = dev_dataset.dropna(subset=['abstractive'])\n",
        "dev_dataset = dev_dataset.reset_index(drop=True)\n",
        "\n",
        "test_dataset = test_dataset.dropna(subset=['abstractive'])\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "# use the pre-defined \"summarize\" for abstractive summary\n",
        "train_dataset.original = 'summarize: ' + train_dataset.original\n",
        "dev_dataset.original = 'summarize: ' + dev_dataset.original\n",
        "test_dataset.original = 'summarize: ' + test_dataset.original\n",
        "print(train_dataset.head(1))\n",
        "print(len(train_dataset))\n",
        "print(len(dev_dataset))\n",
        "print(len(test_dataset))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    meeting  ...                                        abstractive\n",
            "0  Bdb001.C  ...  On the one hand, a bespoke XML structure that ...\n",
            "\n",
            "[1 rows x 4 columns]\n",
            "222\n",
            "213\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "2Jv3VHyUcENB",
        "outputId": "19c04a92-67cd-4f59-e86c-ad82dd20e5ff"
      },
      "source": [
        "train_dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meeting</th>\n",
              "      <th>original</th>\n",
              "      <th>extractive</th>\n",
              "      <th>abstractive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bdb001.C</td>\n",
              "      <td>summarize: Yeah , we had a long discussion abo...</td>\n",
              "      <td>I mean , we I sort of already have developed a...</td>\n",
              "      <td>On the one hand, a bespoke XML structure that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bdb001.C</td>\n",
              "      <td>summarize: You 're gonna actually run out of s...</td>\n",
              "      <td>Because you have a two - gigabyte limit on mos...</td>\n",
              "      <td>Phone-level analysis can be included in the sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bdb001.C</td>\n",
              "      <td>summarize: Um , th what would would would what...</td>\n",
              "      <td>Um , th what would would would what would worr...</td>\n",
              "      <td>Its advantages are that it is easier to read, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bdb001.C</td>\n",
              "      <td>summarize: But that 's the advantage of ATLAS ...</td>\n",
              "      <td>I guess I 'm just a little hesitant to try to ...</td>\n",
              "      <td>XML standards offer libraries that can be used...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bdb001.F</td>\n",
              "      <td>summarize: Oh , that 's good .  Cuz we have a ...</td>\n",
              "      <td>and the main thing that I was gonna ask people...</td>\n",
              "      <td>Two main options were discussed as to the orga...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    meeting  ...                                        abstractive\n",
              "0  Bdb001.C  ...  On the one hand, a bespoke XML structure that ...\n",
              "1  Bdb001.C  ...  Phone-level analysis can be included in the sa...\n",
              "2  Bdb001.C  ...  Its advantages are that it is easier to read, ...\n",
              "3  Bdb001.C  ...  XML standards offer libraries that can be used...\n",
              "4  Bdb001.F  ...  Two main options were discussed as to the orga...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENujMiWE70j",
        "outputId": "9f9358b9-b8db-4746-b96d-754060f49145"
      },
      "source": [
        "#train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
        "#test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "#train_dataset = train_dataset.reset_index(drop=True)\n",
        "#print(\"FULL Dataset: {}\".format(df.shape))\n",
        "\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"DEV Dataset: {}\".format(dev_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (222, 4)\n",
            "DEV Dataset: (213, 4)\n",
            "TEST Dataset: (40, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4EXP96GZ8n"
      },
      "source": [
        "## Dataset Transformation\n",
        "\n",
        "Tokenize the input and also perform the attention masking to make sure everything can be done in tensors. \n",
        "\n",
        "Tunable Hyprparam:\n",
        "\n",
        "*   MAX_LEN\n",
        "*   SUMMARY_LEN\n",
        "* TRAIN_BATCH_SIZE\n",
        "* DEV_BATCH_SIZE\n",
        "* TEST_BATCH_SIZE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07SniX-oGTcj"
      },
      "source": [
        "# most code from https://colab.research.google.com/drive/1ypT7oCjtBOTSMJv7J5_1vO7hDYSD_-oU?authuser=2#scrollTo=932p8NhxeNw4\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.abstractive = self.data.abstractive\n",
        "        self.original = self.data.original\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstractive)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        original = str(self.original[index])\n",
        "        original = ' '.join(original.split())\n",
        "\n",
        "        abstractive = str(self.abstractive[index])\n",
        "        abstractive = ' '.join(abstractive.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([original], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([abstractive], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOB45_8K6kK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "00839e28-f354-4b81-a9b4-ff7e55cfaa7d"
      },
      "source": [
        "### Training Dataset and Test Dataset \n",
        "\n",
        "# TRAIN Dataset: (1231, 4)\n",
        "# DEV Dataset: (744, 4)\n",
        "# TEST Dataset: (165, 4)\n",
        "\n",
        "MAX_LEN = 1024\n",
        "SUMMARY_LEN= 150\n",
        "\n",
        "# note here only uses the t5-small model.\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "dev_set = CustomDataset(dev_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ce9a47cbaa51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# note here only uses the t5-small model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5-small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSUMMARY_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdev_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSUMMARY_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/utils/dummy_sentencepiece_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mrequires_sentencepiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_sentencepiece\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCEPIECE_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws6q9x_TMgtP",
        "outputId": "b3c6ac15-8c58-4ca5-faf6-057bd1b396bb"
      },
      "source": [
        "# double checking the result size, only for one point\n",
        "# https://stackoverflow.com/questions/43627405/understanding-getitem-method\n",
        "print(train_set[0]['source_ids'].shape)\n",
        "print(train_set[0]['source_mask'].shape)\n",
        "print(train_set[0]['target_ids'].shape)\n",
        "print(train_set[0]['target_ids_y'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024])\n",
            "torch.Size([1024])\n",
            "torch.Size([150])\n",
            "torch.Size([150])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx2e48uKnY3"
      },
      "source": [
        "## Fine Tuning\n",
        "\n",
        "Here we directly use the pre-trained model t5-small and will save checkpoint every 500 steps. \n",
        "\n",
        "Tunable Parameter:\n",
        "* T5ForConditionalGeneration or T5\n",
        "* epoch - train, dev, test\n",
        "* optimizer - LEARNING_RATE, Adam\n",
        "* output: num_beams, length_penalty,early_stopping\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9MmjbqbYjHB"
      },
      "source": [
        "### Training & Validation Functions\n",
        "\n",
        "The training part uses the t5-small pretrained model, didn't make any change to the model layer structures, and fine tune the parameters based on the dataset we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05VKilCKKptn"
      },
      "source": [
        "losslist = []\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "  # put into train mode \n",
        "  model.train()\n",
        "  # enumerate the dataloader for training set into the defined network\n",
        "  for _,data in enumerate(loader, 0):\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      # https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
        "      y_ids = y[:, :-1].contiguous()\n",
        "      lm_labels = y[:, 1:].clone().detach()\n",
        "      lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "      loss = outputs[0]\n",
        "      losslist.append(loss)\n",
        "      if _%10==0:\n",
        "        wandb.log({\"Training Loss\": loss.item()})\n",
        "      if _%500==0:\n",
        "        print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "      \n",
        "      # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "      # https://discuss.pytorch.org/t/how-are-optimizer-step-and-loss-backward-related/7350\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VNL1ru0Wvsc"
      },
      "source": [
        "# https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81\n",
        "\n",
        "def dev(epoch, tokenizer, model, device, loader):\n",
        "  #https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  #rouge_metric = load_metric('rouge') \n",
        "  # https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _, data in enumerate(loader, 0):\n",
        "\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask, \n",
        "          max_length=150, \n",
        "          num_beams=9,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "      if _%100==0:\n",
        "          print(f'Completed {_}')\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "      #print(preds)\n",
        "      #print(target)\n",
        "      #rouge_metric.add(preds, target)\n",
        "      \n",
        "    #rouge_results = rouge_metric.compute(rouge_types=[\"rouge2\"]) \n",
        "  return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJddO-eY3whv"
      },
      "source": [
        "def compute_rouge_scores(cand_list, ref_list):\n",
        "    \"\"\"\n",
        "    :param cand_list: list of candidate summaries\n",
        "    :param ref_list: list of reference summaries\n",
        "    :return: rouge scores\n",
        "    \"\"\"\n",
        "    rouge = Rouge()\n",
        "    rouge_1_f_score = 0.\n",
        "    rouge_2_f_score = 0.\n",
        "    rouge_L_f_score = 0.\n",
        "\n",
        "    rouge_1_r_score = 0.\n",
        "    rouge_2_r_score = 0.\n",
        "    rouge_L_r_score = 0.\n",
        "\n",
        "    rouge_1_p_score = 0.\n",
        "    rouge_2_p_score = 0.\n",
        "    rouge_L_p_score = 0.\n",
        "\n",
        "    doc_count = len(cand_list)\n",
        "\n",
        "    for cand, ref in zip(cand_list, ref_list):\n",
        "        rouge_scores = rouge.get_scores(cand, ref)[0]\n",
        "        rouge_1_f_score += rouge_scores['rouge-1']['f']\n",
        "        rouge_2_f_score += rouge_scores['rouge-2']['f']\n",
        "        rouge_L_f_score += rouge_scores['rouge-l']['f']\n",
        "\n",
        "        rouge_1_r_score += rouge_scores['rouge-1']['r']\n",
        "        rouge_2_r_score += rouge_scores['rouge-2']['r']\n",
        "        rouge_L_r_score += rouge_scores['rouge-l']['r']\n",
        "\n",
        "        rouge_1_p_score += rouge_scores['rouge-1']['p']\n",
        "        rouge_2_p_score += rouge_scores['rouge-2']['p']\n",
        "        rouge_L_p_score += rouge_scores['rouge-l']['p']\n",
        "    rouge_1_f_score = rouge_1_f_score / doc_count\n",
        "    rouge_2_f_score = rouge_2_f_score / doc_count\n",
        "    rouge_L_f_score = rouge_L_f_score / doc_count\n",
        "\n",
        "    results_dict = {}\n",
        "    results_dict['rouge_1_f_score'] = rouge_1_f_score\n",
        "    results_dict['rouge_2_f_score'] = rouge_2_f_score\n",
        "    results_dict['rouge_l_f_score'] = rouge_L_f_score\n",
        "\n",
        "    return results_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4beVpzOMCoFu"
      },
      "source": [
        "AMI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_AMI_as_dev.csv\"\n",
        "ICSI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_ICSI_as_dev.csv\"\n",
        "\n",
        "amigold = pd.read_csv(AMI_PATH)\n",
        "icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "def rouge_per_document(final_df,gold):\n",
        "  merged_df = pd.concat([dev_dataset.meeting, final_df.Generated_Abstractive_Summary], axis=1)\n",
        "  merged_df[\"meetinglevel\"] = merged_df.meeting.apply(lambda x: x.split(\".\")[0]) \n",
        "\n",
        "  gas_list =[]\n",
        "  meeting_list = []\n",
        "  generated_abstractive = \"\"\n",
        "  for me in set(merged_df.meetinglevel):\n",
        "    for gas in merged_df[merged_df.meetinglevel == me]['Generated_Abstractive_Summary']:\n",
        "      generated_abstractive+= gas + \" \"\n",
        "    gas_list.append(generated_abstractive)\n",
        "    meeting_list.append(me)\n",
        "    generated_abstractive = \" \"\n",
        "  per_doc_summary = pd.DataFrame(\n",
        "    {'Meeting': meeting_list,\n",
        "     'Generated_Abstractive_Summary': gas_list\n",
        "    })\n",
        "  \n",
        "  new_df = pd.merge(per_doc_summary, gold,  how='left', left_on='Meeting', right_on ='meeting')\n",
        "  rouge_results_perdoc = compute_rouge_scores(new_df.Generated_Abstractive_Summary,\n",
        "                                      new_df.abstractive)\n",
        "  return rouge_results_perdoc\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxyXnBnZYex-"
      },
      "source": [
        "### Run Epoch\n",
        "Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y5TwfR5FBIki",
        "outputId": "8db9d52b-c893-4620-cd80-c4ee206551e3"
      },
      "source": [
        "id = wandb.util.generate_id()\n",
        "id\n",
        "#dwlkfpg3 AMI 1024\n",
        "#1aei9r6r ICSI 1024\n",
        "#3ugok7an ICSI 512\n",
        "#30e6cuxp AMI 512\n",
        "#3fsv41il ICSI 1024 Cleaned \n",
        "#2knqed4a AMI 1024 Cleaned \n",
        "#3le4t5oh AMI 1024 Cleaned t5-base\n",
        "#3oar0l9l ICSI 1024 t5-base\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2zpb09p1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "i8gLtGWbh7AZ",
        "outputId": "a5ab9740-d106-4867-8acf-964c51ee4da5"
      },
      "source": [
        "#run = wandb.init(project=\"T5_1024_MSFT_AMI_01\",resume=True)\n",
        "run = wandb.init(project=\"T5_1024_MSFT_ICSI_01\", id=\"3oar0l9l\", resume=\"allow\")\n",
        "\n",
        "config = wandb.config          # Initialize config\n",
        "config.TRAIN_BATCH_SIZE = 1    # input batch size for training (default: 64)\n",
        "config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n",
        "config.EPOCHS = 50        # number of epochs to train (default: 10)\n",
        "config.LEARNING_RATE = 0.0005   # learning rate (default: 0.01)\n",
        "config.SEED = 42               # random seed (default: 42)\n",
        "config.BEAMS = 9\n",
        "config.MAX_LEN = MAX_LEN\n",
        "config.SUMMARY_LEN = SUMMARY_LEN \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwuqq09\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Resuming run <strong style=\"color:#cdcd00\">solar-wildflower-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01/runs/3oar0l9l\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01/runs/3oar0l9l</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201125_013202-3oar0l9l</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5vvXTv0OQLK"
      },
      "source": [
        "# https://deeplizard.com/learn/video/kWVgvsejXsE#:~:text=The%20num_workers%20attribute%20tells%20the,sequentially%20inside%20the%20main%20process\n",
        "# num_workers to default 0\n",
        "# This means that the training process will work sequentially inside the main process. \n",
        "# After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
        "\n",
        "TEST_BATCH_SIZE = 1 \n",
        "\n",
        "train_params = {\n",
        "  'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "  'shuffle': True,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "dev_params = {\n",
        "  'batch_size': config.VALID_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "test_params = {\n",
        "  'batch_size': TEST_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "\n",
        "training_loader = DataLoader(train_set, **train_params)\n",
        "dev_loader = DataLoader(dev_set, **dev_params)\n",
        "test_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKxdX61LW6dY"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePGtVuaUXqli"
      },
      "source": [
        "# optimizer \n",
        "# https://pytorch.org/docs/stable/optim.html\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUD8KqP20gmT"
      },
      "source": [
        "# CP_TEMP_NAME = 'epoch10'\n",
        "# CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI512_NoNA/\" + CP_TEMP_NAME +\".pt\"\n",
        "# checkpoint = torch.load(CP_PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmes1VwYbjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51201195-dfa1-4fda-e561-bd91206a1986"
      },
      "source": [
        "training_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_ICSI_base\"\n",
        "start_train_time = time.time()\n",
        "wandb.watch(model, log='all')\n",
        "\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Training =================== ##\n",
        "  print(\"start training epoch\" + str(i))\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_TEMP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_ICSI1024_Based/\"+ CP_TEMP_NAME +\".pt\"\n",
        "  train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "  torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_epoch': i\n",
        "      }, CP_TEMP_PATH)\n",
        "  training_time = time.time() - start_train_time\n",
        "  print(\"done training epoch\" +str(i))\n",
        "  wandb.log({'epoch_traingTime': training_time,\n",
        "             'epoch': i})\n",
        "  print(\"--- %s seconds ---\" % (training_time))\n",
        "  training_time_log.append(training_time)\n",
        "  i+=1\n",
        "  ## ================= Validation =================== ##\n",
        "  # print(\"strat validation epoch\" + str(i))\n",
        "  # predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  # final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "  #                           'Golden_Abstractive_Text':actuals})\n",
        "  # final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "  # print(\"done validation epoch\" +str(i))\n",
        "\n",
        "  # rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "  #                                      final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  # wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "  #            'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "  #            'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "  #            'epoch': i})\n",
        "  # i+=1\n",
        "\n",
        "#run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "start training epoch0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1156: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  7.927191734313965\n",
            "done training epoch0\n",
            "--- 100.13376355171204 seconds ---\n",
            "start training epoch1\n",
            "Epoch: 1, Loss:  1.8690929412841797\n",
            "done training epoch1\n",
            "--- 189.97118210792542 seconds ---\n",
            "start training epoch2\n",
            "Epoch: 2, Loss:  0.527834415435791\n",
            "done training epoch2\n",
            "--- 273.95350551605225 seconds ---\n",
            "start training epoch3\n",
            "Epoch: 3, Loss:  0.35699981451034546\n",
            "done training epoch3\n",
            "--- 359.2053573131561 seconds ---\n",
            "start training epoch4\n",
            "Epoch: 4, Loss:  0.09226126968860626\n",
            "done training epoch4\n",
            "--- 455.89185190200806 seconds ---\n",
            "start training epoch5\n",
            "Epoch: 5, Loss:  0.08761376142501831\n",
            "done training epoch5\n",
            "--- 540.7267656326294 seconds ---\n",
            "start training epoch6\n",
            "Epoch: 6, Loss:  0.07429489493370056\n",
            "done training epoch6\n",
            "--- 624.6981515884399 seconds ---\n",
            "start training epoch7\n",
            "Epoch: 7, Loss:  0.05986139178276062\n",
            "done training epoch7\n",
            "--- 709.078638792038 seconds ---\n",
            "start training epoch8\n",
            "Epoch: 8, Loss:  0.22455963492393494\n",
            "done training epoch8\n",
            "--- 796.7590210437775 seconds ---\n",
            "start training epoch9\n",
            "Epoch: 9, Loss:  0.13103750348091125\n",
            "done training epoch9\n",
            "--- 892.9985163211823 seconds ---\n",
            "start training epoch10\n",
            "Epoch: 10, Loss:  0.1108304038643837\n",
            "done training epoch10\n",
            "--- 981.6975257396698 seconds ---\n",
            "start training epoch11\n",
            "Epoch: 11, Loss:  0.006240974646061659\n",
            "done training epoch11\n",
            "--- 1066.3251657485962 seconds ---\n",
            "start training epoch12\n",
            "Epoch: 12, Loss:  0.017556915059685707\n",
            "done training epoch12\n",
            "--- 1156.8388538360596 seconds ---\n",
            "start training epoch13\n",
            "Epoch: 13, Loss:  0.01991262473165989\n",
            "done training epoch13\n",
            "--- 1251.0344610214233 seconds ---\n",
            "start training epoch14\n",
            "Epoch: 14, Loss:  0.16353186964988708\n",
            "done training epoch14\n",
            "--- 1337.5562574863434 seconds ---\n",
            "start training epoch15\n",
            "Epoch: 15, Loss:  0.07687719166278839\n",
            "done training epoch15\n",
            "--- 1425.1780245304108 seconds ---\n",
            "start training epoch16\n",
            "Epoch: 16, Loss:  0.02980230189859867\n",
            "done training epoch16\n",
            "--- 1510.7822318077087 seconds ---\n",
            "start training epoch17\n",
            "Epoch: 17, Loss:  0.002890980336815119\n",
            "done training epoch17\n",
            "--- 1597.8206796646118 seconds ---\n",
            "start training epoch18\n",
            "Epoch: 18, Loss:  0.0012966797221451998\n",
            "done training epoch18\n",
            "--- 1691.0178961753845 seconds ---\n",
            "start training epoch19\n",
            "Epoch: 19, Loss:  0.003924667369574308\n",
            "done training epoch19\n",
            "--- 1778.3892323970795 seconds ---\n",
            "start training epoch20\n",
            "Epoch: 20, Loss:  0.002022142754867673\n",
            "done training epoch20\n",
            "--- 1864.670551776886 seconds ---\n",
            "start training epoch21\n",
            "Epoch: 21, Loss:  0.006258703302592039\n",
            "done training epoch21\n",
            "--- 1949.6263942718506 seconds ---\n",
            "start training epoch22\n",
            "Epoch: 22, Loss:  0.007263926789164543\n",
            "done training epoch22\n",
            "--- 2040.6229677200317 seconds ---\n",
            "start training epoch23\n",
            "Epoch: 23, Loss:  0.0012613028520718217\n",
            "done training epoch23\n",
            "--- 2129.6373674869537 seconds ---\n",
            "start training epoch24\n",
            "Epoch: 24, Loss:  0.0015808625612407923\n",
            "done training epoch24\n",
            "--- 2217.2160215377808 seconds ---\n",
            "start training epoch25\n",
            "Epoch: 25, Loss:  0.004799528978765011\n",
            "done training epoch25\n",
            "--- 2304.2545902729034 seconds ---\n",
            "start training epoch26\n",
            "Epoch: 26, Loss:  0.020584547892212868\n",
            "done training epoch26\n",
            "--- 2392.2642674446106 seconds ---\n",
            "start training epoch27\n",
            "Epoch: 27, Loss:  0.02348930574953556\n",
            "done training epoch27\n",
            "--- 2487.352681875229 seconds ---\n",
            "start training epoch28\n",
            "Epoch: 28, Loss:  0.050721440464258194\n",
            "done training epoch28\n",
            "--- 2571.696718454361 seconds ---\n",
            "start training epoch29\n",
            "Epoch: 29, Loss:  0.07677294313907623\n",
            "done training epoch29\n",
            "--- 2661.0375204086304 seconds ---\n",
            "start training epoch30\n",
            "Epoch: 30, Loss:  0.09695262461900711\n",
            "done training epoch30\n",
            "--- 2748.9304869174957 seconds ---\n",
            "start training epoch31\n",
            "Epoch: 31, Loss:  0.12880054116249084\n",
            "done training epoch31\n",
            "--- 2842.761965036392 seconds ---\n",
            "start training epoch32\n",
            "Epoch: 32, Loss:  0.30449986457824707\n",
            "done training epoch32\n",
            "--- 2931.615823030472 seconds ---\n",
            "start training epoch33\n",
            "Epoch: 33, Loss:  0.015281579457223415\n",
            "done training epoch33\n",
            "--- 3019.0966782569885 seconds ---\n",
            "start training epoch34\n",
            "Epoch: 34, Loss:  0.12879769504070282\n",
            "done training epoch34\n",
            "--- 3110.4477350711823 seconds ---\n",
            "start training epoch35\n",
            "Epoch: 35, Loss:  0.1838473379611969\n",
            "done training epoch35\n",
            "--- 3197.11727643013 seconds ---\n",
            "start training epoch36\n",
            "Epoch: 36, Loss:  0.0009673673193901777\n",
            "done training epoch36\n",
            "--- 3292.7666552066803 seconds ---\n",
            "start training epoch37\n",
            "Epoch: 37, Loss:  0.08625348657369614\n",
            "done training epoch37\n",
            "--- 3380.100021839142 seconds ---\n",
            "start training epoch38\n",
            "Epoch: 38, Loss:  0.049442268908023834\n",
            "done training epoch38\n",
            "--- 3468.619715452194 seconds ---\n",
            "start training epoch39\n",
            "Epoch: 39, Loss:  0.22183334827423096\n",
            "done training epoch39\n",
            "--- 3556.654823064804 seconds ---\n",
            "start training epoch40\n",
            "Epoch: 40, Loss:  0.0328826867043972\n",
            "done training epoch40\n",
            "--- 3650.3281769752502 seconds ---\n",
            "start training epoch41\n",
            "Epoch: 41, Loss:  0.11562120169401169\n",
            "done training epoch41\n",
            "--- 3736.0040497779846 seconds ---\n",
            "start training epoch42\n",
            "Epoch: 42, Loss:  0.002208494348451495\n",
            "done training epoch42\n",
            "--- 3824.2359120845795 seconds ---\n",
            "start training epoch43\n",
            "Epoch: 43, Loss:  0.06460724025964737\n",
            "done training epoch43\n",
            "--- 3911.4602632522583 seconds ---\n",
            "start training epoch44\n",
            "Epoch: 44, Loss:  0.009826217778027058\n",
            "done training epoch44\n",
            "--- 3997.619369506836 seconds ---\n",
            "start training epoch45\n",
            "Epoch: 45, Loss:  0.01825072057545185\n",
            "done training epoch45\n",
            "--- 4093.4198186397552 seconds ---\n",
            "start training epoch46\n",
            "Epoch: 46, Loss:  0.07738197594881058\n",
            "done training epoch46\n",
            "--- 4178.637118339539 seconds ---\n",
            "start training epoch47\n",
            "Epoch: 47, Loss:  0.0780363529920578\n",
            "done training epoch47\n",
            "--- 4266.788059473038 seconds ---\n",
            "start training epoch48\n",
            "Epoch: 48, Loss:  0.06831004470586777\n",
            "done training epoch48\n",
            "--- 4352.839517831802 seconds ---\n",
            "start training epoch49\n",
            "Epoch: 49, Loss:  0.10888365656137466\n",
            "done training epoch49\n",
            "--- 4446.346657991409 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3754ae93acf34703a9e6181470e5bfe7",
            "437b3dd852d64002a845521f0e0c05ab",
            "8fa7dd800c0b4a54aa30e678c1a169ec",
            "bc13303ab3884b38af22cee892fb363b",
            "472bcc0f7d5e4f44b722eda1985959a1",
            "c071dbadc93740ef9b43e171ae9e3e5a",
            "a3b0344f9ce84674a47e9335da2a9977",
            "2147e54c40244d04a1eeadcbc4540b01"
          ]
        },
        "id": "2tVOuLscBjxq",
        "outputId": "f67fd25a-3c0f-4b18-b1c7-463efda87e08"
      },
      "source": [
        "validation_time_log = []\n",
        "MODEL_NAME = \"T5_1024_MSFT_ICSI_base\"\n",
        "start_validation_time = time.time()\n",
        "\n",
        "print(\"starting fine-tuning with training and validation\")\n",
        "i = 0\n",
        "for epoch in range(config.EPOCHS):\n",
        "\n",
        "  ## ================= Validation =================== ##\n",
        "  print(\"strat validation epoch\" + str(i))\n",
        "  model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "  model = model.to(device)\n",
        "  # optimizer \n",
        "  # https://pytorch.org/docs/stable/optim.html\n",
        "  optimizer = torch.optim.Adam(params = model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "  CP_TEMP_NAME = 'epoch' + str(i)\n",
        "  CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_ICSI1024_Based/\" + CP_TEMP_NAME +\".pt\"\n",
        "  checkpoint = torch.load(CP_PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  wandb.watch(model, log='all')\n",
        "\n",
        "  predictions, actuals = dev(epoch, tokenizer, model, device, dev_loader)\n",
        "  final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "                            'Golden_Abstractive_Text':actuals})\n",
        "  final_df.to_csv('/content/drive/My Drive/W266/results/'+MODEL_NAME + \"_epoch\" +str(i)+'.csv')\n",
        "  print(\"done validation epoch\" +str(i))\n",
        "\n",
        "  rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "                                       final_df.Golden_Abstractive_Text)\n",
        "  \n",
        "  validation_time = time.time() - start_validation_time\n",
        "  validation_time_log.append(validation_time)\n",
        "\n",
        "  # amigold = pd.read_csv(AMI_PATH)\n",
        "  # icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "  rouge_results_perdoc = rouge_per_document(final_df,icsigold)\n",
        "  wandb.log({'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "            'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "            'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "            'rouge1_doclevel': rouge_results_perdoc.get(\"rouge_1_f_score\"), \n",
        "            'rougeL_doclevel': rouge_results_perdoc.get(\"rouge_l_f_score\"),  \n",
        "            'rouge2_doclevel': rouge_results_perdoc.get(\"rouge_2_f_score\"),\n",
        "            'epoch_validationTime': validation_time,\n",
        "            'epoch': i})\n",
        "  i+=1\n",
        "\n",
        "run.finish()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting fine-tuning with training and validation\n",
            "strat validation epoch0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch0\n",
            "strat validation epoch1\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch1\n",
            "strat validation epoch2\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch2\n",
            "strat validation epoch3\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch3\n",
            "strat validation epoch4\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch4\n",
            "strat validation epoch5\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch5\n",
            "strat validation epoch6\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch6\n",
            "strat validation epoch7\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch7\n",
            "strat validation epoch8\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch8\n",
            "strat validation epoch9\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch9\n",
            "strat validation epoch10\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch10\n",
            "strat validation epoch11\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch11\n",
            "strat validation epoch12\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch12\n",
            "strat validation epoch13\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch13\n",
            "strat validation epoch14\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch14\n",
            "strat validation epoch15\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch15\n",
            "strat validation epoch16\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch16\n",
            "strat validation epoch17\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch17\n",
            "strat validation epoch18\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch18\n",
            "strat validation epoch19\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch19\n",
            "strat validation epoch20\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch20\n",
            "strat validation epoch21\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch21\n",
            "strat validation epoch22\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch22\n",
            "strat validation epoch23\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch23\n",
            "strat validation epoch24\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch24\n",
            "strat validation epoch25\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch25\n",
            "strat validation epoch26\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch26\n",
            "strat validation epoch27\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch27\n",
            "strat validation epoch28\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch28\n",
            "strat validation epoch29\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch29\n",
            "strat validation epoch30\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch30\n",
            "strat validation epoch31\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch31\n",
            "strat validation epoch32\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch32\n",
            "strat validation epoch33\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch33\n",
            "strat validation epoch34\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch34\n",
            "strat validation epoch35\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch35\n",
            "strat validation epoch36\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch36\n",
            "strat validation epoch37\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch37\n",
            "strat validation epoch38\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch38\n",
            "strat validation epoch39\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch39\n",
            "strat validation epoch40\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch40\n",
            "strat validation epoch41\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch41\n",
            "strat validation epoch42\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch42\n",
            "strat validation epoch43\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch43\n",
            "strat validation epoch44\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch44\n",
            "strat validation epoch45\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch45\n",
            "strat validation epoch46\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch46\n",
            "strat validation epoch47\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch47\n",
            "strat validation epoch48\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch48\n",
            "strat validation epoch49\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "done validation epoch49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 6203<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3754ae93acf34703a9e6181470e5bfe7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20201125_013202-3oar0l9l/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20201125_013202-3oar0l9l/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>Training Loss</td><td>0.08014</td></tr><tr><td>_timestamp</td><td>1606287591</td></tr><tr><td>_runtime</td><td>24267</td></tr><tr><td>_step</td><td>1249</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>epoch_traingTime</td><td>4446.34666</td></tr><tr><td>rouge1</td><td>0.156</td></tr><tr><td>rougeL</td><td>0.12162</td></tr><tr><td>rouge2</td><td>0.0075</td></tr><tr><td>rouge1_doclevel</td><td>0.18051</td></tr><tr><td>rougeL_doclevel</td><td>0.13797</td></tr><tr><td>rouge2_doclevel</td><td>0.01307</td></tr><tr><td>epoch_validationTime</td><td>19618.71371</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>rouge1</td><td>▂▂▅▅▅▅▄▆▆▅▂▅▄▄▅▃▅▆▄▄▆▆▄▇▅▆▇▇▅██▇█▇▇▇▇█▁▇</td></tr><tr><td>rougeL</td><td>▁▂▅▅▅▅▄▇▆▅▂▅▅▅▅▃▅▇▄▄▇▇▄▇▄███▃████▇███▇▁█</td></tr><tr><td>rouge2</td><td>▃▂▄▇▆▄▄▅▄▅▃▇▅▄▄▂▄▄▅▄▄▅▄▃▆█▃▃▂▆▆▅▆▆▃▇▃▃▁▄</td></tr><tr><td>rouge1_doclevel</td><td>▄▅▇▆▆█▇▇█▇▄▇▇▇█▇▇▇▇▇▇▇▇▅▄▃▄▄▂▄▄▅▄▅▄▄▄▄▁▅</td></tr><tr><td>rougeL_doclevel</td><td>▆▇▇▇▆▇▇██▇▆▇▇▇▇▆▇▇█▇▇▇█▅▂▄▄▄▂▄▄▅▄▄▄▅▄▄▁▄</td></tr><tr><td>rouge2_doclevel</td><td>▄▅▅▆▆▆▆▇▇▇▄██▇▇▄▇▇▇▇▆▆█▄▄▄▂▂▁▄▄▅▄▄▂▄▂▂▁▃</td></tr><tr><td>epoch_validationTime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">solar-wildflower-4</strong>: <a href=\"https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01/runs/3oar0l9l\" target=\"_blank\">https://wandb.ai/wuqq09/T5_1024_MSFT_ICSI_01/runs/3oar0l9l</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiml8KGSeSGB"
      },
      "source": [
        "#### Checkpoint \n",
        "\n",
        "Remember to change the CP_NAME to a new model pt name.\n",
        "\n",
        "The model is then saved as checkpoints to Google Drive with the related tunable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVgDGjccbZQY"
      },
      "source": [
        "# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
        "# Checkpoint Saving\n",
        "CP_NAME = MODEL_NAME\n",
        "\n",
        "CP_TRAIN_EPOCHS = TRAIN_EPOCHS\n",
        "CP_DEV_EPOCHS = DEV_EPOCHS\n",
        "CP_LEARNING_RATE = LEARNING_RATE\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/\"+ CP_NAME +\".pt\"\n",
        "CP_MAX_LEN = MAX_LEN\n",
        "CP_SUMMARY_LEN = SUMMARY_LEN\n",
        "CP_TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
        "CP_DEV_BATCH_SIZE = DEV_BATCH_SIZE\n",
        "CP_MODEL = 'T5ForConditionalGeneration,t5-small'\n",
        "CP_OPTIMIZER_OPTION = 'Adam'\n",
        "CP_LOSSLIST = losslist\n",
        "CP_TEST_OPTIONS = {\n",
        "    \"num_beams\":          12,\n",
        "    \"repetition_penalty\": 2.5, \n",
        "    \"length_penalty\":     1.0, \n",
        "    \"early_stopping\":     True\n",
        "}\n",
        "CT_TRAIN_TIME = training_time\n",
        "#CT_EVALUATE_TIME = evaluating_time\n",
        "\n",
        "torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_epoch': CP_TRAIN_EPOCHS,\n",
        "            'dev_epoch': CP_DEV_EPOCHS,\n",
        "            'learning_rate': CP_LEARNING_RATE,\n",
        "            'max_source_length':CP_MAX_LEN,\n",
        "            'max_target_length':CP_SUMMARY_LEN,\n",
        "            'train_batch_size':CP_TRAIN_BATCH_SIZE,\n",
        "            'dev_batch_size':CP_DEV_BATCH_SIZE,\n",
        "            'model_option':CP_MODEL,\n",
        "            'optimizer_option':CP_OPTIMIZER_OPTION,\n",
        "            'losslist': CP_LOSSLIST,\n",
        "            'training_time': CT_TRAIN_TIME,\n",
        "            #'evaluating_time': CT_EVALUATE_TIME,\n",
        "            'test_option': CP_TEST_OPTIONS\n",
        "            }, CP_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-EORf2EbxV3",
        "outputId": "7db9540d-f6a2-41a4-fd93-417f63e01b10"
      },
      "source": [
        "MODEL_NAME = \"epoch61\"\n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/\" + MODEL_NAME +\".pt\"\n",
        "print(CP_PATH)\n",
        "checkpoint = torch.load(CP_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# train_epoch = checkpoint['train_epoch']\n",
        "# dev_epoch = checkpoint['dev_epoch']\n",
        "# losslist = checkpoint['losslist']\n",
        "# learning_rate = checkpoint['learning_rate']\n",
        "# max_source_length = checkpoint['max_source_length']\n",
        "# max_target_length = checkpoint['max_target_length']\n",
        "# train_batch_size = checkpoint['train_batch_size']\n",
        "# dev_batch_size = checkpoint['dev_batch_size']\n",
        "# optimizer_option = checkpoint['optimizer_option']\n",
        "# test_option = checkpoint['test_option']\n",
        "# training_time = checkpoint['training_time']\n",
        "\n",
        "\n",
        "# evaluating_time = checkpoint['evaluating_time']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_AMI1024_NoNA/epoch61.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPFN2UlXn37l"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMPmQP1qn6Yv",
        "outputId": "6d5a2cfa-88a0-43c4-b951-a866b93b83aa"
      },
      "source": [
        "test_dataset_512_icsi = pd.read_csv('/content/drive/My Drive/W266/data/512_tokens/ICSI_512_test.csv',encoding='latin-1')\n",
        "test_dataset_512_ami = pd.read_csv('/content/drive/My Drive/W266/data/512_tokens/AMI_512_test.csv',encoding='latin-1')\n",
        "\n",
        "test_dataset_512_icsi = test_dataset_512_icsi.dropna(subset=['abstractive'])\n",
        "test_dataset_512_icsi = test_dataset_512_icsi.reset_index(drop=True)\n",
        "\n",
        "test_dataset_512_ami = test_dataset_512_ami.dropna(subset=['abstractive'])\n",
        "test_dataset_512_ami = test_dataset_512_ami.reset_index(drop=True)\n",
        "\n",
        "# use the pre-defined \"summarize\" for abstractive summary\n",
        "test_dataset_512_icsi.original = 'summarize: ' + test_dataset_512_icsi.original\n",
        "test_dataset_512_ami.original = 'summarize: ' + test_dataset_512_ami.original\n",
        "\n",
        "print(len(test_dataset_512_ami))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25aYtvxpdGl"
      },
      "source": [
        "MAX_LEN_512 = 512\n",
        "MAX_LEN_1024 = 1024\n",
        "SUMMARY_LEN= 150\n",
        "\n",
        "# note here only uses the t5-small model.\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "test_set_512_icsi  = CustomDataset(test_dataset_512_icsi, tokenizer, MAX_LEN_512, SUMMARY_LEN)\n",
        "test_set_512_ami  = CustomDataset(test_dataset_512_ami, tokenizer, MAX_LEN_512, SUMMARY_LEN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNgYmRkLqTjT"
      },
      "source": [
        "# https://deeplizard.com/learn/video/kWVgvsejXsE#:~:text=The%20num_workers%20attribute%20tells%20the,sequentially%20inside%20the%20main%20process\n",
        "# num_workers to default 0\n",
        "# This means that the training process will work sequentially inside the main process. \n",
        "# After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
        "\n",
        "TEST_BATCH_SIZE = 1 \n",
        "\n",
        "test_params = {\n",
        "  'batch_size': TEST_BATCH_SIZE,\n",
        "  'shuffle': False,\n",
        "  'num_workers': 0\n",
        "  }\n",
        "test_loader_512_icsi = DataLoader(test_set_512_icsi, **test_params)\n",
        "test_loader_512_ami = DataLoader(test_set_512_ami, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMKUN-nCpwtp"
      },
      "source": [
        "# https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81\n",
        "\n",
        "def test(epoch, tokenizer, model, device, loader, beams):\n",
        "  #https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  #rouge_metric = load_metric('rouge') \n",
        "  # https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _, data in enumerate(loader, 0):\n",
        "\n",
        "      y = data['target_ids'].to(device, dtype = torch.long)\n",
        "      ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask, \n",
        "          max_length=150, \n",
        "          num_beams=beams,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "      if _%100==0:\n",
        "          print(f'Completed {_}')\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "  return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs8ziDgiqCjk"
      },
      "source": [
        "AMI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_AMI_as_test.csv\"\n",
        "ICSI_PATH = \"/content/drive/My Drive/W266/data/gold_abstractive_summary/goldsummary_ICSI_as_test.csv\"\n",
        "\n",
        "amigold = pd.read_csv(AMI_PATH)\n",
        "icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "def rouge_per_document(final_df,gold, test_dataset):\n",
        "  merged_df = pd.concat([test_dataset.meeting, final_df.Generated_Abstractive_Summary], axis=1)\n",
        "  merged_df[\"meetinglevel\"] = merged_df.meeting.apply(lambda x: x.split(\".\")[0]) \n",
        "\n",
        "  gas_list =[]\n",
        "  meeting_list = []\n",
        "  generated_abstractive = \"\"\n",
        "  for me in set(merged_df.meetinglevel):\n",
        "    for gas in merged_df[merged_df.meetinglevel == me]['Generated_Abstractive_Summary']:\n",
        "      generated_abstractive+= gas + \" \"\n",
        "    gas_list.append(generated_abstractive)\n",
        "    meeting_list.append(me)\n",
        "    generated_abstractive = \" \"\n",
        "  per_doc_summary = pd.DataFrame(\n",
        "    {'Meeting': meeting_list,\n",
        "     'Generated_Abstractive_Summary': gas_list\n",
        "    })\n",
        "  \n",
        "  new_df = pd.merge(per_doc_summary, gold,  how='left', left_on='Meeting', right_on ='meeting')\n",
        "  pring(new_df.head(5))\n",
        "  rouge_results_perdoc = compute_rouge_scores(new_df.Generated_Abstractive_Summary,\n",
        "                                      new_df.abstractive)\n",
        "  return rouge_results_perdoc\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626,
          "referenced_widgets": [
            "961afc58a76f4e09a608cfd2c9b3ad48",
            "a509624c90114a37b2d1dc8548ee2de8",
            "4601da506b8c4535a1bbd50f2a5d9276",
            "15f1cc56d5414f7cadf0b9e1152f9d7a",
            "23bcb7d20c874b93a2f43926f5062a57",
            "58234155adf6421a866f80c30792f96f",
            "64e946fefa3046d5b9ccaf8e74c3af0b",
            "d25868c379fe429ba5c9d8b972545a5d",
            "928005046f2b4252962f3a7f1ca1b85a",
            "59628bd2a2c140f48076c0bb94e25cf3",
            "c45562d249184bfe8bc769ea85944570",
            "4d6ae993c3094dff960d18fd097e2153",
            "d2f36e345f5d45bbb22185dd14521861",
            "fea38bc12f7f4ca9b396549573539257",
            "b138aead78f6413fb1189771b56d8732",
            "3ffcc1567fe44879af3aaa7d990ec079"
          ]
        },
        "id": "WxLGRjF8rHlq",
        "outputId": "b8941883-8594-4e57-dab4-fe30b848683d"
      },
      "source": [
        "start_test_time = time.time()\n",
        "LEARNING_RATE = 0.0005\n",
        "MODEL_NAME = \"ICSI_512_4\"\n",
        "## ================= TESTING =================== ##\n",
        "print(\"strat testing\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "CP_TEMP_NAME = 'epoch4' \n",
        "CP_PATH = \"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_ICSI512_NoNA/\" + CP_TEMP_NAME +\".pt\"\n",
        "checkpoint = torch.load(CP_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "predictions, actuals = test(epoch, tokenizer, model, device, dev_loader,9)\n",
        "final_df = pd.DataFrame({'Generated_Abstractive_Summary':predictions,\n",
        "                          'Golden_Abstractive_Text':actuals})\n",
        "final_df.to_csv('/content/drive/My Drive/W266/final/'+MODEL_NAME +'_prediction.csv')\n",
        "\n",
        "rouge_results = compute_rouge_scores(final_df.Generated_Abstractive_Summary,\n",
        "                                      final_df.Golden_Abstractive_Text)\n",
        "\n",
        "test_time = time.time() - start_test_time\n",
        "\n",
        "# amigold = pd.read_csv(AMI_PATH)\n",
        "# icsigold = pd.read_csv(ICSI_PATH)\n",
        "\n",
        "rouge_results_perdoc = rouge_per_document(final_df,icsigold,test_set_512_icsi)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "          'model': MODEL_NAME,\n",
        "          'rouge1': rouge_results.get(\"rouge_1_f_score\"), \n",
        "          'rougeL': rouge_results.get(\"rouge_l_f_score\"),  \n",
        "          'rouge2': rouge_results.get(\"rouge_2_f_score\"),\n",
        "          'rouge1_doclevel': rouge_results_perdoc.get(\"rouge_1_f_score\"), \n",
        "          'rougeL_doclevel': rouge_results_perdoc.get(\"rouge_l_f_score\"),  \n",
        "          'rouge2_doclevel': rouge_results_perdoc.get(\"rouge_2_f_score\"),\n",
        "          'testTime': test_time\n",
        "          })\n",
        "print(results)\n",
        "results.to_csv('/content/drive/My Drive/W266/final/'+MODEL_NAME +'_rouge.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "strat testing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "961afc58a76f4e09a608cfd2c9b3ad48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "928005046f2b4252962f3a7f1ca1b85a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1df4603ae95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mCP_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/W266/checkpoints/MSFT_50EPOCH_Intransit_ICSI512_NoNA/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCP_TEMP_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCP_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for T5ForConditionalGeneration:\n\tUnexpected key(s) in state_dict: \"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukA0Qa1XuFYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}